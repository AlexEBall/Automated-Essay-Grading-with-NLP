{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"../utils\")\n",
    "\n",
    "from helpers import adding_stanford_nlp_groups_NER_to_stop_words, \n",
    "    removing_stanford_nlp_groups_NER_from_stop_words, \n",
    "    punct_space_stop, line_review, \n",
    "    lemmatized_sentence_corpus\n",
    "\n",
    "os.chdir(\"../notebooks\")\n",
    "\n",
    "# from ../utils.helpers import adding_stanford_nlp_groups_NER_to_stop_words, removing_stanford_nlp_groups_NER_from_stop_words, punct_space_stop, line_review, lemmatized_sentence_corpus\n",
    "\n",
    "essays = pd.read_csv('../data/intermediate/prepped_essays_df.csv')\n",
    "\n",
    "# try svm, k-nn, random forrest\n",
    "# remove @Person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = essays[essays['essay_set'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>prompt</th>\n",
       "      <th>has_source_material</th>\n",
       "      <th>source_text</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "1             5.0             4.0             NaN            9.0   \n",
       "2             4.0             3.0             NaN            7.0   \n",
       "3             5.0             5.0             NaN           10.0   \n",
       "4             4.0             4.0             NaN            8.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN   \n",
       "\n",
       "                                              prompt  has_source_material  \\\n",
       "0  More and more people use computers, but not ev...                    0   \n",
       "1  More and more people use computers, but not ev...                    0   \n",
       "2  More and more people use computers, but not ev...                    0   \n",
       "3  More and more people use computers, but not ev...                    0   \n",
       "4  More and more people use computers, but not ev...                    0   \n",
       "\n",
       "   source_text  grade_7  grade_8  grade_10  \n",
       "0          NaN        0        1         0  \n",
       "1          NaN        0        1         0  \n",
       "2          NaN        0        1         0  \n",
       "3          NaN        0        1         0  \n",
       "4          NaN        0        1         0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>prompt</th>\n",
       "      <th>has_source_material</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  domain1_score  \\\n",
       "0             4.0             4.0            8.0   \n",
       "\n",
       "                                              prompt  has_source_material  \\\n",
       "0  More and more people use computers, but not ev...                    0   \n",
       "\n",
       "   grade_7  grade_8  grade_10  \n",
       "0        0        1         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('../data/intermediate')\n",
    "\n",
    "essay_set1_txt_filepath = os.path.join(intermediate_directory, 'essay_set1_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 1,783 essays in the txt file.\n",
      "CPU times: user 46.1 ms, sys: 3.94 ms, total: 50 ms\n",
      "Wall time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    essay_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(essay_set1_txt_filepath, 'w', encoding='utf_8') as essay_set1_txt_file:\n",
    "\n",
    "        # loop through all essays in the dataframe\n",
    "        for row in essays.itertuples():\n",
    "\n",
    "            # write the essay as a line in the new file and escape newline characters in the original essays\n",
    "            essay_set1_txt_file.write(row.essay.replace('\\n', '\\\\n') + '\\n')\n",
    "            essay_count += 1\n",
    "\n",
    "    print('Text from {:,} essays written to the new txt file.'.format(essay_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(essay_set1_txt_filepath, encoding='utf_8') as essay_set1_txt_file:\n",
    "        for essay_count, line in enumerate(essay_set1_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print('Text from {:,} essays in the txt file.'.format(essay_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_essay = essays.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.6 ms, sys: 38.3 ms, total: 113 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_essay = nlp(test_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble!\n",
      "\n",
      "Sentence 2:\n",
      "Thing about!\n",
      "\n",
      "Sentence 3:\n",
      "Dont you think so?\n",
      "\n",
      "Sentence 4:\n",
      "How would you feel if your teenager is always on the phone with friends!\n",
      "\n",
      "Sentence 5:\n",
      "Do you ever time to chat with your friends or buisness partner about things.\n",
      "\n",
      "Sentence 6:\n",
      "Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect.\n",
      "\n",
      "Sentence 7:\n",
      "Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it.\n",
      "\n",
      "Sentence 8:\n",
      "How did you learn about other countrys\n",
      "\n",
      "Sentence 9:\n",
      "/states outside of yours?\n",
      "\n",
      "Sentence 10:\n",
      "Well I have by computer/internet\n",
      "\n",
      "Sentence 11:\n",
      ", it's a new way to learn about what going on in our time!\n",
      "\n",
      "Sentence 12:\n",
      "You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows.\n",
      "\n",
      "Sentence 13:\n",
      "Believe it or not the computer is much interesting then in class all day reading out of books.\n",
      "\n",
      "Sentence 14:\n",
      "If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right.\n",
      "\n",
      "Sentence 15:\n",
      "You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by.\n",
      "\n",
      "Sentence 16:\n",
      "Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place.\n",
      "\n",
      "Sentence 17:\n",
      "Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble.\n",
      "\n",
      "Sentence 18:\n",
      "Thank you for listening.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_essay.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: @CAPS1 - ORG\n",
      "\n",
      "Entity 2: @DATE1 - ORG\n",
      "\n",
      "Entity 3: all day - DATE\n",
      "\n",
      "Entity 4: @CAPS2 - ORG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_essay.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_text = [token.orth_ for token in parsed_essay]\n",
    "# token_pos = [token.pos_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_pos),\n",
    "#              columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lemma = [token.lemma_ for token in parsed_essay]\n",
    "# token_shape = [token.shape_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "#              columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_entity_type = [token.ent_type_ for token in parsed_essay]\n",
    "# token_entity_iob = [token.ent_iob_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n",
    "#              columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_attributes = [(token.orth_,\n",
    "#                      token.prob,\n",
    "#                      token.is_stop,\n",
    "#                      token.is_punct,\n",
    "#                      token.is_space,\n",
    "#                      token.like_num,\n",
    "#                      token.is_oov)\n",
    "#                     for token in parsed_essay]\n",
    "\n",
    "# df = pd.DataFrame(token_attributes,\n",
    "#                   columns=['text',\n",
    "#                            'log_probability',\n",
    "#                            'stop?',\n",
    "#                            'punctuation?',\n",
    "#                            'whitespace?',\n",
    "#                            'number?',\n",
    "#                            'out of vocab.?'])\n",
    "\n",
    "# df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "#                                        .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing_stanford_nlp_groups_NER_from_stop_words(nlp)\n",
    "adding_stanford_nlp_groups_NER_to_stop_words(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all_essays.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_set1_all_filepath = os.path.join(intermediate_directory, 'essay_set1_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 1e+03 ns, total: 4 Âµs\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(essays_set1_all_filepath, codecs, nlp):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe computer benefit way like talk friend website like facebook mysace\n",
      "\n",
      "computer help find coordibate location able ourselfs million information\n",
      "\n",
      "computer benefit help job plan house plan type page report job write\n",
      "\n",
      "let wonder world technology\n",
      "\n",
      "computer help life talk make friend line\n",
      "\n",
      "people myspace facebooks aim benefit have conversation\n",
      "\n",
      "people believe computer bad friend talk\n",
      "\n",
      "fortunate have computer help school work social life friend\n",
      "\n",
      "computer help find location coordibate million information online\n",
      "\n",
      "internet lot know website help location coordinate like\n",
      "\n",
      "use computer\n",
      "\n",
      "suppose vacation\n",
      "\n",
      "million information find internet\n",
      "\n",
      "question computer\n",
      "\n",
      "easily draw house plan computer hour hand ugly erazer mark garrentee find job drawing like\n",
      "\n",
      "apple job worker write long paper like word essay job fit people know like write word non stopp hour\n",
      "\n",
      "hav\n",
      "\n",
      "computer\n",
      "\n",
      "computer need lot adays\n",
      "\n",
      "hope essay impact descion computer great machine work\n",
      "\n",
      "day show mom use computer\n",
      "\n",
      "say great invention sense slice bread\n",
      "\n",
      "buy computer help chat online friend find location million information click button help self get job neat prepared print work boss love\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 19, 42):\n",
    "    print(' '.join(unigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.LineSentence"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a phrase model for word pairs, let's apply it to the sentences data and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 20.5 ms, total: 129 ms\n",
      "Wall time: 236 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe computer benefit way like talk friend website like facebook mysace\n",
      "\n",
      "computer help find coordibate location able ourselfs million information\n",
      "\n",
      "computer benefit help job plan house plan type page report job write\n",
      "\n",
      "let wonder world technology\n",
      "\n",
      "computer help life talk make friend line\n",
      "\n",
      "people myspace facebooks aim benefit have conversation\n",
      "\n",
      "people believe computer bad friend talk\n",
      "\n",
      "fortunate have computer help school_work social life friend\n",
      "\n",
      "computer help find location coordibate million information online\n",
      "\n",
      "internet lot know website help location coordinate like\n",
      "\n",
      "use computer\n",
      "\n",
      "suppose vacation\n",
      "\n",
      "million information find internet\n",
      "\n",
      "question computer\n",
      "\n",
      "easily draw house plan computer hour hand ugly erazer mark garrentee find job drawing like\n",
      "\n",
      "apple job worker write long paper like word essay job fit people know like write word non stopp hour\n",
      "\n",
      "hav\n",
      "\n",
      "computer\n",
      "\n",
      "computer need lot adays\n",
      "\n",
      "hope essay impact descion computer great machine work\n",
      "\n",
      "day show mom use computer\n",
      "\n",
      "say great_invention sense slice bread\n",
      "\n",
      "buy computer help chat online friend find location million information click_button help self get job neat prepared print work boss love\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 19, 42):\n",
    "    print(' '.join(bigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 97.7 ms, sys: 10.1 ms, total: 108 ms\n",
      "Wall time: 225 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right away\n",
      "\n",
      "way able communicate family friend computer\n",
      "\n",
      "computer easy e_mail tell answer house phone\n",
      "\n",
      "happen think\n",
      "\n",
      "People need communicate computer lot\n",
      "\n",
      "work need talk employee co_worker leave desk e_mail information\n",
      "\n",
      "employee fast_easy e_mail information oppose talk phone person\n",
      "\n",
      "lot people agree computer life lot easy\n",
      "\n",
      "computer teach_hand_eye_coordination let communicate people\n",
      "\n",
      "critical reason computer let people learn_faraway_place people\n",
      "\n",
      "difference way people feel computer\n",
      "\n",
      "write local_newspaper\n",
      "\n",
      "dear_reader dramatic effect human life\n",
      "\n",
      "change way today\n",
      "\n",
      "know computer\n",
      "\n",
      "device allow people buy thing online talk people online provide_entertainment people\n",
      "\n",
      "good quality everyone life easy\n",
      "\n",
      "imagine look refrigerator notice\n",
      "\n",
      "car need grocery shopping store far\n",
      "\n",
      "computer look food online\n",
      "\n",
      "ther great_deal\n",
      "\n",
      "company deliver free\n",
      "\n",
      "amazing easy way buy food leave_house\n",
      "\n",
      "food purchase\n",
      "\n",
      "product sell computer\n",
      "\n",
      "need new toy kid\n",
      "\n",
      "new hat friend\n",
      "\n",
      "maybe curtain room\n",
      "\n",
      "easy access internet computer buy item\n",
      "\n",
      "computer way communication\n",
      "\n",
      "let want talk friend relative_live_far_away country\n",
      "\n",
      "dosen't\n",
      "\n",
      "phone\n",
      "\n",
      "computer\n",
      "\n",
      "communicate email adress\n",
      "\n",
      "friend family talk ease computer\n",
      "\n",
      "type want boom instant chat\n",
      "\n",
      "let face\n",
      "\n",
      "matter child teenager\n",
      "\n",
      "computer change\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 205, 245):\n",
    "    print(' '.join(trigram_sentence))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_essays_all_filepath = os.path.join(intermediate_directory, 'trigram_essays_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Âµs, sys: 1 Âµs, total: 4 Âµs\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "#     with codecs.open(trigram_essays_all_filepath, 'w', encoding='utf_8') as f:\n",
    "#         for sentence in lemmatized_sentence_corpus(essays_set1_all_filepath, codecs, nlp):\n",
    "#             f.write(sentence + '\\n')\n",
    "    \n",
    "    \n",
    "    with codecs.open(trigram_essays_all_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_essay in nlp.pipe(line_review(essays_set1_all_filepath, codecs), batch_size=100, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_essays = [token.lemma_ for token in parsed_essay\n",
    "                              if not punct_space_stop(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_essays = bigram_model[unigram_essays]\n",
    "            trigram_essays = trigram_model[bigram_essays]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_essays = ' '.join(trigram_essays)\n",
    "            f.write(trigram_essays + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Dear @CAPS1 times, @CAPS2 you think computers benefit society? Well I think so! There are countless reasons why computers are both resourceful and helpful. Many citizens in our own community of watertown think computers are a great resource for many things while others disagree with this completely. Computers can benefit society because you can learn many new things on the internet, also you can interact with your friends and family, and lastly there are many applications used for business. On both a computer and the internet there are more than @NUM1 million things you can learn. When you are struggling with homework a computer is a great resource. You can quickly open @CAPS3.com and search any topic at any time. For example, if you did not know a conversion it is easily found on the internet. Another thing you can be taught or informed about is news. There are websites such as nytimes.com and cnn.com that give you daily news. I personally use these websites weekly. On the computer you can find vacation sports you want to learn about and go to. When my family was planning our trip to @ORGANIZATION1 in @LOCATION1 we used the computer constantly. In the end, computers can teach you many things. Secondly you can interact with people on the internet in many ways. These include social networking websites, webchat and email. Social networking websites are a great way to connect to family members and friends from the past. Websites that were created for this include: facebook, myspace, and twitter. These free websites let you add friends, send messages, and post pictures. Web chatting is a way to video people. You can personally see someone from acroos the @ORGANIZATION1 from your webcam. Lastly, emailing is a great way to inform people. You can write a visual letter to one of your friends. In conclusion, the computer is one excellent way to interact. Lastly, the computer is used for business. An application called microsoft office is used in countless bussinesses, microsoft office word, powerpoint, excel and @CAPS4. Each of these are used to plan, sell, present, manage, and write about products. Microsoft is something you should download in the near future. Business also use webchatting, as I previously state, for conference calls. A production company can be face to face with a company from china without being in the room with them. This makes it much easier than to fly half way across the @ORGANIZATION1. Finally, computers are used when manufacturing a product in or at a business. Work sites such as @CAPS5 companies use computers to control their machines. I know that the @ORGANIZATION2 factory uses computers in their factory due to the fact that I watche the show unwrapped about them on @CAPS5 @CAPS6. In the end computers are a necessary item in a business. In conslusion, computers have a great affect on people. The can teach people many new and exciting things they did not know. Also, connect humans with each from half way across the @ORGANIZATION1. Lastly, businesses would not run well without a computer. Computers can benefit society because you can learn many new things on the internet. Also you can interact with your friends and family, and there are many applications used for business. If I were an expert I would have nothing to worry about concerning computers. They are a magnificent thing. @CAPS2 you have the same belief and understanding as me?\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "dear time think computer benefit_society think countless reason computer resourceful helpful citizen community watertown think computer great_resource thing disagree completely computer benefit_society learn_new thing internet interact friend family lastly application business computer internet million thing learn struggle homework computer great_resource quickly open @CAPS3.com search_topic time example know conversion easily find internet thing teach inform news website nytimes.com cnn.com daily news personally use website weekly computer find vacation sport want learn family plan_trip computer constantly end computer teach thing secondly interact people internet way include social_networking website webchat email social_networking website great_way connect family_member friend past website create include facebook_myspace_twitter free website let add friend send_message post_picture web chatting way video people personally acroo webcam lastly emailing great_way inform people write visual letter friend conclusion computer excellent way interact lastly computer business application call microsoft office countless bussinesse microsoft office word powerpoint excel plan sell present manage write product Microsoft download near future business use webchatting previously state conference call production company face_face company china room make easy fly half way finally computer manufacture product business work site company use computer control machine know factory use computer factory fact watche unwrap end computer necessary item business conslusion computer great affect people teach people new exciting thing know connect human half way lastly business run computer computer benefit_society learn_new thing internet interact friend family application business expert worry concern computer magnificent thing belief understanding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original:' + '\\n')\n",
    "\n",
    "for essay in it.islice(line_review(essays_set1_all_filepath, codecs), 301, 302):\n",
    "    print(essay)\n",
    "\n",
    "print('----' + '\\n')\n",
    "print('Transformed:' + '\\n')\n",
    "\n",
    "with codecs.open(trigram_essays_all_filepath, encoding='utf_8') as f:\n",
    "    for essay in it.islice(f, 301, 302):\n",
    "        print(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topic modeling* is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) or LDA, a popular approach to topic modeling.\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a *vector* of token counts. There are two layers in this model &mdash; documents and tokens &mdash; and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "* Document vectors tend to be large (one dimension for each token $\\Rightarrow$ lots of dimensions)\n",
    "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "* The dimensions are fully indepedent from each other &mdash; there's no sense of connection between related tokens, such as _knife_ and _fork_.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of *topics*, and the *topics* are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow [*Dirichlet*](https://en.wikipedia.org/wiki/Dirichlet_distribution) probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html) class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.02 ms, sys: 1.16 ms, total: 3.19 ms\n",
      "Wall time: 2.49 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_essays = LineSentence(trigram_essays_all_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_essays)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many NLP techniques, LDA uses a simplifying assumption known as the [*bag-of-words* model](https://en.wikipedia.org/wiki/Bag-of-words_model). In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded. \n",
    "\n",
    "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The `trigram_bow_generator` function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n",
    "\n",
    "In the following code, \"bag-of-words\" is abbreviated as `bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for essay in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 ms, sys: 6.32 ms, total: 8.93 ms\n",
      "Wall time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_essays_all_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bag-of-words corpus, we're finally ready to learn our topic model from the essays. We simply need to pass the bag-of-words matrix and Dictionary from our previous steps to `LdaMulticore` as inputs, along with the number of topics the model should learn. For this demo, we're asking for 5 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.07 ms, sys: 8.54 ms, total: 14.6 ms\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=5,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=5):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print('{:20} {}'.format('term', 'frequency') + '\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=5):\n",
    "        print('{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "look                 0.010\n",
      "website              0.010\n",
      "school               0.009\n",
      "kid                  0.009\n",
      "not                  0.009\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: 'reasons to spend time online',\n",
    "               1: 'spend time online kids play games',\n",
    "               2: 'internet helps to learn about the world',\n",
    "               3: 'want to find/learn but could be bad',\n",
    "               4: 'internet is a tool to find and look'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickle stops this viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'wb') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.75 ms, sys: 4.51 ms, total: 9.26 ms\n",
      "Wall time: 16.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                              trigram_dictionary, sort=False)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el83657204019843523185828\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el83657204019843523185828_data = {\"mdsDat\": {\"x\": [0.011143905583468495, -0.01698736351633523, -0.00033482783994106136, 0.006911478148367019, -0.0007331923755592263], \"y\": [-0.0004183800360298901, -0.0045101520750190065, -0.0010824326295014067, -0.008883108832738804, 0.014894073573289108], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.485776901245117, 21.14453125, 20.690431594848633, 19.94957733154297, 11.729681968688965]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [1321.0, 1265.0, 1517.0, 984.0, 1400.0, 538.0, 718.0, 803.0, 1418.0, 801.0, 423.0, 567.0, 913.0, 682.0, 921.0, 1683.0, 1013.0, 727.0, 1416.0, 826.0, 1335.0, 587.0, 469.0, 566.0, 1049.0, 693.0, 926.0, 400.0, 386.0, 442.0, 11.430745124816895, 6.501492023468018, 12.360570907592773, 7.342963695526123, 5.976726531982422, 96.81603240966797, 6.364297389984131, 8.047477722167969, 6.651736736297607, 8.843684196472168, 30.84814453125, 5.0804123878479, 23.471233367919922, 5.306797981262207, 16.481443405151367, 6.182516098022461, 5.311837673187256, 9.564943313598633, 5.2643327713012695, 25.408512115478516, 22.987722396850586, 5.760831356048584, 11.202978134155273, 6.387103080749512, 29.5895938873291, 8.37467098236084, 5.368636608123779, 56.8532829284668, 9.573328971862793, 7.268885612487793, 142.64910888671875, 35.43284606933594, 67.06747436523438, 172.15126037597656, 221.44131469726562, 160.07269287109375, 161.40428161621094, 465.9224853515625, 259.0859375, 40.91803741455078, 340.09716796875, 602.2416381835938, 392.6796875, 495.5767822265625, 257.447509765625, 78.73686218261719, 550.7266845703125, 237.04981994628906, 456.9629821777344, 201.43545532226562, 99.03672790527344, 267.35736083984375, 321.088623046875, 158.5079803466797, 142.80091857910156, 183.09854125976562, 120.15636444091797, 322.1669921875, 221.79237365722656, 443.03021240234375, 285.7012023925781, 246.1699981689453, 292.429443359375, 226.87327575683594, 386.3874206542969, 270.00634765625, 365.7406005859375, 320.3104248046875, 321.1336669921875, 219.97683715820312, 292.3861083984375, 279.65411376953125, 280.35809326171875, 10.956417083740234, 15.136321067810059, 6.348607063293457, 7.01135778427124, 5.9768967628479, 7.95532751083374, 15.535371780395508, 11.599770545959473, 8.372383117675781, 8.024826049804688, 6.588970184326172, 39.42599105834961, 7.5400590896606445, 6.996922492980957, 9.607866287231445, 6.09064245223999, 5.010710716247559, 8.283581733703613, 5.432545185089111, 10.638986587524414, 7.123000621795654, 18.65250015258789, 9.169783592224121, 17.021400451660156, 24.85984230041504, 5.518791675567627, 328.65576171875, 31.274080276489258, 5.4071197509765625, 44.95875930786133, 332.7325744628906, 33.638267517089844, 336.3236083984375, 46.01255416870117, 51.110294342041016, 53.333438873291016, 173.65846252441406, 284.34368896484375, 416.156005859375, 161.0265655517578, 227.1717071533203, 60.12006378173828, 74.39395141601562, 38.234580993652344, 175.7195281982422, 34.42774200439453, 186.45455932617188, 357.3378601074219, 80.84146881103516, 346.7313537597656, 347.9313049316406, 53.28654861450195, 110.79183959960938, 280.174072265625, 317.6932678222656, 325.1785583496094, 242.8608856201172, 94.8246078491211, 207.5907440185547, 302.0387878417969, 184.01947021484375, 195.86553955078125, 292.1167907714844, 169.51084899902344, 201.533935546875, 228.92599487304688, 128.0385284423828, 206.477783203125, 180.83462524414062, 139.87962341308594, 240.072021484375, 176.0928192138672, 165.50511169433594, 182.9606475830078, 166.84426879882812, 8.063498497009277, 12.444735527038574, 5.563347816467285, 6.345722198486328, 4.005357265472412, 10.805828094482422, 7.322457790374756, 5.301687240600586, 14.606014251708984, 6.44906759262085, 4.617188930511475, 4.956389427185059, 5.5914626121521, 6.074221611022949, 8.253729820251465, 20.14676856994629, 6.306580543518066, 14.33448314666748, 17.484878540039062, 6.086307048797607, 7.906676292419434, 4.759139537811279, 15.686378479003906, 74.01847076416016, 7.3537373542785645, 7.032500743865967, 28.44359588623047, 4.276683807373047, 5.3890485763549805, 4.1410417556762695, 133.63177490234375, 314.3782958984375, 64.0761947631836, 132.91055297851562, 34.5712890625, 440.6492004394531, 182.1903839111328, 221.78421020507812, 150.0210723876953, 169.51416015625, 62.753150939941406, 45.133636474609375, 44.81513595581055, 285.560546875, 186.87303161621094, 194.14700317382812, 372.2867736816406, 127.71136474609375, 106.19925689697266, 46.44919204711914, 234.45655822753906, 167.12606811523438, 152.16439819335938, 382.6324157714844, 298.2167663574219, 189.79934692382812, 232.26390075683594, 189.58377075195312, 101.41261291503906, 163.1529998779297, 273.092041015625, 255.35836791992188, 309.0505676269531, 290.57403564453125, 268.02734375, 276.3119812011719, 248.93829345703125, 184.9762420654297, 249.45236206054688, 193.7070770263672, 175.87295532226562, 181.2244110107422, 163.80062866210938, 15.91976547241211, 10.57763671875, 10.807873725891113, 583.7044677734375, 9.901422500610352, 4.393457889556885, 8.736263275146484, 5.997557640075684, 5.555962085723877, 6.802992343902588, 49.23931121826172, 4.201472282409668, 5.61128568649292, 4.986950874328613, 4.463072299957275, 8.303768157958984, 12.046682357788086, 5.082255840301514, 4.786317348480225, 11.571266174316406, 9.144929885864258, 42.786808013916016, 6.150380611419678, 23.029800415039062, 15.81849193572998, 6.299149036407471, 4.5276665687561035, 11.727530479431152, 8.367456436157227, 12.824562072753906, 81.71613311767578, 168.42431640625, 125.97103118896484, 32.82777786254883, 21.931777954101562, 283.2448425292969, 21.056333541870117, 50.670005798339844, 114.0395278930664, 87.21696472167969, 103.68868255615234, 355.30938720703125, 83.44424438476562, 262.7776794433594, 305.64935302734375, 22.121763229370117, 94.77249908447266, 108.67278289794922, 21.906768798828125, 378.5022888183594, 120.18048858642578, 93.78072357177734, 150.39773559570312, 337.9850769042969, 315.7628173828125, 273.2418518066406, 213.56954956054688, 297.57855224609375, 242.15005493164062, 226.1795196533203, 328.6531677246094, 151.35238647460938, 197.16395568847656, 166.95094299316406, 147.6448516845703, 273.1531066894531, 233.69918823242188, 196.23590087890625, 238.41664123535156, 147.3426055908203, 163.99392700195312, 148.25482177734375, 207.4586639404297, 149.41685485839844, 146.48521423339844, 166.90452575683594, 150.31399536132812, 5.3024773597717285, 6.32088041305542, 5.006011962890625, 7.334535598754883, 5.915136814117432, 3.326599597930908, 4.437312126159668, 3.9695467948913574, 4.340888500213623, 6.1749958992004395, 27.64261817932129, 3.5451388359069824, 7.576930046081543, 3.3129334449768066, 3.2024645805358887, 3.2454299926757812, 24.15894317626953, 4.456840515136719, 9.66705322265625, 3.8008360862731934, 4.064565658569336, 5.263088703155518, 126.22486114501953, 3.1685898303985596, 4.197540760040283, 2.6497108936309814, 33.31929397583008, 6.395484924316406, 3.1570305824279785, 8.786025047302246, 6.753481388092041, 9.748102188110352, 39.11891555786133, 138.76992797851562, 84.1382064819336, 27.477191925048828, 78.45348358154297, 108.59642791748047, 33.46773910522461, 262.4610290527344, 173.69435119628906, 32.42005920410156, 138.17437744140625, 225.3583526611328, 122.72307586669922, 69.48621368408203, 66.48792266845703, 113.31173706054688, 111.53302764892578, 188.33726501464844, 90.5662612915039, 93.15434265136719, 67.88168334960938, 211.07386779785156, 139.21861267089844, 66.74556732177734, 48.804447174072266, 135.01547241210938, 72.95027923583984, 180.2805633544922, 157.51373291015625, 77.32337951660156, 162.9393310546875, 149.6901397705078, 111.01608276367188, 69.29564666748047, 104.38446807861328, 132.05738830566406, 83.86042022705078, 111.0094985961914, 122.4449462890625, 110.7913818359375, 100.5484848022461, 88.02652740478516, 89.39479064941406, 85.6746597290039], \"Term\": [\"internet\", \"spend_time\", \"reason\", \"website\", \"life\", \"have\", \"student\", \"outside\", \"day\", \"example\", \"hand_eye_coordination\", \"able\", \"not\", \"person\", \"game\", \"online\", \"exercise\", \"important\", \"find\", \"play\", \"world\", \"instead\", \"sit\", \"new\", \"get\", \"say\", \"bad\", \"right\", \"hour\", \"make\", \"address\", \"comfortable\", \"collage\", \"power_point\", \"newpaper\", \"teach_hand_eye_coordination\", \"reaction\", \"give_ability\", \"material\", \"dear_local\", \"ability_learn\", \"professional\", \"hit\", \"give_ability_learn\", \"bear\", \"technique\", \"aol\", \"spell\", \"throught\", \"past\", \"ball\", \"quickly_easily\", \"nearly\", \"vaction\", \"virus\", \"compter\", \"stressful\", \"history\", \"eachother\", \"chore\", \"teacher\", \"drive\", \"ability\", \"hand_eye_coordination\", \"homework\", \"teach\", \"happen\", \"look\", \"effect\", \"plus\", \"place\", \"online\", \"school\", \"find\", \"student\", \"fast\", \"learn\", \"society\", \"kid\", \"job\", \"ask\", \"allow\", \"work\", \"away\", \"lastly\", \"home\", \"benefit\", \"get\", \"say\", \"want\", \"lot\", \"type\", \"exercise\", \"great\", \"go\", \"website\", \"reason\", \"world\", \"day\", \"child\", \"internet\", \"information\", \"life\", \"exercis\", \"depression\", \"toll\", \"kick\", \"hire\", \"burn_calorie\", \"jog\", \"eyesight\", \"somthe\", \"hurt_eye\", \"play_baseball\", \"brother\", \"teach_hand_eye\", \"daily_basis\", \"eye_sight\", \"sudden\", \"bus\", \"extremly\", \"bike_ride\", \"exercise_enjoy_nature_interact\", \"till\", \"drug\", \"possitive_effect\", \"ignore\", \"excersize\", \"xbox\", \"outside\", \"animal\", \"eat_junk_food\", \"spend_hour\", \"play\", \"park\", \"game\", \"happy\", \"experience\", \"unhealthy\", \"spend\", \"People\", \"spend_time\", \"sit\", \"fun\", \"addict\", \"affect\", \"inside\", \"chat\", \"year_old\", \"technology\", \"day\", \"enjoy\", \"kid\", \"life\", \"dear_local_newspaper\", \"interact\", \"information\", \"go\", \"reason\", \"get\", \"nature\", \"bad\", \"want\", \"allow\", \"lot\", \"online\", \"example\", \"school\", \"world\", \"research\", \"look\", \"website\", \"believe\", \"learn\", \"work\", \"not\", \"internet\", \"exercise\", \"live_faraway\", \"time_consume\", \"send_e_mail\", \"completly\", \"push_button\", \"plant\", \"tip\", \"tag\", \"vision\", \"vast\", \"house_hold\", \"positive_negative_effect\", \"gain_knowledge\", \"nature_beautiful\", \"plenty\", \"social_skill\", \"shy\", \"team\", \"positive_affect\", \"awesome\", \"spot\", \"claim\", \"sight\", \"positive\", \"degree\", \"cause_obesity\", \"addicting\", \"quote\", \"neighborhood\", \"teaching\", \"cause\", \"bad\", \"adult\", \"different\", \"user\", \"day\", \"instead\", \"great\", \"positive_effect\", \"new\", \"show\", \"move\", \"letter\", \"exercise\", \"let\", \"person\", \"find\", \"make\", \"country\", \"human\", \"game\", \"believe\", \"communicate\", \"learn\", \"spend_time\", \"type\", \"work\", \"example\", \"problem\", \"easy\", \"world\", \"information\", \"want\", \"reason\", \"kid\", \"go\", \"life\", \"not\", \"online\", \"look\", \"lot\", \"internet\", \"allow\", \"wanna\", \"schoolwork\", \"soldier\", \"internet\", \"shop_online\", \"watch_tv\", \"poeple\", \"army\", \"airport\", \"thay\", \"newspaper\", \"fool\", \"someone\", \"radio\", \"pro_con\", \"normal\", \"watch_video\", \"spend_time_outdoors\", \"customer\", \"usefull\", \"acess\", \"tool\", \"correctly\", \"shop\", \"argument\", \"peer\", \"worldwide\", \"u\", \"accurate\", \"abuse\", \"news\", \"tell\", \"communication\", \"beneficial\", \"cell_phone\", \"not\", \"bully\", \"school_work\", \"problem\", \"try\", \"play_game\", \"world\", \"buy\", \"website\", \"information\", \"practice\", \"fact\", \"project\", \"head\", \"learn\", \"read\", \"@PERSON1\", \"live\", \"want\", \"go\", \"look\", \"lot\", \"life\", \"school\", \"get\", \"online\", \"let\", \"place\", \"child\", \"believe\", \"reason\", \"spend_time\", \"work\", \"find\", \"society\", \"People\", \"fun\", \"kid\", \"important\", \"great\", \"day\", \"bad\", \"credit_card\", \"reflex\", \"children\", \"hang_friend_family\", \"tech\", \"pose\", \"othe\", \"rumor\", \"natural_disaster\", \"law\", \"will_not\", \"spell_check\", \"memory\", \"eat_junk_food\", \"take_care\", \"fail_class\", \"benifit\", \"will_able\", \"conclude\", \"leave_home\", \"long_period_time\", \"helpfull\", \"have\", \"react\", \"neighbor\", \"private\", \"second_reason\", \"percent\", \"page_essay\", \"dictionary\", \"crime\", \"party\", \"activity\", \"student\", \"hand_eye_coordination\", \"access\", \"right\", \"able\", \"lazy\", \"reason\", \"website\", \"learn_new\", \"example\", \"life\", \"important\", \"hour\", \"give\", \"say\", \"person\", \"spend_time\", \"new\", \"instead\", \"start\", \"online\", \"exercise\", \"opinion\", \"mean\", \"get\", \"sit\", \"want\", \"world\", \"dear\", \"go\", \"find\", \"not\", \"away\", \"place\", \"day\", \"technology\", \"information\", \"learn\", \"kid\", \"school\", \"outside\", \"look\", \"work\"], \"Total\": [1321.0, 1265.0, 1517.0, 984.0, 1400.0, 538.0, 718.0, 803.0, 1418.0, 801.0, 423.0, 567.0, 913.0, 682.0, 921.0, 1683.0, 1013.0, 727.0, 1416.0, 826.0, 1335.0, 587.0, 469.0, 566.0, 1049.0, 693.0, 926.0, 400.0, 386.0, 442.0, 21.56943130493164, 12.414937973022461, 23.7750244140625, 14.136963844299316, 12.264032363891602, 199.85256958007812, 13.196868896484375, 16.68840217590332, 13.831567764282227, 18.497358322143555, 65.83228302001953, 10.842844009399414, 50.303443908691406, 11.414472579956055, 35.75699234008789, 13.950246810913086, 12.040178298950195, 21.7296199798584, 11.996275901794434, 58.374267578125, 52.95527648925781, 13.284601211547852, 25.846782684326172, 14.784673690795898, 68.68597412109375, 19.571866989135742, 12.554434776306152, 133.2932891845703, 22.5101261138916, 17.0941219329834, 336.7071533203125, 83.35535430908203, 161.69358825683594, 423.93707275390625, 553.5221557617188, 396.2790222167969, 401.89959716796875, 1228.7440185546875, 673.5318603515625, 98.97499084472656, 920.8524780273438, 1683.537841796875, 1092.925537109375, 1416.2791748046875, 718.4112548828125, 200.32913208007812, 1674.37841796875, 674.8859252929688, 1389.9716796875, 571.5606079101562, 265.6333312988281, 815.6611938476562, 1011.35595703125, 454.1384582519531, 404.1477966308594, 537.5457153320312, 333.094482421875, 1049.71728515625, 693.61669921875, 1572.3851318359375, 938.7233276367188, 786.4476318359375, 1013.2769775390625, 740.0411376953125, 1459.0948486328125, 984.2733154296875, 1517.107421875, 1335.151611328125, 1418.0826416015625, 741.514892578125, 1321.2255859375, 1231.845458984375, 1400.1646728515625, 21.16122055053711, 30.12693977355957, 12.923210144042969, 14.395647048950195, 12.640684127807617, 16.896820068359375, 33.01955795288086, 24.888015747070312, 18.274662017822266, 17.7268009185791, 14.769914627075195, 89.8892822265625, 17.19717788696289, 15.993721008300781, 21.986419677734375, 14.079639434814453, 11.629072189331055, 19.39021110534668, 12.84391975402832, 25.19537925720215, 16.9417667388916, 44.52405548095703, 21.930173873901367, 40.87114715576172, 60.51807403564453, 13.464529037475586, 803.7924194335938, 76.61663055419922, 13.341182708740234, 111.16625213623047, 826.5804443359375, 83.31672668457031, 921.8778686523438, 116.61695861816406, 132.624755859375, 139.05931091308594, 485.8382263183594, 825.6563720703125, 1265.6573486328125, 469.8168640136719, 699.2481689453125, 164.94366455078125, 211.13841247558594, 101.68939208984375, 579.24072265625, 90.88436889648438, 649.6573486328125, 1418.0826416015625, 250.34024047851562, 1389.9716796875, 1400.1646728515625, 154.3084716796875, 376.7203369140625, 1231.845458984375, 1459.0948486328125, 1517.107421875, 1049.71728515625, 322.1109619140625, 926.841552734375, 1572.3851318359375, 815.6611938476562, 938.7233276367188, 1683.537841796875, 801.2064208984375, 1092.925537109375, 1335.151611328125, 549.989990234375, 1228.7440185546875, 984.2733154296875, 637.5897827148438, 1674.37841796875, 1011.35595703125, 913.337158203125, 1321.2255859375, 1013.2769775390625, 17.367218017578125, 28.994827270507812, 13.189695358276367, 15.510332107543945, 9.982595443725586, 28.05738067626953, 19.022916793823242, 13.820975303649902, 38.138519287109375, 16.849761962890625, 12.126480102539062, 13.044373512268066, 14.950532913208008, 16.452484130859375, 22.416704177856445, 54.899742126464844, 17.27741050720215, 39.31731033325195, 48.39997100830078, 16.86697006225586, 21.993650436401367, 13.312331199645996, 43.879722595214844, 207.17153930664062, 20.601638793945312, 19.886714935302734, 80.55296325683594, 12.202864646911621, 15.476004600524902, 11.909719467163086, 389.195068359375, 926.841552734375, 189.8115234375, 406.9975891113281, 102.09449768066406, 1418.0826416015625, 587.4732666015625, 740.0411376953125, 495.9671936035156, 566.5961303710938, 197.46934509277344, 138.8819122314453, 138.26449584960938, 1013.2769775390625, 645.1253662109375, 682.3563232421875, 1416.2791748046875, 442.7252502441406, 363.3571472167969, 146.6667938232422, 921.8778686523438, 637.5897827148438, 572.5626831054688, 1674.37841796875, 1265.6573486328125, 786.4476318359375, 1011.35595703125, 801.2064208984375, 376.0764465332031, 696.0795288085938, 1335.151611328125, 1231.845458984375, 1572.3851318359375, 1517.107421875, 1389.9716796875, 1459.0948486328125, 1400.1646728515625, 913.337158203125, 1683.537841796875, 1228.7440185546875, 938.7233276367188, 1321.2255859375, 815.6611938476562, 32.21083068847656, 23.03746223449707, 24.03888511657715, 1321.2255859375, 23.620464324951172, 10.778223991394043, 21.79522705078125, 15.089393615722656, 13.983278274536133, 17.16437530517578, 124.82093048095703, 10.663701057434082, 14.49304485321045, 12.991997718811035, 11.721261024475098, 22.080997467041016, 32.03672409057617, 13.529284477233887, 12.818252563476562, 31.363481521606445, 24.788599014282227, 116.38257598876953, 16.82573699951172, 63.10526657104492, 43.36878967285156, 17.29920768737793, 12.47389030456543, 32.42009735107422, 23.161346435546875, 35.653663635253906, 230.43817138671875, 481.856201171875, 367.43829345703125, 93.80311584472656, 62.30596923828125, 913.337158203125, 59.97407150268555, 152.14378356933594, 376.0764465332031, 281.72784423828125, 344.9087829589844, 1335.151611328125, 273.81072998046875, 984.2733154296875, 1231.845458984375, 64.96273040771484, 337.3879699707031, 395.5089111328125, 64.7626724243164, 1674.37841796875, 455.6393127441406, 342.9592590332031, 602.0228881835938, 1572.3851318359375, 1459.0948486328125, 1228.7440185546875, 938.7233276367188, 1400.1646728515625, 1092.925537109375, 1049.71728515625, 1683.537841796875, 645.1253662109375, 920.8524780273438, 741.514892578125, 637.5897827148438, 1517.107421875, 1265.6573486328125, 1011.35595703125, 1416.2791748046875, 674.8859252929688, 825.6563720703125, 699.2481689453125, 1389.9716796875, 727.9706420898438, 740.0411376953125, 1418.0826416015625, 926.841552734375, 15.510494232177734, 19.890745162963867, 16.09942054748535, 24.364248275756836, 20.696895599365234, 11.7506103515625, 15.77294635772705, 14.383955001831055, 15.753610610961914, 23.23758316040039, 106.27356719970703, 13.669635772705078, 29.949195861816406, 13.341182708740234, 12.931451797485352, 13.111186981201172, 99.09485626220703, 18.379993438720703, 39.942962646484375, 15.77402114868164, 16.905241012573242, 22.036964416503906, 538.2271118164062, 13.528928756713867, 17.942411422729492, 11.330060005187988, 144.53225708007812, 27.824583053588867, 13.786275863647461, 38.384559631347656, 29.62264060974121, 43.355560302734375, 188.10064697265625, 718.4112548828125, 423.93707275390625, 130.0206756591797, 400.15264892578125, 567.6904907226562, 162.57708740234375, 1517.107421875, 984.2733154296875, 162.35926818847656, 801.2064208984375, 1400.1646728515625, 727.9706420898438, 386.7403564453125, 371.368896484375, 693.61669921875, 682.3563232421875, 1265.6573486328125, 566.5961303710938, 587.4732666015625, 410.5993957519531, 1683.537841796875, 1013.2769775390625, 407.8783264160156, 277.882568359375, 1049.71728515625, 469.8168640136719, 1572.3851318359375, 1335.151611328125, 515.0162963867188, 1459.0948486328125, 1416.2791748046875, 913.337158203125, 454.1384582519531, 920.8524780273438, 1418.0826416015625, 649.6573486328125, 1231.845458984375, 1674.37841796875, 1389.9716796875, 1092.925537109375, 803.7924194335938, 1228.7440185546875, 1011.35595703125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6935999989509583, 0.6816999912261963, 0.6743999719619751, 0.6735000014305115, 0.6097999811172485, 0.6037999987602234, 0.5993000268936157, 0.5992000102996826, 0.5964999794960022, 0.5906000137329102, 0.5705000162124634, 0.5703999996185303, 0.5662999749183655, 0.5626999735832214, 0.554099977016449, 0.5148000121116638, 0.5102999806404114, 0.5080000162124634, 0.5048999786376953, 0.4968000054359436, 0.49410000443458557, 0.49300000071525574, 0.4925999939441681, 0.489300012588501, 0.4864000082015991, 0.4796999990940094, 0.47909998893737793, 0.476500004529953, 0.47360000014305115, 0.4733999967575073, 0.46970000863075256, 0.4731000065803528, 0.44859999418258667, 0.42739999294281006, 0.4124000072479248, 0.4221000075340271, 0.4162999987602234, 0.3587999939918518, 0.373199999332428, 0.44530001282691956, 0.33250001072883606, 0.30059999227523804, 0.30489999055862427, 0.2784999907016754, 0.30230000615119934, 0.39469999074935913, 0.21660000085830688, 0.2822999954223633, 0.21610000729560852, 0.2856999933719635, 0.3418999910354614, 0.21310000121593475, 0.18119999766349792, 0.2759999930858612, 0.2881999909877777, 0.2515999972820282, 0.30889999866485596, 0.14740000665187836, 0.188400000333786, 0.061900001019239426, 0.13899999856948853, 0.1670999974012375, 0.08579999953508377, 0.1462000012397766, -0.00019999999494757503, 0.035100001841783524, -0.0940999984741211, -0.09889999777078629, -0.1565999984741211, 0.11339999735355377, -0.17970000207424164, -0.1542000025510788, -0.27970001101493835, 0.8955000042915344, 0.8654999732971191, 0.8429999947547913, 0.8343999981880188, 0.8047999739646912, 0.8004999756813049, 0.7997999787330627, 0.7904000282287598, 0.7731999754905701, 0.7613000273704529, 0.7465999722480774, 0.7296000123023987, 0.7293000221252441, 0.7271000146865845, 0.7258999943733215, 0.7157999873161316, 0.711899995803833, 0.7032999992370605, 0.6933000087738037, 0.6916999816894531, 0.6873000264167786, 0.6837000250816345, 0.6818000078201294, 0.6777999997138977, 0.6640999913215637, 0.661899983882904, 0.659500002861023, 0.657800018787384, 0.650600016117096, 0.6485000252723694, 0.6438000202178955, 0.6467999815940857, 0.5454000234603882, 0.6237999796867371, 0.6003000140190125, 0.5954999923706055, 0.5249999761581421, 0.4878000020980835, 0.4415000081062317, 0.4830000102519989, 0.429500013589859, 0.5444999933242798, 0.5105999708175659, 0.5756000280380249, 0.3610000014305115, 0.5831000208854675, 0.30550000071525574, 0.1754000037908554, 0.4235000014305115, 0.16529999673366547, 0.16140000522136688, 0.49050000309944153, 0.32989999651908875, 0.07289999723434448, 0.02930000051856041, 0.01360000018030405, 0.09000000357627869, 0.33090001344680786, 0.0575999990105629, -0.09600000083446503, 0.06480000168085098, -0.013299999758601189, -0.19769999384880066, 0.0006000000284984708, -0.13689999282360077, -0.20960000157356262, 0.09619999676942825, -0.2298000007867813, -0.1404999941587448, 0.03689999878406525, -0.38850000500679016, -0.19419999420642853, -0.1543000042438507, -0.42329999804496765, -0.2500999867916107, 0.8083000183105469, 0.7297000288963318, 0.7123000025749207, 0.6818000078201294, 0.6622999906539917, 0.6212999820709229, 0.6208000183105469, 0.6172999739646912, 0.6157000064849854, 0.6151000261306763, 0.6098999977111816, 0.6078000068664551, 0.5920000076293945, 0.5791000127792358, 0.5763999819755554, 0.5730000138282776, 0.5677000284194946, 0.5665000081062317, 0.5572999715805054, 0.5562000274658203, 0.5525000095367432, 0.5468999743461609, 0.5468000173568726, 0.5462999939918518, 0.5453000068664551, 0.5360000133514404, 0.534500002861023, 0.5270000100135803, 0.5206000208854675, 0.51910001039505, 0.5065000057220459, 0.4943000078201294, 0.4894999861717224, 0.4564000070095062, 0.4925999939441681, 0.4066999852657318, 0.40470001101493835, 0.37049999833106995, 0.3797999918460846, 0.36880001425743103, 0.42910000681877136, 0.4514999985694885, 0.4489000141620636, 0.3089999854564667, 0.33649998903274536, 0.31859999895095825, 0.2393999993801117, 0.33230000734329224, 0.34540000557899475, 0.42570000886917114, 0.20640000700950623, 0.23659999668598175, 0.25029999017715454, 0.09939999878406525, 0.12999999523162842, 0.15389999747276306, 0.10429999977350235, 0.13420000672340393, 0.26489999890327454, 0.12470000237226486, -0.011500000022351742, 0.0019000000320374966, -0.05130000039935112, -0.07720000296831131, -0.07050000131130219, -0.0885000005364418, -0.15160000324249268, -0.021400000900030136, -0.33390000462532043, -0.2718999981880188, -0.09929999709129333, -0.41110000014305115, -0.029899999499320984, 0.9071999788284302, 0.8335999846458435, 0.8126000165939331, 0.7950000166893005, 0.7425000071525574, 0.7146000266075134, 0.6977999806404114, 0.689300000667572, 0.6890000104904175, 0.6865000128746033, 0.6818000078201294, 0.6805999875068665, 0.663100004196167, 0.6545000076293945, 0.646399974822998, 0.6340000033378601, 0.633899986743927, 0.6328999996185303, 0.6269000172615051, 0.614799976348877, 0.614799976348877, 0.611299991607666, 0.6055999994277954, 0.6039000153541565, 0.6033999919891357, 0.6017000079154968, 0.5985000133514404, 0.5950999855995178, 0.5938000082969666, 0.5895000100135803, 0.5752000212669373, 0.5608000159263611, 0.5414999723434448, 0.5619999766349792, 0.567799985408783, 0.44119998812675476, 0.5652999877929688, 0.512499988079071, 0.4187000095844269, 0.43939998745918274, 0.4101000130176544, 0.2881999909877777, 0.4237000048160553, 0.2913999855518341, 0.21809999644756317, 0.5346999764442444, 0.34220001101493835, 0.32010000944137573, 0.527999997138977, 0.125, 0.2793000042438507, 0.31529998779296875, 0.22499999403953552, 0.07460000365972519, 0.08139999955892563, 0.10859999805688858, 0.131400004029274, 0.0632999986410141, 0.10490000247955322, 0.07699999958276749, -0.021700000390410423, 0.16210000216960907, 0.07069999724626541, 0.12099999934434891, 0.14910000562667847, -0.10260000079870224, -0.07729999721050262, -0.027799999341368675, -0.16979999840259552, 0.09019999951124191, -0.004399999976158142, 0.0608999989926815, -0.29010000824928284, 0.0284000001847744, -0.007799999788403511, -0.5277000069618225, -0.2071000039577484, 1.069700002670288, 0.9966999888420105, 0.9749000072479248, 0.9424999952316284, 0.8906000256538391, 0.8810999989509583, 0.8748000264167786, 0.8555999994277954, 0.8540999889373779, 0.817799985408783, 0.7964000105857849, 0.79339998960495, 0.7687000036239624, 0.75, 0.7473000288009644, 0.7468000054359436, 0.7315999865531921, 0.7261999845504761, 0.7243000268936157, 0.7199000120162964, 0.7177000045776367, 0.7110000252723694, 0.692799985408783, 0.6915000081062317, 0.6904000043869019, 0.6899999976158142, 0.6757000088691711, 0.6726999878883362, 0.6690000295639038, 0.6686000227928162, 0.6646000146865845, 0.6506999731063843, 0.572700023651123, 0.49880000948905945, 0.5259000062942505, 0.588699996471405, 0.513700008392334, 0.48910000920295715, 0.5625, 0.3885999917984009, 0.4083999991416931, 0.5320000052452087, 0.3853999972343445, 0.3163999915122986, 0.3626999855041504, 0.42640000581741333, 0.42289999127388, 0.3312999904155731, 0.33180001378059387, 0.2379000037908554, 0.3095000088214874, 0.30149999260902405, 0.3431999981403351, 0.066600002348423, 0.15809999406337738, 0.3330000042915344, 0.40369999408721924, 0.09220000356435776, 0.28049999475479126, -0.02280000038444996, 0.005799999926239252, 0.2468000054359436, -0.04910000041127205, -0.10419999808073044, 0.035599999129772186, 0.2630000114440918, -0.03420000150799751, -0.23080000281333923, 0.0957999974489212, -0.2635999917984009, -0.4724999964237213, -0.3862999975681305, -0.24289999902248383, -0.06870000064373016, -0.47760000824928284, -0.3253999948501587], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.192299842834473, -8.756600379943848, -8.114100456237793, -8.634900093078613, -8.840700149536133, -6.055799961090088, -8.777899742126465, -8.54319953918457, -8.733699798583984, -8.44890022277832, -7.19950008392334, -9.003199577331543, -7.472799777984619, -8.959600448608398, -7.826399803161621, -8.806900024414062, -8.958700180053711, -8.370499610900879, -8.967700004577637, -7.393499851226807, -7.493599891662598, -8.8774995803833, -8.212400436401367, -8.774299621582031, -7.241199970245361, -8.503399848937988, -8.947999954223633, -6.588099956512451, -8.369600296020508, -8.645000457763672, -5.6682000160217285, -7.060999870300293, -6.422900199890137, -5.480199813842773, -5.228400230407715, -5.552999973297119, -5.5447001457214355, -4.484600067138672, -5.071400165557861, -6.916999816894531, -4.7993998527526855, -4.22790002822876, -4.655600070953369, -4.422900199890137, -5.0777997970581055, -6.262499809265137, -4.317399978637695, -5.160299777984619, -4.504000186920166, -5.3231000900268555, -6.033100128173828, -5.039999961853027, -4.856900215148926, -5.56279993057251, -5.667200088500977, -5.418600082397461, -5.839799880981445, -4.853499889373779, -5.226900100708008, -4.534999847412109, -4.973700046539307, -5.122600078582764, -4.950399875640869, -5.20419979095459, -4.671800136566162, -5.030200004577637, -4.7266998291015625, -4.859300136566162, -4.8566999435424805, -5.235099792480469, -4.950500011444092, -4.995100021362305, -4.992499828338623, -8.009499549865723, -7.686299800872803, -8.555100440979004, -8.45580005645752, -8.615500450134277, -8.329500198364258, -7.660299777984619, -7.952400207519531, -8.278400421142578, -8.320799827575684, -8.517999649047852, -6.729000091552734, -8.383199691772461, -8.457900047302246, -8.140800476074219, -8.596599578857422, -8.791799545288086, -8.28909969329834, -8.711000442504883, -8.038900375366211, -8.440099716186523, -7.477399826049805, -8.1875, -7.568900108337402, -7.190100193023682, -8.695199966430664, -4.608399868011475, -6.960599899291992, -8.715700149536133, -6.597599983215332, -4.5960001945495605, -6.887700080871582, -4.585299968719482, -6.57450008392334, -6.469399929046631, -6.426799774169922, -5.246300220489502, -4.753200054168701, -4.372300148010254, -5.321800231933594, -4.977700233459473, -6.307000160217285, -6.093999862670898, -6.7596001625061035, -5.234499931335449, -6.864500045776367, -5.17519998550415, -4.524700164794922, -6.010900020599365, -4.554800033569336, -4.551400184631348, -6.427700042724609, -5.695700168609619, -4.76800012588501, -4.642300128936768, -4.61899995803833, -4.910900115966797, -5.851399898529053, -5.067800045013428, -4.692800045013428, -5.188300132751465, -5.125999927520752, -4.726200103759766, -5.270500183105469, -5.097400188446045, -4.96999979019165, -5.551000118255615, -5.073200225830078, -5.2058000564575195, -5.462600231170654, -4.922399997711182, -5.232399940490723, -5.294400215148926, -5.1940999031066895, -5.286300182342529, -8.294300079345703, -7.860400199890137, -8.665499687194824, -8.533900260925293, -8.994000434875488, -8.00160026550293, -8.390700340270996, -8.713600158691406, -7.700200080871582, -8.5177001953125, -8.851900100708008, -8.781000137329102, -8.660400390625, -8.57759952545166, -8.270999908447266, -7.378600120544434, -8.54010009765625, -7.718999862670898, -7.520299911499023, -8.575599670410156, -8.314000129699707, -8.821599960327148, -7.628900051116943, -6.077400207519531, -8.386500358581543, -8.431099891662598, -7.033699989318848, -8.928500175476074, -8.69729995727539, -8.960700035095215, -5.486599922180176, -4.631100177764893, -6.22160005569458, -5.492000102996826, -6.838600158691406, -4.293399810791016, -5.176599979400635, -4.980000019073486, -5.3709001541137695, -5.248700141906738, -6.242499828338623, -6.572000026702881, -6.579100131988525, -4.727200031280518, -5.151199817657471, -5.113100051879883, -4.461999893188477, -5.531899929046631, -5.716400146484375, -6.543300151824951, -4.9243998527526855, -5.262899875640869, -5.3566999435424805, -4.434599876403809, -4.683800220489502, -5.135700225830078, -4.933800220489502, -5.1367998123168945, -5.762499809265137, -5.2870001792907715, -4.771900177001953, -4.839000225067139, -4.648200035095215, -4.709799766540527, -4.790599822998047, -4.7600998878479, -4.864500045776367, -5.161399841308594, -4.862400054931641, -5.115300178527832, -5.211900234222412, -5.1819000244140625, -5.2829999923706055, -7.577600002288818, -7.986499786376953, -7.964900016784668, -3.975800037384033, -8.052499771118164, -8.865099906921387, -8.17770004272461, -8.553899765014648, -8.63029956817627, -8.427800178527832, -6.448500156402588, -8.909799575805664, -8.620400428771973, -8.73840045928955, -8.84939956665039, -8.228500366210938, -7.856400012969971, -8.719499588012695, -8.779399871826172, -7.896699905395508, -8.131999969482422, -6.589000225067139, -8.52869987487793, -7.208399772644043, -7.584000110626221, -8.504799842834473, -8.835000038146973, -7.883299827575684, -8.22089958190918, -7.793799877166748, -5.941999912261963, -5.218699932098389, -5.509200096130371, -6.853899955749512, -7.257299900054932, -4.69890022277832, -7.297999858856201, -6.419899940490723, -5.608699798583984, -5.876800060272217, -5.703800201416016, -4.4721999168396, -5.921000003814697, -4.773900032043457, -4.622799873352051, -7.248600006103516, -5.793700218200684, -5.656899929046631, -7.258399963378906, -4.408999919891357, -5.55620002746582, -5.804200172424316, -5.331900119781494, -4.522200107574463, -4.590199947357178, -4.734799861907959, -4.981200218200684, -4.649499893188477, -4.855599880218506, -4.923900127410889, -4.55019998550415, -5.3256001472473145, -5.061200141906738, -5.227499961853027, -5.350399971008301, -4.735199928283691, -4.891200065612793, -5.065899848937988, -4.871200084686279, -5.352399826049805, -5.2453999519348145, -5.34630012512207, -5.010300159454346, -5.338500022888184, -5.35830020904541, -5.227799892425537, -5.332499980926514, -8.145899772644043, -7.970300197601318, -8.203499794006348, -7.821499824523926, -8.036600112915039, -8.612199783325195, -8.32409954071045, -8.435500144958496, -8.345999717712402, -7.993599891662598, -6.494800090789795, -8.548500061035156, -7.789000034332275, -8.616299629211426, -8.650199890136719, -8.636899948120117, -6.629499912261963, -8.319700241088867, -7.545400142669678, -8.478899955749512, -8.411800384521484, -8.153400421142578, -4.976099967956543, -8.660799980163574, -8.379599571228027, -8.839699745178223, -6.308000087738037, -7.958499908447266, -8.66450023651123, -7.640999794006348, -7.904099941253662, -7.5370001792907715, -6.147500038146973, -4.88129997253418, -5.381700038909912, -6.500800132751465, -5.451600074768066, -5.126500129699707, -6.303500175476074, -4.24399995803833, -4.656799793243408, -6.335299968719482, -4.8856000900268555, -4.396399974822998, -5.004199981689453, -5.572999954223633, -5.617099761962891, -5.084000110626221, -5.099800109863281, -4.575900077819824, -5.308000087738037, -5.279900074005127, -5.596399784088135, -4.461900234222412, -4.8780999183654785, -5.6132001876831055, -5.926300048828125, -4.908699989318848, -5.5243000984191895, -4.61959981918335, -4.7546000480651855, -5.466100215911865, -4.720699787139893, -4.805600166320801, -5.104400157928467, -5.575699806213379, -5.165999889373779, -4.9309000968933105, -5.385000228881836, -5.104499816894531, -5.006499767303467, -5.106500148773193, -5.203499794006348, -5.33650016784668, -5.321100234985352, -5.36359977722168]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.22160066664218903, 0.1778636872768402, 0.23909544944763184, 0.27408504486083984, 0.08747394382953644, 0.20589679479599, 0.3439687490463257, 0.18167364597320557, 0.19862984120845795, 0.06903598457574844, 0.4143639802932739, 0.16079796850681305, 0.10513713210821152, 0.21027426421642303, 0.11132166534662247, 0.47089359164237976, 0.2278517335653305, 0.12152092903852463, 0.060760464519262314, 0.12152092903852463, 0.2871282994747162, 0.12330662459135056, 0.17086775600910187, 0.22723649442195892, 0.19200603663921356, 0.19633325934410095, 0.14023804664611816, 0.14023804664611816, 0.3646188974380493, 0.16828565299510956, 0.23842361569404602, 0.17689494788646698, 0.13843952119350433, 0.2307325303554535, 0.2076592743396759, 0.21587690711021423, 0.12952615320682526, 0.17270153760910034, 0.3454030752182007, 0.12952615320682526, 0.16136451065540314, 0.16136451065540314, 0.20170563459396362, 0.36307013034820557, 0.12102337926626205, 0.1967032104730606, 0.18075428903102875, 0.20201951265335083, 0.21265211701393127, 0.20733581483364105, 0.18188028037548065, 0.3637605607509613, 0.16975492238998413, 0.13944154977798462, 0.14550422132015228, 0.13655611872673035, 0.21104127168655396, 0.3475973904132843, 0.19862708449363708, 0.08689934760332108, 0.5099809765815735, 0.13908572494983673, 0.09272381663322449, 0.13908572494983673, 0.09272381663322449, 0.13170960545539856, 0.22654052078723907, 0.3371765911579132, 0.18966183066368103, 0.11590445041656494, 0.19418542087078094, 0.3504810035228729, 0.1373506635427475, 0.13261443376541138, 0.1847129613161087, 0.14302797615528107, 0.07151398807764053, 0.2145419716835022, 0.4290839433670044, 0.14302797615528107, 0.32734179496765137, 0.22558386623859406, 0.201063871383667, 0.1642839014530182, 0.0821419507265091, 0.20883195102214813, 0.40461188554763794, 0.195779949426651, 0.09136397391557693, 0.09136397391557693, 0.4152762293815613, 0.08305525034666061, 0.08305525034666061, 0.33222100138664246, 0.08305525034666061, 0.13834834098815918, 0.1844644546508789, 0.1844644546508789, 0.3689289093017578, 0.11529028415679932, 0.19881515204906464, 0.19881515204906464, 0.06627171486616135, 0.3976303040981293, 0.1325434297323227, 0.372694194316864, 0.19952315092086792, 0.18070021271705627, 0.15434810519218445, 0.09411469846963882, 0.35011348128318787, 0.21359124779701233, 0.1387242078781128, 0.14533013105392456, 0.15193603932857513, 0.17786242067813873, 0.17786242067813873, 0.35572484135627747, 0.11857493966817856, 0.11857493966817856, 0.20068155229091644, 0.22441807389259338, 0.33878499269485474, 0.16183996200561523, 0.07444638013839722, 0.43432876467704773, 0.18883860111236572, 0.15107087790966034, 0.11330315470695496, 0.11330315470695496, 0.4474649131298065, 0.16779935359954834, 0.16779935359954834, 0.13983279466629028, 0.08389967679977417, 0.19761922955513, 0.21957692503929138, 0.2619239091873169, 0.23212416470050812, 0.08939917385578156, 0.2451944202184677, 0.1705700308084488, 0.13858814537525177, 0.35180068016052246, 0.09594564139842987, 0.3602581322193146, 0.1951398253440857, 0.18613336980342865, 0.15310971438884735, 0.10507529228925705, 0.2623748779296875, 0.11100475490093231, 0.24219219386577606, 0.1513701230287552, 0.24219219386577606, 0.2335735559463501, 0.3892892599105835, 0.1557157039642334, 0.1557157039642334, 0.0778578519821167, 0.16687195003032684, 0.43386706709861755, 0.20024634897708893, 0.13349756598472595, 0.06674878299236298, 0.21676033735275269, 0.21676033735275269, 0.16673871874809265, 0.35015130043029785, 0.050021614879369736, 0.11836546659469604, 0.4734618663787842, 0.17754819989204407, 0.11836546659469604, 0.11836546659469604, 0.1719827651977539, 0.42995691299438477, 0.1719827651977539, 0.1719827651977539, 0.1719827651977539, 0.20086868107318878, 0.19721652567386627, 0.2045208364725113, 0.303129106760025, 0.09495609998703003, 0.15930314362049103, 0.236385315656662, 0.34430035948753357, 0.17985840141773224, 0.08222097903490067, 0.15085448324680328, 0.20113930106163025, 0.35199376940727234, 0.2514241337776184, 0.05028482526540756, 0.272847056388855, 0.09629896283149719, 0.19259792566299438, 0.3530961871147156, 0.09629896283149719, 0.24514850974082947, 0.3038460314273834, 0.1484702229499817, 0.19508296251296997, 0.10703667253255844, 0.2966899275779724, 0.19689422845840454, 0.16992241144180298, 0.2252146303653717, 0.11058443039655685, 0.24845615029335022, 0.062114037573337555, 0.062114037573337555, 0.24845615029335022, 0.3105701804161072, 0.4094974994659424, 0.17549891769886017, 0.11699928343296051, 0.17549891769886017, 0.11699928343296051, 0.15023665130138397, 0.22535496950149536, 0.37559160590171814, 0.15023665130138397, 0.07511832565069199, 0.5047313570976257, 0.16824378073215485, 0.12618283927440643, 0.16824378073215485, 0.04206094518303871, 0.5638368725776672, 0.1610962599515915, 0.08054812997579575, 0.08054812997579575, 0.1610962599515915, 0.30913645029067993, 0.1659207046031952, 0.2654731273651123, 0.1641741693019867, 0.09431281685829163, 0.26126834750175476, 0.1469634473323822, 0.16601426899433136, 0.34291473031044006, 0.08164636045694351, 0.12894630432128906, 0.12894630432128906, 0.3868389129638672, 0.1934194564819336, 0.12894630432128906, 0.4087499678134918, 0.2043749839067459, 0.15328124165534973, 0.15328124165534973, 0.05109374597668648, 0.1752498894929886, 0.22532129287719727, 0.1752498894929886, 0.1752498894929886, 0.2503570020198822, 0.23773103952407837, 0.17829827964305878, 0.11886551976203918, 0.35659655928611755, 0.11886551976203918, 0.184391587972641, 0.18989580869674683, 0.29172399640083313, 0.19815215468406677, 0.1376056671142578, 0.19341742992401123, 0.19341742992401123, 0.12894496321678162, 0.19341742992401123, 0.32236239314079285, 0.23630574345588684, 0.23630574345588684, 0.1687898188829422, 0.13503184914588928, 0.23630574345588684, 0.3120550215244293, 0.07801375538110733, 0.07801375538110733, 0.39006876945495605, 0.07801375538110733, 0.1875736117362976, 0.4376717507839203, 0.1875736117362976, 0.12504906952381134, 0.06252453476190567, 0.22636198997497559, 0.25174838304519653, 0.3109833002090454, 0.11776464432477951, 0.09308343380689621, 0.2543608844280243, 0.2135854810476303, 0.19222693145275116, 0.19028523564338684, 0.14950983226299286, 0.48655596375465393, 0.16218532621860504, 0.1081235483288765, 0.216247096657753, 0.05406177416443825, 0.17497418820858002, 0.3434678614139557, 0.16849365830421448, 0.16201314330101013, 0.14905208349227905, 0.2912389636039734, 0.1456194818019867, 0.3397787809371948, 0.1456194818019867, 0.09707965701818466, 0.1327715367078781, 0.49789324402809143, 0.19915729761123657, 0.09957864880561829, 0.06638576835393906, 0.18236500024795532, 0.05210428312420845, 0.23446927964687347, 0.2865735590457916, 0.23446927964687347, 0.28992801904678345, 0.1351359337568283, 0.32678326964378357, 0.15479207038879395, 0.09336664527654648, 0.41988903284072876, 0.10797146707773209, 0.15595878660678864, 0.20394611358642578, 0.0959746390581131, 0.1347586065530777, 0.42673560976982117, 0.22459769248962402, 0.11229884624481201, 0.11229884624481201, 0.44424450397491455, 0.1776978075504303, 0.13327334821224213, 0.13327334821224213, 0.08884890377521515, 0.27295732498168945, 0.1939433515071869, 0.2341686487197876, 0.19537997245788574, 0.10343645513057709, 0.07495587319135666, 0.3747793734073639, 0.14991174638271332, 0.14991174638271332, 0.22486762702465057, 0.3845400810241699, 0.14550165832042694, 0.18113471567630768, 0.1707417368888855, 0.11729215085506439, 0.16777166724205017, 0.32355964183807373, 0.2236955612897873, 0.16377711296081543, 0.12383147329092026, 0.20718756318092346, 0.212180033326149, 0.2371423840522766, 0.1722402572631836, 0.1722402572631836, 0.13219191133975983, 0.4130997359752655, 0.13219191133975983, 0.16523988544940948, 0.16523988544940948, 0.14176875352859497, 0.5198187828063965, 0.09451250731945038, 0.18902501463890076, 0.04725625365972519, 0.2881739139556885, 0.16481180489063263, 0.28225255012512207, 0.12730970978736877, 0.1371786743402481, 0.15875926613807678, 0.43658798933029175, 0.19844909012317657, 0.11906944960355759, 0.07937963306903839, 0.17342162132263184, 0.38454359769821167, 0.15834148228168488, 0.1885017603635788, 0.09802091866731644, 0.20628966391086578, 0.41257932782173157, 0.15471723675727844, 0.15471723675727844, 0.051572415977716446, 0.13644786179065704, 0.4548262059688568, 0.18193048238754272, 0.09096524119377136, 0.13644786179065704, 0.16071993112564087, 0.4821597635746002, 0.16071993112564087, 0.2008998990058899, 0.04017998278141022, 0.27268311381340027, 0.12448576837778091, 0.21636812388896942, 0.28157493472099304, 0.10966603457927704, 0.22881223261356354, 0.22881223261356354, 0.07627074420452118, 0.22881223261356354, 0.22881223261356354, 0.3943510353565216, 0.16472890973091125, 0.22962212562561035, 0.16472890973091125, 0.04991785064339638, 0.3502134382724762, 0.11297208070755005, 0.26266008615493774, 0.16804596781730652, 0.10591132193803787, 0.18755213916301727, 0.18755213916301727, 0.18755213916301727, 0.37510427832603455, 0.09377606958150864, 0.2188064306974411, 0.3246343731880188, 0.1401505321264267, 0.2116558998823166, 0.1058279499411583, 0.20066174864768982, 0.13377448916435242, 0.40132349729537964, 0.13377448916435242, 0.13377448916435242, 0.18440620601177216, 0.364473432302475, 0.2538297176361084, 0.1453554779291153, 0.04989815130829811, 0.30674925446510315, 0.23149089515209198, 0.1171744093298912, 0.21529605984687805, 0.12860605120658875, 0.24773210287094116, 0.19118456542491913, 0.2073410004377365, 0.17233538627624512, 0.17772085964679718, 0.47937482595443726, 0.05992185324430466, 0.17976556718349457, 0.17976556718349457, 0.05992185324430466, 0.4380403757095337, 0.17521615326404572, 0.08760807663202286, 0.08760807663202286, 0.17521615326404572, 0.26454758644104004, 0.21794334053993225, 0.1891583651304245, 0.2165726274251938, 0.11171309649944305, 0.3067397177219391, 0.09729188680648804, 0.29998332262039185, 0.19728632271289825, 0.09864316135644913, 0.405720591545105, 0.09435362368822098, 0.20521913468837738, 0.09435362368822098, 0.1981426179409027, 0.24626247584819794, 0.12313123792409897, 0.16417497396469116, 0.16417497396469116, 0.2873062193393707, 0.40059757232666016, 0.19407832622528076, 0.1891019493341446, 0.13436192274093628, 0.07962187379598618, 0.1629265546798706, 0.394453763961792, 0.17150163650512695, 0.1457763910293579, 0.12862622737884521, 0.26754504442214966, 0.22667011618614197, 0.16721566021442413, 0.10404529422521591, 0.23410192131996155, 0.20073291659355164, 0.2161739021539688, 0.13896894454956055, 0.3397018611431122, 0.1080869510769844, 0.1815132051706314, 0.31764811277389526, 0.13613490760326385, 0.13613490760326385, 0.22689150273799896, 0.15821927785873413, 0.4746578633785248, 0.2373289316892624, 0.07910963892936707, 0.07910963892936707, 0.42762842774391174, 0.15004506707191467, 0.1200360506772995, 0.225067600607872, 0.07502253353595734, 0.45722514390945435, 0.13915547728538513, 0.19879354536533356, 0.11927612870931625, 0.05963806435465813, 0.3404361605644226, 0.20463375747203827, 0.2325383573770523, 0.15998639166355133, 0.06325043737888336, 0.39926135540008545, 0.22221332788467407, 0.11742980778217316, 0.16620834171772003, 0.09394384920597076, 0.23788569867610931, 0.23012855648994446, 0.10859999060630798, 0.24564284086227417, 0.1784142702817917, 0.247392475605011, 0.08246415853500366, 0.4123207926750183, 0.16492831707000732, 0.08246415853500366, 0.2454543262720108, 0.12954534590244293, 0.31363609433174133, 0.2045452743768692, 0.10909081250429153, 0.056411758065223694, 0.45129406452178955, 0.22564703226089478, 0.16923527419567108, 0.11282351613044739, 0.12233568727970123, 0.41594135761260986, 0.14680282771587372, 0.22020424902439117, 0.12233568727970123, 0.27198898792266846, 0.1387418657541275, 0.21566803753376007, 0.20467858016490936, 0.16896285116672516, 0.2273012399673462, 0.2273012399673462, 0.20700648427009583, 0.248407781124115, 0.0901087075471878, 0.17700961232185364, 0.3736869692802429, 0.18684348464012146, 0.16717574000358582, 0.09833867102861404, 0.20086020231246948, 0.1685183048248291, 0.3098013401031494, 0.1634116917848587, 0.1583050787448883, 0.32650214433670044, 0.29464828968048096, 0.16457831859588623, 0.09821609407663345, 0.11679751425981522, 0.22100691497325897, 0.13850776851177216, 0.13699401915073395, 0.44201382994651794, 0.06130671501159668, 0.35166874527931213, 0.15396443009376526, 0.2361954301595688, 0.16446200013160706, 0.09272857010364532, 0.24228064715862274, 0.4845612943172455, 0.12114032357931137, 0.12114032357931137, 0.060570161789655685, 0.2083963304758072, 0.4862580895423889, 0.13893088698387146, 0.13893088698387146, 0.06946544349193573, 0.32878369092941284, 0.24964536726474762, 0.19280968606472015, 0.14892390370368958, 0.07985774427652359, 0.3538309633731842, 0.12619145214557648, 0.1608322560787201, 0.20042173564434052, 0.1583579033613205, 0.17213493585586548, 0.1291012018918991, 0.21516866981983185, 0.21516866981983185, 0.2582024037837982, 0.1783769130706787, 0.202980637550354, 0.202980637550354, 0.20913155376911163, 0.202980637550354, 0.3290773332118988, 0.14333677291870117, 0.2287416011095047, 0.22635266184806824, 0.07286286354064941, 0.25868555903434753, 0.08622852712869644, 0.2463672161102295, 0.20325294137001038, 0.19709376990795135, 0.12679074704647064, 0.12679074704647064, 0.2535814940929413, 0.19018612802028656, 0.2535814940929413, 0.18756043910980225, 0.2015112191438675, 0.2898661494255066, 0.2340630292892456, 0.0883549228310585, 0.2748355567455292, 0.16634783148765564, 0.3254631757736206, 0.1735803484916687, 0.06509263068437576, 0.19997647404670715, 0.2485421895980835, 0.17783622443675995, 0.21283210813999176, 0.1606953889131546, 0.2757370173931122, 0.17108984291553497, 0.19766689836978912, 0.24915996193885803, 0.10630825161933899, 0.2878987193107605, 0.11515948921442032, 0.4606379568576813, 0.11515948921442032, 0.05757974460721016, 0.29576626420021057, 0.23661300539970398, 0.17745976150035858, 0.059153251349925995, 0.23661300539970398, 0.37924903631210327, 0.16765086352825165, 0.15788479149341583, 0.22217808663845062, 0.07243168354034424, 0.3046691119670868, 0.2087942212820053, 0.18748869001865387, 0.2279691994190216, 0.0724388062953949, 0.14455917477607727, 0.21909750998020172, 0.28911834955215454, 0.19876886904239655, 0.1490766555070877, 0.5060886740684509, 0.07229838520288467, 0.14459677040576935, 0.21689514815807343, 0.07229838520288467, 0.24110904335975647, 0.15474162995815277, 0.21951718628406525, 0.20872126519680023, 0.1763334721326828, 0.26711902022361755, 0.16694939136505127, 0.16694939136505127, 0.16694939136505127, 0.26711902022361755, 0.25921303033828735, 0.18000903725624084, 0.324016273021698, 0.1512075960636139, 0.08640433847904205, 0.19043253362178802, 0.12695501744747162, 0.25391003489494324, 0.19043253362178802, 0.25391003489494324, 0.2390480637550354, 0.2949294149875641, 0.24215257167816162, 0.12728533148765564, 0.0993446484208107, 0.1823432892560959, 0.24312438070774078, 0.3646865785121918, 0.12156219035387039, 0.060781095176935196, 0.4255848824977875, 0.11606860160827637, 0.19344767928123474, 0.15475814044475555, 0.07737907022237778, 0.11146773397922516, 0.22293546795845032, 0.22293546795845032, 0.22293546795845032, 0.22293546795845032, 0.19384847581386566, 0.25846463441848755, 0.32308080792427063, 0.12923231720924377, 0.06461615860462189, 0.2276753932237625, 0.1641380786895752, 0.30003735423088074, 0.15001867711544037, 0.1606082320213318, 0.4892354905605316, 0.16307850182056427, 0.16307850182056427, 0.16307850182056427, 0.08153925091028214, 0.1562241166830063, 0.1258472055196762, 0.2516944110393524, 0.35584381222724915, 0.10848896950483322, 0.17625248432159424, 0.18426397442817688, 0.136195108294487, 0.3925623595714569, 0.10414920002222061, 0.18115124106407166, 0.13586342334747314, 0.18115124106407166, 0.3623024821281433, 0.09057562053203583, 0.18503572046756744, 0.1817510575056076, 0.2025538980960846, 0.3098527193069458, 0.12153233587741852, 0.3575803339481354, 0.17344427108764648, 0.14790281653404236, 0.19542180001735687, 0.125331312417984, 0.26233312487602234, 0.17652322351932526, 0.20839548110961914, 0.18878178298473358, 0.16426467895507812, 0.12679938971996307, 0.25359877943992615, 0.06339969485998154, 0.25359877943992615, 0.25359877943992615, 0.15800099074840546, 0.40930965542793274, 0.18412713706493378, 0.13933946192264557, 0.10948099941015244, 0.14507180452346802, 0.21760772168636322, 0.14507180452346802, 0.21760772168636322, 0.21760772168636322, 0.24004784226417542, 0.4080813229084015, 0.1440287083387375, 0.1320263147354126, 0.08401674032211304, 0.2767811119556427, 0.1614556461572647, 0.18452073633670807, 0.13839055597782135, 0.2306509166955948, 0.4282709062099457, 0.15417751669883728, 0.17130835354328156, 0.15417751669883728, 0.10278501361608505, 0.11561223119497299, 0.1734183430671692, 0.23122446238994598, 0.3468366861343384, 0.057806115597486496, 0.14375777542591095, 0.21563665568828583, 0.1796972155570984, 0.21563665568828583, 0.21563665568828583, 0.24474017322063446, 0.12749937176704407, 0.28430894017219543, 0.18025772273540497, 0.1641371101140976, 0.36922308802604675, 0.14551733434200287, 0.15746279060840607, 0.21393220126628876, 0.11293882876634598, 0.10692373663187027, 0.21384747326374054, 0.3920536935329437, 0.17820623517036438, 0.10692373663187027, 0.21897445619106293, 0.40286460518836975, 0.15122544765472412, 0.17179210484027863, 0.05565096437931061, 0.1354103982448578, 0.47393637895584106, 0.20311559736728668, 0.0677051991224289, 0.1354103982448578, 0.21744880080223083, 0.23194538056850433, 0.2000528872013092, 0.3015289902687073, 0.052187711000442505, 0.1338287740945816, 0.17843836545944214, 0.3568767309188843, 0.17843836545944214, 0.1338287740945816, 0.4142460525035858, 0.13134631514549255, 0.14144986867904663, 0.21217481791973114, 0.10103562474250793, 0.1835264265537262, 0.13764481246471405, 0.22940802574157715, 0.41293445229530334, 0.0917632132768631, 0.2553058862686157, 0.2553058862686157, 0.17020392417907715, 0.08510196208953857, 0.2553058862686157, 0.20755746960639954, 0.15446136891841888, 0.35719192028045654, 0.14480753242969513, 0.13515369594097137, 0.2272728681564331, 0.10330584645271301, 0.3512398898601532, 0.14462818205356598, 0.18595051765441895, 0.2580815851688385, 0.20364250242710114, 0.3024393618106842, 0.10887816548347473, 0.12904079258441925, 0.22998420894145966, 0.15332281589508057, 0.38330700993537903, 0.07666140794754028, 0.15332281589508057, 0.2735956311225891, 0.41039347648620605, 0.09119854867458344, 0.13679781556129456, 0.09119854867458344, 0.49515581130981445, 0.14147309958934784, 0.14147309958934784, 0.07073654979467392, 0.14147309958934784, 0.3078688383102417, 0.12314753234386444, 0.16932785511016846, 0.3386557102203369, 0.06157376617193222, 0.17652156949043274, 0.17652156949043274, 0.17652156949043274, 0.2647823691368103, 0.2647823691368103, 0.17063011229038239, 0.17063011229038239, 0.17063011229038239, 0.34126022458076477, 0.17063011229038239, 0.17549623548984528, 0.12763363122940063, 0.2685624063014984, 0.3031298518180847, 0.12497459352016449, 0.4611336290836334, 0.09222672879695892, 0.18445345759391785, 0.09222672879695892, 0.09222672879695892, 0.2781226933002472, 0.23514008522033691, 0.12136262655258179, 0.2755942940711975, 0.09102196991443634, 0.20034870505332947, 0.10017435252666473, 0.40069741010665894, 0.20034870505332947, 0.10017435252666473, 0.4516507387161255, 0.1505502462387085, 0.1505502462387085, 0.22582536935806274, 0.07527512311935425, 0.24584391713142395, 0.08194797486066818, 0.32779189944267273, 0.16389594972133636, 0.08194797486066818, 0.23091137409210205, 0.15394091606140137, 0.07697045803070068, 0.3848522901535034, 0.07697045803070068, 0.2217470407485962, 0.14783136546611786, 0.14783136546611786, 0.14783136546611786, 0.2217470407485962, 0.4546532928943634, 0.15155109763145447, 0.15155109763145447, 0.15155109763145447, 0.07577554881572723, 0.23702959716320038, 0.19752465188503265, 0.18435634672641754, 0.2633662223815918, 0.11632007360458374, 0.24124857783317566, 0.2142234593629837, 0.1918123960494995, 0.17994770407676697, 0.1726970672607422, 0.25137320160865784, 0.20109854638576508, 0.05027463659644127, 0.1508239060640335, 0.301647812128067, 0.2763686776161194, 0.23273150622844696, 0.21273113787174225, 0.16727577149868011, 0.11091110855340958, 0.23990844190120697, 0.17993132770061493, 0.16493704915046692, 0.21741703152656555, 0.19492560625076294, 0.20856572687625885, 0.13904382288455963, 0.13904382288455963, 0.20856572687625885, 0.27808764576911926, 0.32006150484085083, 0.1989571452140808, 0.15858903527259827, 0.16003075242042542, 0.1629141867160797, 0.35958534479141235, 0.18482503294944763, 0.14273616671562195, 0.22142405807971954, 0.09241251647472382, 0.24976374208927155, 0.1183091402053833, 0.16431824862957, 0.3352092206478119, 0.13145460188388824, 0.13022267818450928, 0.13022267818450928, 0.1736302375793457, 0.4774831533432007, 0.08681511878967285, 0.2559982240200043, 0.1660528928041458, 0.1868095099925995, 0.15913403034210205, 0.22832272946834564, 0.15163353085517883, 0.22745028138160706, 0.4549005627632141, 0.07581676542758942, 0.07581676542758942, 0.3010842204093933, 0.12677229940891266, 0.12677229940891266, 0.36447036266326904, 0.07923269271850586, 0.3386893570423126, 0.08467233926057816, 0.08467233926057816, 0.4233616888523102, 0.08467233926057816, 0.19749900698661804, 0.18230678141117096, 0.3190368711948395, 0.1519223153591156, 0.1519223153591156, 0.3472742736339569, 0.17363713681697845, 0.3472742736339569, 0.11575809121131897, 0.057879045605659485, 0.25068527460098267, 0.1367374211549759, 0.36463311314582825, 0.1367374211549759, 0.1367374211549759, 0.16176515817642212, 0.3426867127418518, 0.20859192311763763, 0.12983782589435577, 0.15537969768047333, 0.273225337266922, 0.16393519937992096, 0.3643004298210144, 0.10929013043642044, 0.0910751074552536, 0.35117045044898987, 0.18521648645401, 0.18817995488643646, 0.2178145945072174, 0.05778754502534866, 0.20799633860588074, 0.08319853246212006, 0.12479779869318008, 0.4575919508934021, 0.12479779869318008, 0.13799722492694855, 0.20699584484100342, 0.20699584484100342, 0.41399168968200684, 0.06899861246347427, 0.16416172683238983, 0.43776458501815796, 0.16416172683238983, 0.10944114625453949, 0.10944114625453949, 0.46020132303237915, 0.09204026311635971, 0.18408052623271942, 0.23010066151618958, 0.046020131558179855, 0.29261934757232666, 0.14630967378616333, 0.14630967378616333, 0.14630967378616333, 0.29261934757232666, 0.18936344981193542, 0.3581438958644867, 0.15848897397518158, 0.21406301856040955, 0.08027363568544388, 0.19790178537368774, 0.40479910373687744, 0.14392857253551483, 0.16191963851451874, 0.09895089268684387, 0.10192332416772842, 0.32868295907974243, 0.23545077443122864, 0.18488416075706482, 0.14853940904140472, 0.14782747626304626, 0.14782747626304626, 0.07391373813152313, 0.36956870555877686, 0.2217412143945694, 0.22733834385871887, 0.09093533456325531, 0.36374133825302124, 0.13640300929546356, 0.13640300929546356, 0.24354638159275055, 0.21188536286354065, 0.18996618688106537, 0.1875307261943817, 0.16561155021190643, 0.39826563000679016, 0.07965312898159027, 0.15930625796318054, 0.15930625796318054, 0.15930625796318054, 0.357733815908432, 0.12388447672128677, 0.1531156450510025, 0.1712111234664917, 0.19348248839378357, 0.21307364106178284, 0.4261472821235657, 0.14204908907413483, 0.14204908907413483, 0.07102454453706741, 0.144707590341568, 0.3617689609527588, 0.3617689609527588, 0.072353795170784, 0.072353795170784, 0.15466167032718658, 0.15466167032718658, 0.30932334065437317, 0.15466167032718658, 0.23199251294136047, 0.40375590324401855, 0.13626761734485626, 0.22458922863006592, 0.15393194556236267, 0.08327465504407883, 0.23259630799293518, 0.46519261598587036, 0.1744472235441208, 0.058149076998233795, 0.058149076998233795, 0.48535779118537903, 0.11008114367723465, 0.215158611536026, 0.10007376968860626, 0.09507007896900177, 0.42470139265060425, 0.13364729285240173, 0.13067735731601715, 0.21086573600769043, 0.10097795724868774, 0.16793006658554077, 0.08396503329277039, 0.33586013317108154, 0.08396503329277039, 0.25189509987831116, 0.20347271859645844, 0.15260453522205353, 0.35607725381851196, 0.12717044353485107, 0.12717044353485107, 0.19326570630073547, 0.144949272274971, 0.19326570630073547, 0.24158212542533875, 0.289898544549942, 0.4300999045372009, 0.14336663484573364, 0.21504995226860046, 0.07168331742286682, 0.07168331742286682, 0.24628368020057678, 0.286304771900177, 0.16778075695037842, 0.1693200320005417, 0.12929892539978027, 0.2448863387107849, 0.19715425372123718, 0.13697032630443573, 0.3486517369747162, 0.07263577729463577, 0.17478060722351074, 0.23304080963134766, 0.11652040481567383, 0.4078214168548584, 0.058260202407836914, 0.4167960286140442, 0.16671840846538544, 0.25007760524749756, 0.08335920423269272, 0.08335920423269272, 0.17707715928554535, 0.41318005323410034, 0.17707715928554535, 0.2361028790473938, 0.05902571976184845, 0.20693346858024597, 0.10346673429012299, 0.41386693716049194, 0.10346673429012299, 0.13795563578605652, 0.21027269959449768, 0.05256817489862442, 0.36797723174095154, 0.15770451724529266, 0.15770451724529266, 0.15476031601428986, 0.4642809331417084, 0.15476031601428986, 0.07738015800714493, 0.07738015800714493, 0.22340114414691925, 0.1546623259782791, 0.1460699737071991, 0.36947110295295715, 0.10310821980237961, 0.22007054090499878, 0.18812482059001923, 0.1916743516921997, 0.30880865454673767, 0.08873812109231949, 0.31279894709587097, 0.17674413323402405, 0.24159269034862518, 0.16784334182739258, 0.10172323882579803, 0.12338025867938995, 0.06169012933969498, 0.2776055932044983, 0.37014079093933105, 0.18507039546966553, 0.10067646950483322, 0.3811323344707489, 0.165397047996521, 0.24449999630451202, 0.10067646950483322, 0.3188421428203583, 0.09565264731645584, 0.1275368630886078, 0.38261058926582336, 0.09565264731645584, 0.21548663079738617, 0.07835877686738968, 0.3428196609020233, 0.2742557227611542, 0.08815362304449081, 0.405825674533844, 0.0676376074552536, 0.1352752149105072, 0.2705504298210144, 0.0676376074552536, 0.17804405093193054, 0.17804405093193054, 0.3560881018638611, 0.17804405093193054, 0.05934802070260048, 0.43677037954330444, 0.16014914214611053, 0.14559012651443481, 0.16014914214611053, 0.10191309452056885, 0.10488083958625793, 0.26220211386680603, 0.39330315589904785, 0.1573212593793869, 0.07866062968969345, 0.21731820702552795, 0.12418182939291, 0.09313637763261795, 0.49672731757164, 0.09313637763261795, 0.2817375957965851, 0.19206491112709045, 0.1965167373418808, 0.21496005356311798, 0.11447577178478241, 0.2783389985561371, 0.18555933237075806, 0.18555933237075806, 0.3711186647415161, 0.09277966618537903, 0.09364253282546997, 0.15607088804244995, 0.24971342086791992, 0.3745701313018799, 0.12485671043395996, 0.2743140459060669, 0.18389201164245605, 0.09854986518621445, 0.2672021985054016, 0.17678016424179077, 0.16322095692157745, 0.21762792766094208, 0.21762792766094208, 0.16322095692157745, 0.21762792766094208, 0.09409677237272263, 0.2728806436061859, 0.24465161561965942, 0.13173548877239227, 0.26347097754478455, 0.3173956573009491, 0.1740237921476364, 0.22939500212669373, 0.19379922747612, 0.08503435552120209, 0.23967315256595612, 0.17151610553264618, 0.204471156001091, 0.26588740944862366, 0.11833862215280533, 0.3206698000431061, 0.08016745001077652, 0.16033490002155304, 0.4008372724056244, 0.16033490002155304, 0.14853842556476593, 0.445615291595459, 0.2228076457977295, 0.14853842556476593, 0.07426921278238297, 0.19805386662483215, 0.3741017282009125, 0.15404188632965088, 0.15404188632965088, 0.12103291600942612], \"Term\": [\"@PERSON1\", \"@PERSON1\", \"@PERSON1\", \"@PERSON1\", \"@PERSON1\", \"People\", \"People\", \"People\", \"People\", \"People\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability_learn\", \"ability_learn\", \"ability_learn\", \"ability_learn\", \"ability_learn\", \"able\", \"able\", \"able\", \"able\", \"able\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"access\", \"access\", \"access\", \"access\", \"access\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"accurate\", \"acess\", \"acess\", \"acess\", \"acess\", \"acess\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"addict\", \"addict\", \"addict\", \"addict\", \"addict\", \"addicting\", \"addicting\", \"addicting\", \"addicting\", \"addicting\", \"address\", \"address\", \"address\", \"address\", \"address\", \"adult\", \"adult\", \"adult\", \"adult\", \"adult\", \"affect\", \"affect\", \"affect\", \"affect\", \"affect\", \"airport\", \"airport\", \"airport\", \"airport\", \"airport\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"animal\", \"animal\", \"animal\", \"animal\", \"animal\", \"aol\", \"aol\", \"aol\", \"aol\", \"aol\", \"argument\", \"argument\", \"argument\", \"argument\", \"argument\", \"army\", \"army\", \"army\", \"army\", \"army\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"away\", \"away\", \"away\", \"away\", \"away\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"ball\", \"ball\", \"ball\", \"ball\", \"ball\", \"bear\", \"bear\", \"bear\", \"bear\", \"bear\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"beneficial\", \"beneficial\", \"beneficial\", \"beneficial\", \"beneficial\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benifit\", \"benifit\", \"benifit\", \"benifit\", \"benifit\", \"bike_ride\", \"bike_ride\", \"bike_ride\", \"bike_ride\", \"bike_ride\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"bully\", \"bully\", \"bully\", \"bully\", \"bully\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"bus\", \"bus\", \"bus\", \"bus\", \"bus\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause_obesity\", \"cause_obesity\", \"cause_obesity\", \"cause_obesity\", \"cause_obesity\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"chat\", \"chat\", \"chat\", \"chat\", \"chat\", \"child\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"children\", \"chore\", \"chore\", \"chore\", \"chore\", \"chore\", \"claim\", \"claim\", \"claim\", \"claim\", \"claim\", \"collage\", \"collage\", \"collage\", \"collage\", \"collage\", \"comfortable\", \"comfortable\", \"comfortable\", \"comfortable\", \"comfortable\", \"communicate\", \"communicate\", \"communicate\", \"communicate\", \"communicate\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"completly\", \"completly\", \"completly\", \"completly\", \"completly\", \"compter\", \"compter\", \"compter\", \"compter\", \"compter\", \"conclude\", \"conclude\", \"conclude\", \"conclude\", \"conclude\", \"correctly\", \"correctly\", \"correctly\", \"correctly\", \"correctly\", \"country\", \"country\", \"country\", \"country\", \"country\", \"credit_card\", \"credit_card\", \"credit_card\", \"credit_card\", \"credit_card\", \"crime\", \"crime\", \"crime\", \"crime\", \"crime\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"daily_basis\", \"daily_basis\", \"daily_basis\", \"daily_basis\", \"daily_basis\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dear\", \"dear\", \"dear\", \"dear\", \"dear\", \"dear_local\", \"dear_local\", \"dear_local\", \"dear_local\", \"dear_local\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"different\", \"different\", \"different\", \"different\", \"different\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drug\", \"drug\", \"drug\", \"drug\", \"drug\", \"eachother\", \"eachother\", \"eachother\", \"eachother\", \"eachother\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"eat_junk_food\", \"eat_junk_food\", \"eat_junk_food\", \"eat_junk_food\", \"eat_junk_food\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excersize\", \"excersize\", \"excersize\", \"excersize\", \"excersize\", \"exercis\", \"exercis\", \"exercis\", \"exercis\", \"exercis\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"extremly\", \"extremly\", \"extremly\", \"extremly\", \"extremly\", \"eye_sight\", \"eye_sight\", \"eye_sight\", \"eye_sight\", \"eye_sight\", \"eyesight\", \"eyesight\", \"eyesight\", \"eyesight\", \"eyesight\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fail_class\", \"fail_class\", \"fail_class\", \"fail_class\", \"fail_class\", \"fast\", \"fast\", \"fast\", \"fast\", \"fast\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fool\", \"fool\", \"fool\", \"fool\", \"fool\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"gain_knowledge\", \"gain_knowledge\", \"gain_knowledge\", \"gain_knowledge\", \"gain_knowledge\", \"game\", \"game\", \"game\", \"game\", \"game\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give_ability\", \"give_ability\", \"give_ability\", \"give_ability\", \"give_ability\", \"give_ability_learn\", \"give_ability_learn\", \"give_ability_learn\", \"give_ability_learn\", \"give_ability_learn\", \"go\", \"go\", \"go\", \"go\", \"go\", \"great\", \"great\", \"great\", \"great\", \"great\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hang_friend_family\", \"hang_friend_family\", \"hang_friend_family\", \"hang_friend_family\", \"hang_friend_family\", \"happen\", \"happen\", \"happen\", \"happen\", \"happen\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"have\", \"have\", \"have\", \"have\", \"have\", \"head\", \"head\", \"head\", \"head\", \"head\", \"helpfull\", \"helpfull\", \"helpfull\", \"helpfull\", \"helpfull\", \"hire\", \"hire\", \"hire\", \"hire\", \"hire\", \"history\", \"history\", \"history\", \"history\", \"history\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homework\", \"homework\", \"homework\", \"homework\", \"homework\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"house_hold\", \"house_hold\", \"house_hold\", \"house_hold\", \"house_hold\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hurt_eye\", \"hurt_eye\", \"hurt_eye\", \"hurt_eye\", \"hurt_eye\", \"ignore\", \"ignore\", \"ignore\", \"ignore\", \"ignore\", \"important\", \"important\", \"important\", \"important\", \"important\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inside\", \"inside\", \"inside\", \"inside\", \"inside\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"interact\", \"interact\", \"interact\", \"interact\", \"interact\", \"internet\", \"internet\", \"internet\", \"internet\", \"internet\", \"job\", \"job\", \"job\", \"job\", \"job\", \"jog\", \"jog\", \"jog\", \"jog\", \"jog\", \"kick\", \"kick\", \"kick\", \"kick\", \"kick\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"lastly\", \"lastly\", \"lastly\", \"lastly\", \"lastly\", \"law\", \"law\", \"law\", \"law\", \"law\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"lazy\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn_new\", \"learn_new\", \"learn_new\", \"learn_new\", \"learn_new\", \"leave_home\", \"leave_home\", \"leave_home\", \"leave_home\", \"leave_home\", \"let\", \"let\", \"let\", \"let\", \"let\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"life\", \"life\", \"life\", \"life\", \"life\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"long_period_time\", \"long_period_time\", \"long_period_time\", \"long_period_time\", \"long_period_time\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"make\", \"make\", \"make\", \"make\", \"make\", \"material\", \"material\", \"material\", \"material\", \"material\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"move\", \"move\", \"move\", \"move\", \"move\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature_beautiful\", \"nature_beautiful\", \"nature_beautiful\", \"nature_beautiful\", \"nature_beautiful\", \"nearly\", \"nearly\", \"nearly\", \"nearly\", \"nearly\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"neighborhood\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newpaper\", \"newpaper\", \"newpaper\", \"newpaper\", \"newpaper\", \"news\", \"news\", \"news\", \"news\", \"news\", \"newspaper\", \"newspaper\", \"newspaper\", \"newspaper\", \"newspaper\", \"normal\", \"normal\", \"normal\", \"normal\", \"normal\", \"not\", \"not\", \"not\", \"not\", \"not\", \"online\", \"online\", \"online\", \"online\", \"online\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"othe\", \"othe\", \"othe\", \"othe\", \"othe\", \"outside\", \"outside\", \"outside\", \"outside\", \"outside\", \"page_essay\", \"page_essay\", \"page_essay\", \"page_essay\", \"page_essay\", \"park\", \"park\", \"park\", \"park\", \"park\", \"party\", \"party\", \"party\", \"party\", \"party\", \"past\", \"past\", \"past\", \"past\", \"past\", \"peer\", \"peer\", \"peer\", \"peer\", \"peer\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"person\", \"person\", \"person\", \"person\", \"person\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plant\", \"plant\", \"plant\", \"plant\", \"plant\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play_baseball\", \"play_baseball\", \"play_baseball\", \"play_baseball\", \"play_baseball\", \"play_game\", \"play_game\", \"play_game\", \"play_game\", \"play_game\", \"plenty\", \"plenty\", \"plenty\", \"plenty\", \"plenty\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"poeple\", \"poeple\", \"poeple\", \"poeple\", \"poeple\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive_affect\", \"positive_affect\", \"positive_affect\", \"positive_affect\", \"positive_affect\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"positive_negative_effect\", \"positive_negative_effect\", \"positive_negative_effect\", \"positive_negative_effect\", \"positive_negative_effect\", \"possitive_effect\", \"possitive_effect\", \"possitive_effect\", \"possitive_effect\", \"possitive_effect\", \"power_point\", \"power_point\", \"power_point\", \"power_point\", \"power_point\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"private\", \"private\", \"private\", \"private\", \"private\", \"pro_con\", \"pro_con\", \"pro_con\", \"pro_con\", \"pro_con\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"project\", \"project\", \"project\", \"project\", \"project\", \"push_button\", \"push_button\", \"push_button\", \"push_button\", \"push_button\", \"quickly_easily\", \"quickly_easily\", \"quickly_easily\", \"quickly_easily\", \"quickly_easily\", \"quote\", \"quote\", \"quote\", \"quote\", \"quote\", \"radio\", \"radio\", \"radio\", \"radio\", \"radio\", \"react\", \"react\", \"react\", \"react\", \"react\", \"reaction\", \"reaction\", \"reaction\", \"reaction\", \"reaction\", \"read\", \"read\", \"read\", \"read\", \"read\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reflex\", \"reflex\", \"reflex\", \"reflex\", \"reflex\", \"research\", \"research\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"right\", \"right\", \"rumor\", \"rumor\", \"rumor\", \"rumor\", \"rumor\", \"say\", \"say\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school_work\", \"school_work\", \"school_work\", \"school_work\", \"school_work\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"second_reason\", \"second_reason\", \"second_reason\", \"second_reason\", \"second_reason\", \"send_e_mail\", \"send_e_mail\", \"send_e_mail\", \"send_e_mail\", \"send_e_mail\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop_online\", \"shop_online\", \"shop_online\", \"shop_online\", \"shop_online\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shy\", \"shy\", \"shy\", \"shy\", \"shy\", \"sight\", \"sight\", \"sight\", \"sight\", \"sight\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"social_skill\", \"social_skill\", \"social_skill\", \"social_skill\", \"social_skill\", \"society\", \"society\", \"society\", \"society\", \"society\", \"soldier\", \"soldier\", \"soldier\", \"soldier\", \"soldier\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"somthe\", \"somthe\", \"somthe\", \"somthe\", \"somthe\", \"spell\", \"spell\", \"spell\", \"spell\", \"spell\", \"spell_check\", \"spell_check\", \"spell_check\", \"spell_check\", \"spell_check\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend_hour\", \"spend_hour\", \"spend_hour\", \"spend_hour\", \"spend_hour\", \"spend_time\", \"spend_time\", \"spend_time\", \"spend_time\", \"spend_time\", \"spend_time_outdoors\", \"spend_time_outdoors\", \"spend_time_outdoors\", \"spend_time_outdoors\", \"spend_time_outdoors\", \"spot\", \"spot\", \"spot\", \"spot\", \"spot\", \"start\", \"start\", \"start\", \"start\", \"start\", \"stressful\", \"stressful\", \"stressful\", \"stressful\", \"stressful\", \"student\", \"student\", \"student\", \"student\", \"student\", \"sudden\", \"sudden\", \"sudden\", \"sudden\", \"sudden\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"take_care\", \"take_care\", \"take_care\", \"take_care\", \"take_care\", \"teach\", \"teach\", \"teach\", \"teach\", \"teach\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye_coordination\", \"teach_hand_eye_coordination\", \"teach_hand_eye_coordination\", \"teach_hand_eye_coordination\", \"teach_hand_eye_coordination\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"team\", \"team\", \"team\", \"team\", \"team\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech\", \"technique\", \"technique\", \"technique\", \"technique\", \"technique\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"thay\", \"thay\", \"thay\", \"thay\", \"thay\", \"throught\", \"throught\", \"throught\", \"throught\", \"throught\", \"till\", \"till\", \"till\", \"till\", \"till\", \"time_consume\", \"time_consume\", \"time_consume\", \"time_consume\", \"time_consume\", \"tip\", \"tip\", \"tip\", \"tip\", \"tip\", \"toll\", \"toll\", \"toll\", \"toll\", \"toll\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"try\", \"try\", \"try\", \"try\", \"try\", \"type\", \"type\", \"type\", \"type\", \"type\", \"u\", \"u\", \"u\", \"u\", \"u\", \"unhealthy\", \"unhealthy\", \"unhealthy\", \"unhealthy\", \"unhealthy\", \"usefull\", \"usefull\", \"usefull\", \"usefull\", \"usefull\", \"user\", \"user\", \"user\", \"user\", \"user\", \"vaction\", \"vaction\", \"vaction\", \"vaction\", \"vaction\", \"vast\", \"vast\", \"vast\", \"vast\", \"vast\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"wanna\", \"wanna\", \"wanna\", \"wanna\", \"wanna\", \"want\", \"want\", \"want\", \"want\", \"want\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"watch_video\", \"watch_video\", \"watch_video\", \"watch_video\", \"watch_video\", \"website\", \"website\", \"website\", \"website\", \"website\", \"will_able\", \"will_able\", \"will_able\", \"will_able\", \"will_able\", \"will_not\", \"will_not\", \"will_not\", \"will_not\", \"will_not\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"worldwide\", \"worldwide\", \"worldwide\", \"worldwide\", \"worldwide\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"year_old\", \"year_old\", \"year_old\", \"year_old\", \"year_old\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 2, 4, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el83657204019843523185828\", ldavis_el83657204019843523185828_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el83657204019843523185828\", ldavis_el83657204019843523185828_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el83657204019843523185828\", ldavis_el83657204019843523185828_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing text with LDA\n",
    "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% _Topic A_, 20% _Topic B_, 20% _Topic C_, and 10% _Topic D_.\n",
    "\n",
    "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n",
    "1. Using spaCy to remove punctuation and lemmatize the text\n",
    "1. Applying our first-order phrase model to join word pairs\n",
    "1. Applying our second-order phrase model to join longer phrases\n",
    "1. Removing stopwords\n",
    "1. Creating a bag-of-words representation\n",
    "\n",
    "Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The `lda_description(...)` function will perform all these steps for us, including printing the resulting topical description of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_essay(essay_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(essay_set1_txt_filepath, codecs),essay_number, essay_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(essay_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the essay text with spaCy\n",
    "    parsed_essay = nlp(essay_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_essay = [token.lemma_ for token in parsed_essay\n",
    "                      if not punct_space_stop(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_essay = bigram_model[unigram_essay]\n",
    "    trigram_essay = trigram_model[bigram_essay]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    essay_bow = trigram_dictionary.doc2bow(trigram_essay)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    essay_lda = lda[essay_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    essay_lda = sorted(essay_lda)\n",
    "    \n",
    "    for topic_number, freq in essay_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print('{:25} {}'.format(topic_names[topic_number],round(freq, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear @CAPS1 @CAPS2, I have heard the concern of many scientists about computers. Computers have brought us into the @DATE1 and have improved our society but they are harming our childrens health and education. Because of computers, children and adults around the world are spending less time with family and friends, they are not enjoying athletic activities and they are having health problems. Because of an increase in technology, children and adults are not spending time with their families. The computers are ruining family connections. For example, a study at @ORGANIZATION1 showed that families with more computers in their houses communicated less. The children are talking to their friends online or are going on social networking sites instead of talking in person to their family members. Also, computers are making it very hard for families to have meals together. When the children are on the computer, they do not want to leave. This obsetion with the technology is making it harder for families to grow moved and of were more succesful in school. The computers are children from a better education. Athletic activities are also suffering been so of @PERSON1, fresh air more children They will be on websites all day insted of being on a. Sports teams because of computers. The electricity that runs computers of this radiation. Being exposed to this could lead to brain damage or possible death. When my aunt bought a computer for the first time, she complained about massive headaches that kept returning. This was caused by the radiation emitted from the computer. Another health risk is obesity. When children sit in front of a computer for many hours, they have an increase in apetite. These children eat because they have nothing better to do. While on the computer, the users eat and will gain weight because of the loss of exercise. They could become obese and possibly get diabetes. Therefore, there is less communication in families, a loss of excersise, and health concerns caused by the computers. The consequences of misusing this technology are devastating and could be life thretening. Thus, I urge you to do whatever is possible to use the computer less or face the consequences that come with it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_essay = get_sample_essay(756)\n",
    "print(sample_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasons to spend time online 0.41499999165534973\n"
     ]
    }
   ],
   "source": [
    "lda_description(sample_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "# word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 training epochs so far.\n",
      "CPU times: user 37.2 s, sys: 496 ms, total: 37.7 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "#     t = time()\n",
    "    # initiate the model and perform 15 epochs of training\n",
    "    # workers should be cores - 1\n",
    "    essay2vec_model = Word2Vec(min_count=20, window=5, size=100, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=4)    \n",
    "    essay2vec_model.build_vocab(trigram_sentences)\n",
    "    \n",
    "    for i in range(6):\n",
    "        essay2vec_model.train(trigram_sentences, total_examples=essay2vec_model.corpus_count, epochs=15, report_delay=1)\n",
    "    \n",
    "    essay2vec_model.save(word2vec_filepath)\n",
    "\n",
    "        \n",
    "# load the finished model from disk\n",
    "essay2vec_model = Word2Vec.load(word2vec_filepath)\n",
    "essay2vec_model.init_sims()\n",
    "\n",
    "print('{} training epochs so far.'.format(essay2vec_model.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,250 terms in the essay2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print('{:,} terms in the essay2vec vocabulary.'.format(len(essay2vec_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dear_Local_Newspaper</th>\n",
       "      <td>-0.080966</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.160840</td>\n",
       "      <td>0.153978</td>\n",
       "      <td>-0.150762</td>\n",
       "      <td>-0.227000</td>\n",
       "      <td>-0.119609</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.119844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083057</td>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.172746</td>\n",
       "      <td>0.107085</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>-0.054023</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.089317</td>\n",
       "      <td>-0.006336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dear_Newspaper</th>\n",
       "      <td>-0.160894</td>\n",
       "      <td>-0.078554</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.128657</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>-0.129569</td>\n",
       "      <td>-0.273821</td>\n",
       "      <td>-0.217280</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>0.088784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131236</td>\n",
       "      <td>-0.100623</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>0.176573</td>\n",
       "      <td>-0.096566</td>\n",
       "      <td>-0.069011</td>\n",
       "      <td>0.045533</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>0.079021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr.</th>\n",
       "      <td>-0.048280</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>-0.078311</td>\n",
       "      <td>-0.077604</td>\n",
       "      <td>0.105933</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>-0.245606</td>\n",
       "      <td>-0.125072</td>\n",
       "      <td>0.157281</td>\n",
       "      <td>0.109850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042051</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>-0.073568</td>\n",
       "      <td>0.167597</td>\n",
       "      <td>-0.038416</td>\n",
       "      <td>0.056799</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>-0.012547</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>0.160234</td>\n",
       "      <td>0.161505</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.027028</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.097615</td>\n",
       "      <td>-0.074445</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>-0.013195</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045397</td>\n",
       "      <td>0.061529</td>\n",
       "      <td>-0.110230</td>\n",
       "      <td>-0.138034</td>\n",
       "      <td>-0.101535</td>\n",
       "      <td>0.122856</td>\n",
       "      <td>0.225491</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>-0.037007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>People</th>\n",
       "      <td>-0.054791</td>\n",
       "      <td>0.059666</td>\n",
       "      <td>-0.006936</td>\n",
       "      <td>-0.049464</td>\n",
       "      <td>0.207019</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>0.076224</td>\n",
       "      <td>-0.133590</td>\n",
       "      <td>-0.162718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117228</td>\n",
       "      <td>-0.056509</td>\n",
       "      <td>-0.127031</td>\n",
       "      <td>-0.096924</td>\n",
       "      <td>0.016651</td>\n",
       "      <td>-0.033707</td>\n",
       "      <td>-0.080514</td>\n",
       "      <td>-0.129022</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>-0.022756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0.061915</td>\n",
       "      <td>0.095871</td>\n",
       "      <td>-0.082138</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.046556</td>\n",
       "      <td>-0.055796</td>\n",
       "      <td>-0.045085</td>\n",
       "      <td>-0.001371</td>\n",
       "      <td>-0.175662</td>\n",
       "      <td>-0.038806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>0.111721</td>\n",
       "      <td>-0.185697</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.260463</td>\n",
       "      <td>0.133326</td>\n",
       "      <td>-0.022905</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>-0.020684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn</th>\n",
       "      <td>0.078620</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>0.106955</td>\n",
       "      <td>-0.152974</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>-0.215835</td>\n",
       "      <td>-0.024729</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>-0.039019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200842</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>-0.149448</td>\n",
       "      <td>-0.010260</td>\n",
       "      <td>-0.103228</td>\n",
       "      <td>0.045211</td>\n",
       "      <td>0.028685</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.066922</td>\n",
       "      <td>0.105161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn_far_away</th>\n",
       "      <td>0.068378</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.083156</td>\n",
       "      <td>-0.106033</td>\n",
       "      <td>0.218467</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>-0.183279</td>\n",
       "      <td>-0.111118</td>\n",
       "      <td>-0.029186</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>-0.048395</td>\n",
       "      <td>-0.052231</td>\n",
       "      <td>-0.211182</td>\n",
       "      <td>-0.067038</td>\n",
       "      <td>-0.046495</td>\n",
       "      <td>-0.022695</td>\n",
       "      <td>-0.043916</td>\n",
       "      <td>-0.026545</td>\n",
       "      <td>0.121319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn_faraway_place</th>\n",
       "      <td>-0.043237</td>\n",
       "      <td>-0.050755</td>\n",
       "      <td>0.093305</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.205478</td>\n",
       "      <td>-0.025085</td>\n",
       "      <td>-0.212736</td>\n",
       "      <td>-0.158275</td>\n",
       "      <td>-0.032956</td>\n",
       "      <td>0.036593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275119</td>\n",
       "      <td>-0.010331</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>-0.158186</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>-0.042032</td>\n",
       "      <td>0.026861</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>0.061112</td>\n",
       "      <td>0.114431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.023570</td>\n",
       "      <td>-0.053252</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>-0.085435</td>\n",
       "      <td>0.164259</td>\n",
       "      <td>-0.058869</td>\n",
       "      <td>0.080843</td>\n",
       "      <td>0.106354</td>\n",
       "      <td>-0.153176</td>\n",
       "      <td>-0.034961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>-0.001847</td>\n",
       "      <td>-0.063932</td>\n",
       "      <td>-0.096556</td>\n",
       "      <td>-0.110397</td>\n",
       "      <td>0.145599</td>\n",
       "      <td>0.058895</td>\n",
       "      <td>-0.044561</td>\n",
       "      <td>0.179839</td>\n",
       "      <td>-0.101470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.009471</td>\n",
       "      <td>-0.037858</td>\n",
       "      <td>0.118418</td>\n",
       "      <td>0.268994</td>\n",
       "      <td>0.313056</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>-0.136287</td>\n",
       "      <td>-0.139082</td>\n",
       "      <td>-0.000933</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134323</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>-0.121617</td>\n",
       "      <td>-0.086920</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>-0.044351</td>\n",
       "      <td>-0.079989</td>\n",
       "      <td>0.007832</td>\n",
       "      <td>-0.135252</td>\n",
       "      <td>-0.084513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>-0.060129</td>\n",
       "      <td>0.100730</td>\n",
       "      <td>0.098544</td>\n",
       "      <td>0.117001</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.050971</td>\n",
       "      <td>0.028348</td>\n",
       "      <td>-0.110445</td>\n",
       "      <td>-0.042671</td>\n",
       "      <td>0.020960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138893</td>\n",
       "      <td>-0.028911</td>\n",
       "      <td>-0.187556</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>-0.081402</td>\n",
       "      <td>-0.078189</td>\n",
       "      <td>-0.054812</td>\n",
       "      <td>-0.072359</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.008121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>-0.016774</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>-0.144403</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>-0.151972</td>\n",
       "      <td>-0.002618</td>\n",
       "      <td>-0.021094</td>\n",
       "      <td>-0.106052</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>0.058271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138290</td>\n",
       "      <td>0.049777</td>\n",
       "      <td>-0.109659</td>\n",
       "      <td>-0.086719</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.214862</td>\n",
       "      <td>0.108611</td>\n",
       "      <td>-0.051788</td>\n",
       "      <td>-0.025838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.023603</td>\n",
       "      <td>0.048625</td>\n",
       "      <td>-0.031065</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.164677</td>\n",
       "      <td>0.130134</td>\n",
       "      <td>-0.074918</td>\n",
       "      <td>-0.004850</td>\n",
       "      <td>0.192992</td>\n",
       "      <td>0.155685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029439</td>\n",
       "      <td>-0.012145</td>\n",
       "      <td>-0.067884</td>\n",
       "      <td>-0.066159</td>\n",
       "      <td>-0.064794</td>\n",
       "      <td>0.077432</td>\n",
       "      <td>0.110461</td>\n",
       "      <td>-0.009767</td>\n",
       "      <td>-0.051933</td>\n",
       "      <td>0.118402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account</th>\n",
       "      <td>-0.034982</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>-0.047256</td>\n",
       "      <td>-0.047565</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.026578</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>-0.015990</td>\n",
       "      <td>-0.091662</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081644</td>\n",
       "      <td>-0.045451</td>\n",
       "      <td>-0.062248</td>\n",
       "      <td>-0.130844</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>0.112684</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>-0.031739</td>\n",
       "      <td>-0.023910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accurate</th>\n",
       "      <td>0.086073</td>\n",
       "      <td>-0.032643</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>-0.101803</td>\n",
       "      <td>-0.074570</td>\n",
       "      <td>-0.103560</td>\n",
       "      <td>0.101796</td>\n",
       "      <td>-0.118272</td>\n",
       "      <td>-0.060125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>-0.047235</td>\n",
       "      <td>0.125650</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.088164</td>\n",
       "      <td>0.177889</td>\n",
       "      <td>-0.017273</td>\n",
       "      <td>-0.106321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acess</th>\n",
       "      <td>0.080051</td>\n",
       "      <td>-0.026162</td>\n",
       "      <td>-0.011615</td>\n",
       "      <td>-0.231321</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.039610</td>\n",
       "      <td>-0.229335</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.050250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139043</td>\n",
       "      <td>0.123306</td>\n",
       "      <td>-0.073263</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.102984</td>\n",
       "      <td>0.055651</td>\n",
       "      <td>-0.032433</td>\n",
       "      <td>-0.082574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>-0.249494</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>-0.168136</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>0.041054</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067933</td>\n",
       "      <td>0.086227</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.039736</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>0.140933</td>\n",
       "      <td>-0.012769</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>-0.048643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>-0.099725</td>\n",
       "      <td>0.280859</td>\n",
       "      <td>0.035305</td>\n",
       "      <td>0.055052</td>\n",
       "      <td>0.066803</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.038965</td>\n",
       "      <td>-0.197699</td>\n",
       "      <td>0.072265</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143444</td>\n",
       "      <td>0.135385</td>\n",
       "      <td>-0.052328</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>-0.082166</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.094843</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.072419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>-0.002011</td>\n",
       "      <td>0.124149</td>\n",
       "      <td>-0.012404</td>\n",
       "      <td>0.076643</td>\n",
       "      <td>0.300641</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.082660</td>\n",
       "      <td>-0.099111</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.088329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025298</td>\n",
       "      <td>-0.188665</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>-0.121720</td>\n",
       "      <td>-0.019516</td>\n",
       "      <td>-0.057793</td>\n",
       "      <td>-0.134708</td>\n",
       "      <td>-0.021576</td>\n",
       "      <td>0.065409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <td>-0.014207</td>\n",
       "      <td>-0.051895</td>\n",
       "      <td>-0.065365</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>0.242847</td>\n",
       "      <td>-0.060283</td>\n",
       "      <td>-0.081119</td>\n",
       "      <td>-0.153714</td>\n",
       "      <td>-0.105502</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088216</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>0.063110</td>\n",
       "      <td>-0.042854</td>\n",
       "      <td>-0.248585</td>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.140672</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>-0.057184</td>\n",
       "      <td>-0.010675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>-0.124487</td>\n",
       "      <td>0.123299</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>0.076760</td>\n",
       "      <td>0.122557</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>-0.086286</td>\n",
       "      <td>-0.007715</td>\n",
       "      <td>-0.100940</td>\n",
       "      <td>-0.014237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090606</td>\n",
       "      <td>-0.065074</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.023920</td>\n",
       "      <td>0.155269</td>\n",
       "      <td>0.134975</td>\n",
       "      <td>0.084377</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>-0.051784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <td>-0.052581</td>\n",
       "      <td>0.090988</td>\n",
       "      <td>0.119070</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.055564</td>\n",
       "      <td>0.043385</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>-0.114068</td>\n",
       "      <td>0.162840</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065438</td>\n",
       "      <td>-0.074953</td>\n",
       "      <td>0.056090</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>-0.053827</td>\n",
       "      <td>-0.055392</td>\n",
       "      <td>0.041043</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>-0.047681</td>\n",
       "      <td>-0.048392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>-0.185433</td>\n",
       "      <td>-0.083624</td>\n",
       "      <td>-0.036671</td>\n",
       "      <td>-0.040051</td>\n",
       "      <td>0.042287</td>\n",
       "      <td>0.065492</td>\n",
       "      <td>-0.015215</td>\n",
       "      <td>0.014397</td>\n",
       "      <td>-0.114772</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004022</td>\n",
       "      <td>-0.200743</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.147879</td>\n",
       "      <td>-0.054279</td>\n",
       "      <td>0.063821</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.072677</td>\n",
       "      <td>-0.126859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addict</th>\n",
       "      <td>-0.022299</td>\n",
       "      <td>0.191387</td>\n",
       "      <td>-0.074030</td>\n",
       "      <td>-0.055573</td>\n",
       "      <td>0.310384</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>-0.085369</td>\n",
       "      <td>-0.100936</td>\n",
       "      <td>0.161640</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189984</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>-0.100763</td>\n",
       "      <td>0.062804</td>\n",
       "      <td>-0.072491</td>\n",
       "      <td>-0.054097</td>\n",
       "      <td>-0.050763</td>\n",
       "      <td>-0.024606</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.021930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addicted</th>\n",
       "      <td>-0.180987</td>\n",
       "      <td>0.108654</td>\n",
       "      <td>-0.209175</td>\n",
       "      <td>0.048430</td>\n",
       "      <td>0.220865</td>\n",
       "      <td>-0.021063</td>\n",
       "      <td>-0.116353</td>\n",
       "      <td>-0.085872</td>\n",
       "      <td>-0.082910</td>\n",
       "      <td>-0.066901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121385</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>-0.041007</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>-0.053710</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>-0.151991</td>\n",
       "      <td>-0.012026</td>\n",
       "      <td>-0.053500</td>\n",
       "      <td>0.047624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addicting</th>\n",
       "      <td>0.124235</td>\n",
       "      <td>0.220020</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.060788</td>\n",
       "      <td>0.151284</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>-0.154718</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>-0.035943</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020696</td>\n",
       "      <td>0.044556</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>-0.120047</td>\n",
       "      <td>0.072648</td>\n",
       "      <td>-0.037826</td>\n",
       "      <td>0.137466</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>-0.072847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addiction</th>\n",
       "      <td>-0.041202</td>\n",
       "      <td>0.211525</td>\n",
       "      <td>-0.219157</td>\n",
       "      <td>0.047944</td>\n",
       "      <td>0.194943</td>\n",
       "      <td>-0.022224</td>\n",
       "      <td>-0.121570</td>\n",
       "      <td>-0.164046</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180623</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>-0.135802</td>\n",
       "      <td>-0.037686</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>-0.016029</td>\n",
       "      <td>-0.183707</td>\n",
       "      <td>0.182267</td>\n",
       "      <td>-0.015915</td>\n",
       "      <td>0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addictive</th>\n",
       "      <td>-0.135284</td>\n",
       "      <td>0.265247</td>\n",
       "      <td>-0.142602</td>\n",
       "      <td>0.277454</td>\n",
       "      <td>0.203590</td>\n",
       "      <td>0.044446</td>\n",
       "      <td>-0.005216</td>\n",
       "      <td>-0.042800</td>\n",
       "      <td>0.039721</td>\n",
       "      <td>0.108935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034896</td>\n",
       "      <td>-0.014617</td>\n",
       "      <td>-0.072891</td>\n",
       "      <td>-0.068128</td>\n",
       "      <td>-0.142953</td>\n",
       "      <td>-0.042299</td>\n",
       "      <td>-0.033439</td>\n",
       "      <td>0.133010</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>0.158381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addition</th>\n",
       "      <td>0.067104</td>\n",
       "      <td>0.186199</td>\n",
       "      <td>-0.142296</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.023728</td>\n",
       "      <td>-0.247746</td>\n",
       "      <td>-0.159316</td>\n",
       "      <td>-0.016654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006716</td>\n",
       "      <td>-0.130061</td>\n",
       "      <td>0.046863</td>\n",
       "      <td>0.101913</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.054943</td>\n",
       "      <td>0.125698</td>\n",
       "      <td>0.100385</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>-0.031337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>-0.210274</td>\n",
       "      <td>0.107353</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>0.136388</td>\n",
       "      <td>0.195531</td>\n",
       "      <td>0.053409</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.109844</td>\n",
       "      <td>-0.016581</td>\n",
       "      <td>-0.047279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108592</td>\n",
       "      <td>0.088655</td>\n",
       "      <td>-0.043294</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>-0.045943</td>\n",
       "      <td>-0.083427</td>\n",
       "      <td>-0.278405</td>\n",
       "      <td>-0.181813</td>\n",
       "      <td>0.149525</td>\n",
       "      <td>0.186288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will_not</th>\n",
       "      <td>-0.129235</td>\n",
       "      <td>0.015577</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.126605</td>\n",
       "      <td>0.071264</td>\n",
       "      <td>0.038460</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.035405</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.108512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039451</td>\n",
       "      <td>0.173878</td>\n",
       "      <td>0.027983</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.006654</td>\n",
       "      <td>-0.099778</td>\n",
       "      <td>-0.146238</td>\n",
       "      <td>-0.187370</td>\n",
       "      <td>-0.020311</td>\n",
       "      <td>-0.008023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>-0.065997</td>\n",
       "      <td>0.051860</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>-0.128991</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.201266</td>\n",
       "      <td>-0.089693</td>\n",
       "      <td>0.198117</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>0.220913</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>-0.068581</td>\n",
       "      <td>-0.105390</td>\n",
       "      <td>-0.068331</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.055815</td>\n",
       "      <td>-0.045385</td>\n",
       "      <td>-0.174301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window</th>\n",
       "      <td>-0.124794</td>\n",
       "      <td>0.047326</td>\n",
       "      <td>-0.135098</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.024452</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>0.103574</td>\n",
       "      <td>-0.065450</td>\n",
       "      <td>-0.160271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157480</td>\n",
       "      <td>0.075503</td>\n",
       "      <td>0.128558</td>\n",
       "      <td>-0.207491</td>\n",
       "      <td>0.143063</td>\n",
       "      <td>0.048720</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>0.151653</td>\n",
       "      <td>0.118451</td>\n",
       "      <td>-0.048937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>0.157742</td>\n",
       "      <td>0.211656</td>\n",
       "      <td>0.020748</td>\n",
       "      <td>-0.136406</td>\n",
       "      <td>0.062554</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.095738</td>\n",
       "      <td>0.090334</td>\n",
       "      <td>-0.082855</td>\n",
       "      <td>-0.145515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046139</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>-0.134025</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>0.025053</td>\n",
       "      <td>-0.010894</td>\n",
       "      <td>-0.032358</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>0.100498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>-0.036164</td>\n",
       "      <td>0.063715</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.248363</td>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.038604</td>\n",
       "      <td>0.141808</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>-0.121845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006780</td>\n",
       "      <td>0.145184</td>\n",
       "      <td>0.071114</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.111052</td>\n",
       "      <td>-0.197036</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.042903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.110637</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.036366</td>\n",
       "      <td>-0.035389</td>\n",
       "      <td>-0.121819</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>0.108040</td>\n",
       "      <td>-0.199105</td>\n",
       "      <td>-0.103509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.063284</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>-0.083893</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>0.077423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>-0.045878</td>\n",
       "      <td>-0.054326</td>\n",
       "      <td>0.106562</td>\n",
       "      <td>0.250029</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.076788</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>-0.094184</td>\n",
       "      <td>0.037543</td>\n",
       "      <td>0.172067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>-0.045535</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>-0.041055</td>\n",
       "      <td>0.293838</td>\n",
       "      <td>-0.082126</td>\n",
       "      <td>-0.001794</td>\n",
       "      <td>0.103639</td>\n",
       "      <td>-0.033845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.116637</td>\n",
       "      <td>-0.073446</td>\n",
       "      <td>-0.211282</td>\n",
       "      <td>-0.112790</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.096693</td>\n",
       "      <td>0.166608</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.154851</td>\n",
       "      <td>-0.034203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>-0.172235</td>\n",
       "      <td>-0.083438</td>\n",
       "      <td>-0.119303</td>\n",
       "      <td>-0.044952</td>\n",
       "      <td>0.023880</td>\n",
       "      <td>-0.010212</td>\n",
       "      <td>-0.037849</td>\n",
       "      <td>-0.073563</td>\n",
       "      <td>-0.151407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker</th>\n",
       "      <td>-0.022504</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.057094</td>\n",
       "      <td>-0.108500</td>\n",
       "      <td>0.057872</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.098129</td>\n",
       "      <td>0.028791</td>\n",
       "      <td>-0.088584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>-0.220155</td>\n",
       "      <td>-0.013423</td>\n",
       "      <td>-0.171200</td>\n",
       "      <td>0.163627</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>0.070564</td>\n",
       "      <td>-0.101153</td>\n",
       "      <td>-0.005687</td>\n",
       "      <td>0.089583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>-0.031874</td>\n",
       "      <td>0.139463</td>\n",
       "      <td>-0.143214</td>\n",
       "      <td>-0.134749</td>\n",
       "      <td>-0.035267</td>\n",
       "      <td>-0.100296</td>\n",
       "      <td>-0.087241</td>\n",
       "      <td>0.060740</td>\n",
       "      <td>-0.058429</td>\n",
       "      <td>-0.146486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228013</td>\n",
       "      <td>-0.096753</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.044096</td>\n",
       "      <td>0.204079</td>\n",
       "      <td>-0.045788</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>-0.024602</td>\n",
       "      <td>-0.044594</td>\n",
       "      <td>0.056458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>-0.149530</td>\n",
       "      <td>0.066812</td>\n",
       "      <td>-0.116020</td>\n",
       "      <td>0.057878</td>\n",
       "      <td>0.186423</td>\n",
       "      <td>0.031979</td>\n",
       "      <td>-0.065942</td>\n",
       "      <td>-0.130018</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>-0.087113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>-0.053344</td>\n",
       "      <td>-0.051258</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>-0.053174</td>\n",
       "      <td>-0.227502</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.078444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worry</th>\n",
       "      <td>-0.115249</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>-0.144226</td>\n",
       "      <td>0.076544</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>-0.062495</td>\n",
       "      <td>0.074391</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.009033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173196</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.086681</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>-0.174869</td>\n",
       "      <td>0.051112</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>0.095718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0.066123</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.183848</td>\n",
       "      <td>-0.068840</td>\n",
       "      <td>0.141505</td>\n",
       "      <td>-0.098121</td>\n",
       "      <td>-0.058005</td>\n",
       "      <td>-0.064479</td>\n",
       "      <td>0.136190</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104596</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>-0.104555</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.083674</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>-0.317687</td>\n",
       "      <td>-0.199293</td>\n",
       "      <td>-0.058497</td>\n",
       "      <td>-0.112080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>-0.129512</td>\n",
       "      <td>-0.066839</td>\n",
       "      <td>-0.048920</td>\n",
       "      <td>0.171823</td>\n",
       "      <td>-0.177231</td>\n",
       "      <td>-0.010816</td>\n",
       "      <td>-0.117696</td>\n",
       "      <td>0.048246</td>\n",
       "      <td>-0.043790</td>\n",
       "      <td>0.162369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081206</td>\n",
       "      <td>-0.146629</td>\n",
       "      <td>0.106912</td>\n",
       "      <td>-0.111590</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>0.156758</td>\n",
       "      <td>-0.080771</td>\n",
       "      <td>-0.085659</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>-0.056529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_essay</th>\n",
       "      <td>-0.276980</td>\n",
       "      <td>-0.092888</td>\n",
       "      <td>0.029463</td>\n",
       "      <td>-0.055676</td>\n",
       "      <td>-0.178226</td>\n",
       "      <td>0.107930</td>\n",
       "      <td>-0.072077</td>\n",
       "      <td>0.043175</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>0.241753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096314</td>\n",
       "      <td>-0.044447</td>\n",
       "      <td>0.018310</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>-0.042781</td>\n",
       "      <td>0.205939</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.039154</td>\n",
       "      <td>0.167516</td>\n",
       "      <td>-0.085724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_letter</th>\n",
       "      <td>-0.094048</td>\n",
       "      <td>-0.188189</td>\n",
       "      <td>0.183612</td>\n",
       "      <td>-0.083681</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>-0.073572</td>\n",
       "      <td>-0.097978</td>\n",
       "      <td>0.031064</td>\n",
       "      <td>0.052986</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>-0.225296</td>\n",
       "      <td>-0.089557</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.155174</td>\n",
       "      <td>0.060816</td>\n",
       "      <td>-0.104516</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>-0.053705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_paper</th>\n",
       "      <td>-0.039994</td>\n",
       "      <td>-0.162954</td>\n",
       "      <td>0.073475</td>\n",
       "      <td>0.183336</td>\n",
       "      <td>-0.065429</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>-0.081525</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.197878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022393</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>0.112994</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>-0.064076</td>\n",
       "      <td>0.253173</td>\n",
       "      <td>0.083666</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.068730</td>\n",
       "      <td>-0.072751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>0.003102</td>\n",
       "      <td>-0.065624</td>\n",
       "      <td>0.116759</td>\n",
       "      <td>0.221152</td>\n",
       "      <td>-0.092194</td>\n",
       "      <td>-0.004935</td>\n",
       "      <td>0.056955</td>\n",
       "      <td>0.010158</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.262661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.107070</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>-0.044422</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>-0.155263</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>-0.121560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong</th>\n",
       "      <td>-0.159924</td>\n",
       "      <td>-0.071432</td>\n",
       "      <td>0.066980</td>\n",
       "      <td>0.063522</td>\n",
       "      <td>0.202766</td>\n",
       "      <td>-0.027204</td>\n",
       "      <td>-0.115663</td>\n",
       "      <td>-0.068590</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.168906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>0.031521</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>0.315482</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>-0.107264</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>-0.060642</td>\n",
       "      <td>0.067841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo</th>\n",
       "      <td>0.118643</td>\n",
       "      <td>0.078907</td>\n",
       "      <td>0.077289</td>\n",
       "      <td>0.124349</td>\n",
       "      <td>0.050914</td>\n",
       "      <td>-0.077911</td>\n",
       "      <td>-0.201298</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.066481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080930</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>-0.174040</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.076351</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>-0.025008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-0.084354</td>\n",
       "      <td>0.051403</td>\n",
       "      <td>-0.092265</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>0.095282</td>\n",
       "      <td>0.027088</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>0.102618</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016205</td>\n",
       "      <td>0.062934</td>\n",
       "      <td>-0.077875</td>\n",
       "      <td>0.093595</td>\n",
       "      <td>-0.052626</td>\n",
       "      <td>0.079668</td>\n",
       "      <td>-0.189148</td>\n",
       "      <td>-0.176204</td>\n",
       "      <td>0.061498</td>\n",
       "      <td>-0.014047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_ago</th>\n",
       "      <td>0.143823</td>\n",
       "      <td>0.146433</td>\n",
       "      <td>0.056964</td>\n",
       "      <td>-0.035602</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>-0.061437</td>\n",
       "      <td>-0.182808</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>-0.046499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061817</td>\n",
       "      <td>0.123939</td>\n",
       "      <td>-0.093537</td>\n",
       "      <td>0.038132</td>\n",
       "      <td>-0.105527</td>\n",
       "      <td>-0.060461</td>\n",
       "      <td>0.151425</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>-0.035396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_old</th>\n",
       "      <td>-0.265906</td>\n",
       "      <td>0.137993</td>\n",
       "      <td>0.102887</td>\n",
       "      <td>-0.057300</td>\n",
       "      <td>0.240518</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>-0.163350</td>\n",
       "      <td>0.102529</td>\n",
       "      <td>0.084942</td>\n",
       "      <td>-0.083565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021522</td>\n",
       "      <td>0.070995</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>-0.011797</td>\n",
       "      <td>-0.099303</td>\n",
       "      <td>-0.123955</td>\n",
       "      <td>-0.019513</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.006188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>-0.113401</td>\n",
       "      <td>-0.039949</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.141987</td>\n",
       "      <td>0.136337</td>\n",
       "      <td>-0.221333</td>\n",
       "      <td>-0.167595</td>\n",
       "      <td>-0.077956</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.090622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025857</td>\n",
       "      <td>-0.018503</td>\n",
       "      <td>-0.081860</td>\n",
       "      <td>0.103133</td>\n",
       "      <td>0.075920</td>\n",
       "      <td>-0.016118</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-0.014454</td>\n",
       "      <td>-0.042515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>-0.090288</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.083452</td>\n",
       "      <td>-0.021273</td>\n",
       "      <td>0.105388</td>\n",
       "      <td>0.089513</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.122347</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>-0.106663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.205986</td>\n",
       "      <td>0.165020</td>\n",
       "      <td>-0.191593</td>\n",
       "      <td>-0.152620</td>\n",
       "      <td>0.069355</td>\n",
       "      <td>0.092479</td>\n",
       "      <td>0.042936</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>-0.003103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young_child</th>\n",
       "      <td>-0.088664</td>\n",
       "      <td>0.103843</td>\n",
       "      <td>0.080633</td>\n",
       "      <td>0.131359</td>\n",
       "      <td>0.181052</td>\n",
       "      <td>-0.050389</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>-0.133385</td>\n",
       "      <td>-0.011312</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087158</td>\n",
       "      <td>0.076534</td>\n",
       "      <td>-0.033679</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-0.271744</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>-0.021692</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>-0.151703</td>\n",
       "      <td>-0.009119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>-0.011421</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>-0.077571</td>\n",
       "      <td>-0.061826</td>\n",
       "      <td>-0.050209</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>-0.023727</td>\n",
       "      <td>-0.232815</td>\n",
       "      <td>0.234117</td>\n",
       "      <td>0.036047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>-0.023236</td>\n",
       "      <td>-0.018881</td>\n",
       "      <td>0.106277</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>0.147399</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.256224</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.011791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>-0.039809</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.109364</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.127894</td>\n",
       "      <td>-0.047535</td>\n",
       "      <td>-0.050082</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.067090</td>\n",
       "      <td>-0.066447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053343</td>\n",
       "      <td>-0.015251</td>\n",
       "      <td>-0.038807</td>\n",
       "      <td>-0.086042</td>\n",
       "      <td>-0.101111</td>\n",
       "      <td>-0.011986</td>\n",
       "      <td>0.064623</td>\n",
       "      <td>0.043936</td>\n",
       "      <td>-0.041173</td>\n",
       "      <td>-0.009698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â€™</th>\n",
       "      <td>-0.140483</td>\n",
       "      <td>0.123493</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>-0.166562</td>\n",
       "      <td>-0.131816</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>-0.027659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>-0.125255</td>\n",
       "      <td>0.232657</td>\n",
       "      <td>0.127279</td>\n",
       "      <td>-0.003595</td>\n",
       "      <td>-0.043506</td>\n",
       "      <td>-0.029938</td>\n",
       "      <td>-0.100074</td>\n",
       "      <td>0.238505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2         3         4   \\\n",
       "Dear_Local_Newspaper        -0.080966  0.055885  0.006367  0.160840  0.153978   \n",
       "Dear_Newspaper              -0.160894 -0.078554 -0.005497  0.128657  0.114997   \n",
       "Dr.                         -0.048280  0.085749 -0.078311 -0.077604  0.105933   \n",
       "Facebook                     0.160234  0.161505 -0.004742  0.027028  0.042157   \n",
       "People                      -0.054791  0.059666 -0.006936 -0.049464  0.207019   \n",
       "ability                      0.061915  0.095871 -0.082138  0.038366  0.046556   \n",
       "ability_learn                0.078620 -0.060835  0.106955 -0.152974  0.055909   \n",
       "ability_learn_far_away       0.068378  0.036706  0.083156 -0.106033  0.218467   \n",
       "ability_learn_faraway_place -0.043237 -0.050755  0.093305  0.010634  0.205478   \n",
       "able                         0.023570 -0.053252  0.063354 -0.085435  0.164259   \n",
       "absolutely                   0.009471 -0.037858  0.118418  0.268994  0.313056   \n",
       "abuse                       -0.060129  0.100730  0.098544  0.117001  0.130719   \n",
       "access                      -0.016774  0.015856 -0.144403 -0.247768 -0.151972   \n",
       "accord                       0.023603  0.048625 -0.031065 -0.005102  0.164677   \n",
       "account                     -0.034982  0.061856 -0.047256 -0.047565  0.001725   \n",
       "accurate                     0.086073 -0.032643  0.139284  0.028096 -0.101803   \n",
       "acess                        0.080051 -0.026162 -0.011615 -0.231321  0.000147   \n",
       "act                         -0.249494  0.017735 -0.168136  0.125373  0.117684   \n",
       "action                      -0.099725  0.280859  0.035305  0.055052  0.066803   \n",
       "active                      -0.002011  0.124149 -0.012404  0.076643  0.300641   \n",
       "activity                    -0.014207 -0.051895 -0.065365  0.077290  0.242847   \n",
       "actually                    -0.124487  0.123299 -0.047490  0.076760  0.122557   \n",
       "ad                          -0.052581  0.090988  0.119070  0.018722  0.055564   \n",
       "add                         -0.185433 -0.083624 -0.036671 -0.040051  0.042287   \n",
       "addict                      -0.022299  0.191387 -0.074030 -0.055573  0.310384   \n",
       "addicted                    -0.180987  0.108654 -0.209175  0.048430  0.220865   \n",
       "addicting                    0.124235  0.220020  0.008297  0.060788  0.151284   \n",
       "addiction                   -0.041202  0.211525 -0.219157  0.047944  0.194943   \n",
       "addictive                   -0.135284  0.265247 -0.142602  0.277454  0.203590   \n",
       "addition                     0.067104  0.186199 -0.142296  0.034541 -0.001580   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                        -0.210274  0.107353  0.011885  0.136388  0.195531   \n",
       "will_not                    -0.129235  0.015577  0.009054  0.126605  0.071264   \n",
       "win                         -0.065997  0.051860  0.036722 -0.128991  0.104487   \n",
       "window                      -0.124794  0.047326 -0.135098  0.049944  0.008427   \n",
       "wish                         0.157742  0.211656  0.020748 -0.136406  0.062554   \n",
       "wonder                      -0.036164  0.063715  0.017805 -0.000229  0.248363   \n",
       "wonderful                    0.200375  0.110637  0.073242 -0.036366 -0.035389   \n",
       "word                        -0.045878 -0.054326  0.106562  0.250029  0.028261   \n",
       "work                        -0.116637 -0.073446 -0.211282 -0.112790  0.074468   \n",
       "worker                      -0.022504  0.003409 -0.090817 -0.057094 -0.108500   \n",
       "world                       -0.031874  0.139463 -0.143214 -0.134749 -0.035267   \n",
       "worried                     -0.149530  0.066812 -0.116020  0.057878  0.186423   \n",
       "worry                       -0.115249  0.006581  0.003609 -0.144226  0.076544   \n",
       "worth                        0.066123  0.025590  0.183848 -0.068840  0.141505   \n",
       "write                       -0.129512 -0.066839 -0.048920  0.171823 -0.177231   \n",
       "write_essay                 -0.276980 -0.092888  0.029463 -0.055676 -0.178226   \n",
       "write_letter                -0.094048 -0.188189  0.183612 -0.083681  0.003714   \n",
       "write_paper                 -0.039994 -0.162954  0.073475  0.183336 -0.065429   \n",
       "writing                      0.003102 -0.065624  0.116759  0.221152 -0.092194   \n",
       "wrong                       -0.159924 -0.071432  0.066980  0.063522  0.202766   \n",
       "yahoo                        0.118643  0.078907  0.077289  0.124349  0.050914   \n",
       "year                        -0.084354  0.051403 -0.092265  0.017354  0.049213   \n",
       "year_ago                     0.143823  0.146433  0.056964 -0.035602  0.032863   \n",
       "year_old                    -0.265906  0.137993  0.102887 -0.057300  0.240518   \n",
       "yes                         -0.113401 -0.039949  0.003711  0.141987  0.136337   \n",
       "young                       -0.090288  0.205408  0.083452 -0.021273  0.105388   \n",
       "young_child                 -0.088664  0.103843  0.080633  0.131359  0.181052   \n",
       "youth                       -0.011421  0.034357 -0.077571 -0.061826 -0.050209   \n",
       "youtube                     -0.039809  0.143098  0.109364  0.029712  0.127894   \n",
       "â€™                           -0.140483  0.123493  0.016328  0.021604  0.155623   \n",
       "\n",
       "                                   5         6         7         8         9   \\\n",
       "Dear_Local_Newspaper        -0.150762 -0.227000 -0.119609  0.028311  0.119844   \n",
       "Dear_Newspaper              -0.129569 -0.273821 -0.217280 -0.019305  0.088784   \n",
       "Dr.                          0.043426 -0.245606 -0.125072  0.157281  0.109850   \n",
       "Facebook                     0.097615 -0.074445  0.072042 -0.013195  0.026856   \n",
       "People                       0.001976  0.008313  0.076224 -0.133590 -0.162718   \n",
       "ability                     -0.055796 -0.045085 -0.001371 -0.175662 -0.038806   \n",
       "ability_learn                0.004751 -0.215835 -0.024729  0.032057 -0.039019   \n",
       "ability_learn_far_away       0.031863 -0.183279 -0.111118 -0.029186  0.013766   \n",
       "ability_learn_faraway_place -0.025085 -0.212736 -0.158275 -0.032956  0.036593   \n",
       "able                        -0.058869  0.080843  0.106354 -0.153176 -0.034961   \n",
       "absolutely                   0.026245 -0.136287 -0.139082 -0.000933  0.011531   \n",
       "abuse                        0.050971  0.028348 -0.110445 -0.042671  0.020960   \n",
       "access                      -0.002618 -0.021094 -0.106052 -0.072616  0.058271   \n",
       "accord                       0.130134 -0.074918 -0.004850  0.192992  0.155685   \n",
       "account                      0.026578 -0.001665 -0.015990 -0.091662  0.074502   \n",
       "accurate                    -0.074570 -0.103560  0.101796 -0.118272 -0.060125   \n",
       "acess                        0.039610 -0.229335 -0.029583  0.013439  0.050250   \n",
       "act                          0.136604  0.054115 -0.005799  0.041054 -0.051953   \n",
       "action                       0.053238 -0.038965 -0.197699  0.072265 -0.003704   \n",
       "active                       0.060930  0.082660 -0.099111  0.100015  0.088329   \n",
       "activity                    -0.060283 -0.081119 -0.153714 -0.105502  0.068179   \n",
       "actually                     0.006980 -0.086286 -0.007715 -0.100940 -0.014237   \n",
       "ad                           0.043385  0.015693 -0.114068  0.162840 -0.156114   \n",
       "add                          0.065492 -0.015215  0.014397 -0.114772  0.088571   \n",
       "addict                       0.063680 -0.085369 -0.100936  0.161640  0.005782   \n",
       "addicted                    -0.021063 -0.116353 -0.085872 -0.082910 -0.066901   \n",
       "addicting                    0.075862 -0.154718 -0.115967 -0.035943  0.065051   \n",
       "addiction                   -0.022224 -0.121570 -0.164046  0.000114  0.037053   \n",
       "addictive                    0.044446 -0.005216 -0.042800  0.039721  0.108935   \n",
       "addition                    -0.000304 -0.023728 -0.247746 -0.159316 -0.016654   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                         0.053409  0.076249  0.109844 -0.016581 -0.047279   \n",
       "will_not                     0.038460  0.011696  0.035405  0.003678 -0.108512   \n",
       "win                          0.201266 -0.089693  0.198117  0.128069 -0.058174   \n",
       "window                      -0.024452  0.082130  0.103574 -0.065450 -0.160271   \n",
       "wish                         0.185311  0.095738  0.090334 -0.082855 -0.145515   \n",
       "wonder                      -0.063726 -0.038604  0.141808  0.016108 -0.121845   \n",
       "wonderful                   -0.121819 -0.019157  0.108040 -0.199105 -0.103509   \n",
       "word                        -0.076788  0.055572 -0.094184  0.037543  0.172067   \n",
       "work                         0.096693  0.166608 -0.013387 -0.154851 -0.034203   \n",
       "worker                       0.057872  0.029849 -0.098129  0.028791 -0.088584   \n",
       "world                       -0.100296 -0.087241  0.060740 -0.058429 -0.146486   \n",
       "worried                      0.031979 -0.065942 -0.130018 -0.005233 -0.087113   \n",
       "worry                        0.149391 -0.062495  0.074391  0.156100  0.009033   \n",
       "worth                       -0.098121 -0.058005 -0.064479  0.136190  0.007481   \n",
       "write                       -0.010816 -0.117696  0.048246 -0.043790  0.162369   \n",
       "write_essay                  0.107930 -0.072077  0.043175 -0.009157  0.241753   \n",
       "write_letter                -0.073572 -0.097978  0.031064  0.052986  0.004131   \n",
       "write_paper                  0.028342 -0.081525  0.051980  0.021970  0.197878   \n",
       "writing                     -0.004935  0.056955  0.010158  0.036182  0.262661   \n",
       "wrong                       -0.027204 -0.115663 -0.068590  0.176000  0.168906   \n",
       "yahoo                       -0.077911 -0.201298  0.124579 -0.008861  0.066481   \n",
       "year                         0.095282  0.027088  0.115821  0.102618  0.017117   \n",
       "year_ago                    -0.061437 -0.182808  0.041300  0.014933 -0.046499   \n",
       "year_old                     0.011261 -0.163350  0.102529  0.084942 -0.083565   \n",
       "yes                         -0.221333 -0.167595 -0.077956 -0.124617  0.090622   \n",
       "young                        0.089513 -0.048530 -0.122347  0.026380 -0.106663   \n",
       "young_child                 -0.050389  0.063325 -0.133385 -0.011312  0.003758   \n",
       "youth                       -0.039554 -0.023727 -0.232815  0.234117  0.036047   \n",
       "youtube                     -0.047535 -0.050082  0.001353  0.067090 -0.066447   \n",
       "â€™                           -0.004921 -0.166562 -0.131816  0.039862 -0.027659   \n",
       "\n",
       "                             ...        90        91        92        93  \\\n",
       "Dear_Local_Newspaper         ...  0.083057 -0.036303 -0.172746  0.107085   \n",
       "Dear_Newspaper               ...  0.131236 -0.100623 -0.083030 -0.025299   \n",
       "Dr.                          ...  0.042051  0.012459 -0.073568  0.167597   \n",
       "Facebook                     ...  0.045397  0.061529 -0.110230 -0.138034   \n",
       "People                       ... -0.117228 -0.056509 -0.127031 -0.096924   \n",
       "ability                      ...  0.113731 -0.003393  0.111721 -0.185697   \n",
       "ability_learn                ...  0.200842  0.038983 -0.149448 -0.010260   \n",
       "ability_learn_far_away       ...  0.214203 -0.048395 -0.052231 -0.211182   \n",
       "ability_learn_faraway_place  ...  0.275119 -0.010331 -0.027732 -0.158186   \n",
       "able                         ...  0.029970 -0.001847 -0.063932 -0.096556   \n",
       "absolutely                   ... -0.134323 -0.086783 -0.121617 -0.086920   \n",
       "abuse                        ...  0.138893 -0.028911 -0.187556  0.004646   \n",
       "access                       ...  0.138290  0.049777 -0.109659 -0.086719   \n",
       "accord                       ...  0.029439 -0.012145 -0.067884 -0.066159   \n",
       "account                      ... -0.081644 -0.045451 -0.062248 -0.130844   \n",
       "accurate                     ...  0.047950 -0.004177 -0.047235  0.125650   \n",
       "acess                        ...  0.139043  0.123306 -0.073263  0.036500   \n",
       "act                          ...  0.067933  0.086227  0.030296  0.039736   \n",
       "action                       ... -0.143444  0.135385 -0.052328  0.090600   \n",
       "active                       ... -0.025298 -0.188665 -0.017035  0.020137   \n",
       "activity                     ... -0.088216 -0.000491  0.063110 -0.042854   \n",
       "actually                     ...  0.090606 -0.065074  0.019000  0.023920   \n",
       "ad                           ...  0.065438 -0.074953  0.056090  0.006360   \n",
       "add                          ... -0.004022 -0.200743  0.000062 -0.147879   \n",
       "addict                       ... -0.189984 -0.014849 -0.100763  0.062804   \n",
       "addicted                     ... -0.121385  0.001723 -0.041007  0.033138   \n",
       "addicting                    ... -0.020696  0.044556 -0.041164 -0.002323   \n",
       "addiction                    ... -0.180623  0.028451 -0.135802 -0.037686   \n",
       "addictive                    ... -0.034896 -0.014617 -0.072891 -0.068128   \n",
       "addition                     ... -0.006716 -0.130061  0.046863  0.101913   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "will                         ... -0.108592  0.088655 -0.043294  0.019784   \n",
       "will_not                     ... -0.039451  0.173878  0.027983  0.000860   \n",
       "win                          ... -0.057563  0.220913  0.009554 -0.068581   \n",
       "window                       ... -0.157480  0.075503  0.128558 -0.207491   \n",
       "wish                         ...  0.046139  0.137625 -0.134025 -0.010959   \n",
       "wonder                       ... -0.006780  0.145184  0.071114  0.042687   \n",
       "wonderful                    ...  0.032440  0.067650  0.142903 -0.063284   \n",
       "word                         ...  0.037526 -0.045535  0.082279  0.013238   \n",
       "work                         ...  0.015053 -0.172235 -0.083438 -0.119303   \n",
       "worker                       ... -0.002502 -0.220155 -0.013423 -0.171200   \n",
       "world                        ...  0.228013 -0.096753 -0.002665 -0.044096   \n",
       "worried                      ...  0.092626 -0.053344 -0.051258  0.146055   \n",
       "worry                        ... -0.173196  0.016125  0.086681 -0.016223   \n",
       "worth                        ... -0.104596  0.087585 -0.104555  0.040626   \n",
       "write                        ...  0.081206 -0.146629  0.106912 -0.111590   \n",
       "write_essay                  ...  0.096314 -0.044447  0.018310 -0.127319   \n",
       "write_letter                 ...  0.122145 -0.225296 -0.089557  0.001705   \n",
       "write_paper                  ...  0.022393 -0.094506  0.112994  0.026295   \n",
       "writing                      ...  0.052755 -0.107070  0.100780 -0.044422   \n",
       "wrong                        ...  0.037976  0.031521  0.092895  0.315482   \n",
       "yahoo                        ...  0.080930  0.005401  0.000851 -0.174040   \n",
       "year                         ... -0.016205  0.062934 -0.077875  0.093595   \n",
       "year_ago                     ... -0.061817  0.123939 -0.093537  0.038132   \n",
       "year_old                     ... -0.021522  0.070995  0.019442 -0.011797   \n",
       "yes                          ... -0.025857 -0.018503 -0.081860  0.103133   \n",
       "young                        ...  0.069264  0.205986  0.165020 -0.191593   \n",
       "young_child                  ...  0.087158  0.076534 -0.033679 -0.017185   \n",
       "youth                        ...  0.002169 -0.023236 -0.018881  0.106277   \n",
       "youtube                      ... -0.053343 -0.015251 -0.038807 -0.086042   \n",
       "â€™                            ...  0.000081  0.092527 -0.125255  0.232657   \n",
       "\n",
       "                                   94        95        96        97        98  \\\n",
       "Dear_Local_Newspaper        -0.002207 -0.054023 -0.026584  0.041401  0.089317   \n",
       "Dear_Newspaper               0.176573 -0.096566 -0.069011  0.045533  0.033215   \n",
       "Dr.                         -0.038416  0.056799  0.049680  0.022879 -0.012547   \n",
       "Facebook                    -0.101535  0.122856  0.225491  0.016016  0.037411   \n",
       "People                       0.016651 -0.033707 -0.080514 -0.129022  0.033612   \n",
       "ability                      0.000819  0.260463  0.133326 -0.022905  0.135234   \n",
       "ability_learn               -0.103228  0.045211  0.028685  0.023814  0.066922   \n",
       "ability_learn_far_away      -0.067038 -0.046495 -0.022695 -0.043916 -0.026545   \n",
       "ability_learn_faraway_place -0.004436 -0.042032  0.026861 -0.012597  0.061112   \n",
       "able                        -0.110397  0.145599  0.058895 -0.044561  0.179839   \n",
       "absolutely                   0.001776 -0.044351 -0.079989  0.007832 -0.135252   \n",
       "abuse                       -0.081402 -0.078189 -0.054812 -0.072359  0.006348   \n",
       "access                       0.018677  0.038707  0.214862  0.108611 -0.051788   \n",
       "accord                      -0.064794  0.077432  0.110461 -0.009767 -0.051933   \n",
       "account                      0.021602  0.045767  0.112684  0.020622 -0.031739   \n",
       "accurate                     0.033922 -0.016223  0.088164  0.177889 -0.017273   \n",
       "acess                        0.029464  0.010733  0.102984  0.055651 -0.032433   \n",
       "act                          0.058830  0.093552  0.140933 -0.012769 -0.014208   \n",
       "action                       0.018039 -0.082166  0.013245  0.094843  0.064940   \n",
       "active                      -0.121720 -0.019516 -0.057793 -0.134708 -0.021576   \n",
       "activity                    -0.248585  0.037895  0.140672  0.000508 -0.057184   \n",
       "actually                     0.155269  0.134975  0.084377 -0.108588  0.016488   \n",
       "ad                          -0.053827 -0.055392  0.041043  0.039948 -0.047681   \n",
       "add                         -0.054279  0.063821  0.003096  0.019184  0.072677   \n",
       "addict                      -0.072491 -0.054097 -0.050763 -0.024606  0.045586   \n",
       "addicted                    -0.053710  0.002833 -0.151991 -0.012026 -0.053500   \n",
       "addicting                   -0.120047  0.072648 -0.037826  0.137466  0.007619   \n",
       "addiction                    0.013853 -0.016029 -0.183707  0.182267 -0.015915   \n",
       "addictive                   -0.142953 -0.042299 -0.033439  0.133010 -0.002407   \n",
       "addition                     0.006348 -0.054943  0.125698  0.100385  0.147561   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                        -0.045943 -0.083427 -0.278405 -0.181813  0.149525   \n",
       "will_not                    -0.006654 -0.099778 -0.146238 -0.187370 -0.020311   \n",
       "win                         -0.105390 -0.068331  0.083326  0.055815 -0.045385   \n",
       "window                       0.143063  0.048720 -0.010025  0.151653  0.118451   \n",
       "wish                         0.025053 -0.010894 -0.032358  0.013457  0.069877   \n",
       "wonder                       0.111052 -0.197036  0.003673  0.086538  0.035479   \n",
       "wonderful                    0.077990 -0.083893  0.029566  0.009894  0.029312   \n",
       "word                        -0.041055  0.293838 -0.082126 -0.001794  0.103639   \n",
       "work                        -0.044952  0.023880 -0.010212 -0.037849 -0.073563   \n",
       "worker                       0.163627  0.041264  0.070564 -0.101153 -0.005687   \n",
       "world                        0.204079 -0.045788  0.020667 -0.024602 -0.044594   \n",
       "worried                     -0.053174 -0.227502  0.013293  0.035557  0.054435   \n",
       "worry                        0.018278  0.057971 -0.174869  0.051112  0.063893   \n",
       "worth                        0.083674 -0.016526 -0.317687 -0.199293 -0.058497   \n",
       "write                       -0.037091  0.156758 -0.080771 -0.085659  0.063928   \n",
       "write_essay                 -0.042781  0.205939  0.047902  0.039154  0.167516   \n",
       "write_letter                 0.155174  0.060816 -0.104516  0.033624  0.010150   \n",
       "write_paper                 -0.064076  0.253173  0.083666 -0.006361  0.068730   \n",
       "writing                     -0.146656  0.136562 -0.155263 -0.034360  0.040746   \n",
       "wrong                        0.080589 -0.107264  0.047725  0.101677 -0.060642   \n",
       "yahoo                        0.058013  0.026157  0.105651  0.076351  0.092045   \n",
       "year                        -0.052626  0.079668 -0.189148 -0.176204  0.061498   \n",
       "year_ago                    -0.105527 -0.060461  0.151425 -0.005155  0.005458   \n",
       "year_old                    -0.099303 -0.123955 -0.019513 -0.002453 -0.178045   \n",
       "yes                          0.075920 -0.016118  0.022301  0.074965 -0.014454   \n",
       "young                       -0.152620  0.069355  0.092479  0.042936  0.004319   \n",
       "young_child                 -0.271744  0.113278 -0.021692  0.116293 -0.151703   \n",
       "youth                       -0.002704  0.147399  0.020246  0.256224  0.002119   \n",
       "youtube                     -0.101111 -0.011986  0.064623  0.043936 -0.041173   \n",
       "â€™                            0.127279 -0.003595 -0.043506 -0.029938 -0.100074   \n",
       "\n",
       "                                   99  \n",
       "Dear_Local_Newspaper        -0.006336  \n",
       "Dear_Newspaper               0.079021  \n",
       "Dr.                          0.038800  \n",
       "Facebook                    -0.037007  \n",
       "People                      -0.022756  \n",
       "ability                     -0.020684  \n",
       "ability_learn                0.105161  \n",
       "ability_learn_far_away       0.121319  \n",
       "ability_learn_faraway_place  0.114431  \n",
       "able                        -0.101470  \n",
       "absolutely                  -0.084513  \n",
       "abuse                       -0.008121  \n",
       "access                      -0.025838  \n",
       "accord                       0.118402  \n",
       "account                     -0.023910  \n",
       "accurate                    -0.106321  \n",
       "acess                       -0.082574  \n",
       "act                         -0.048643  \n",
       "action                       0.072419  \n",
       "active                       0.065409  \n",
       "activity                    -0.010675  \n",
       "actually                    -0.051784  \n",
       "ad                          -0.048392  \n",
       "add                         -0.126859  \n",
       "addict                       0.021930  \n",
       "addicted                     0.047624  \n",
       "addicting                   -0.072847  \n",
       "addiction                    0.002725  \n",
       "addictive                    0.158381  \n",
       "addition                    -0.031337  \n",
       "...                               ...  \n",
       "will                         0.186288  \n",
       "will_not                    -0.008023  \n",
       "win                         -0.174301  \n",
       "window                      -0.048937  \n",
       "wish                         0.100498  \n",
       "wonder                       0.042903  \n",
       "wonderful                    0.077423  \n",
       "word                        -0.033845  \n",
       "work                        -0.151407  \n",
       "worker                       0.089583  \n",
       "world                        0.056458  \n",
       "worried                      0.078444  \n",
       "worry                        0.095718  \n",
       "worth                       -0.112080  \n",
       "write                       -0.056529  \n",
       "write_essay                 -0.085724  \n",
       "write_letter                -0.053705  \n",
       "write_paper                 -0.072751  \n",
       "writing                     -0.121560  \n",
       "wrong                        0.067841  \n",
       "yahoo                       -0.025008  \n",
       "year                        -0.014047  \n",
       "year_ago                    -0.035396  \n",
       "year_old                     0.006188  \n",
       "yes                         -0.042515  \n",
       "young                       -0.003103  \n",
       "young_child                 -0.009119  \n",
       "youth                        0.011791  \n",
       "youtube                     -0.009698  \n",
       "â€™                            0.238505  \n",
       "\n",
       "[1250 rows x 100 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in essay2vec_model.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(essay2vec_model.wv.syn0norm[term_indices, :], index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 1,257 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=5):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in essay2vec_model.wv.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print('{:20} {}'.format(word, round(similarity, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What things are like Facebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter              0.667\n",
      "facebook_myspace     0.66\n",
      "skype                0.649\n",
      "Facebook             0.638\n",
      "yahoo                0.628\n"
     ]
    }
   ],
   "source": [
    "get_related_terms('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strongly_believe     0.655\n",
      "dear                 0.604\n",
      "believe              0.598\n",
      "come_attention       0.598\n",
      "great_invention      0.585\n"
     ]
    }
   ],
   "source": [
    "get_related_terms('society')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Word2Vec From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('dear local newspaper think effects computers people great learning skillsaffects give us time chat friendsnew people helps us learn globeastronomy keeps us troble thing dont think would feel teenager always phone friends ever time chat friends buisness partner things well theres new way chat computer plenty sites internet organization organization caps facebook myspace ect think setting meeting boss computer teenager fun phone rushing get cause want use learn countrysstates outside well computerinternet new way learn going time might think child spends lot time computer ask question economy sea floor spreading even dates youll surprise much heshe knows believe computer much interesting class day reading books child home computer local library better friends fresh perpressured something know isnt right might know child caps forbidde hospital bed driveby rather child computer learning chatting playing games safe sound home community place hope reached point understand agree computers great effects child gives us time chat friendsnew people helps us learn globe believe keeps us troble thank listening',\n",
       "      dtype='<U1114')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(test_essay)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# remove_terms = punctuation + '0123456789'\n",
    "\n",
    "# norm_corpus = [[word.lower() for word in sent if word not in remove_terms] for sent in test_essay]\n",
    "# norm_corpus = [' '.join(tok_sent) for tok_sent in norm_corpus]\n",
    "# norm_corpus = filter(None, normalize_corpus(norm_corpus))\n",
    "# norm_corpus = [tok_sent for tok_sent in norm_corpus if len(tok_sent.split()) > 2]\n",
    "\n",
    "# print('Total lines:', len(test_essay))\n",
    "\n",
    "# norm_corpus\n",
    "# print('\\nSample line:', test_essay[1])\n",
    "# print('\\nProcessed line:', norm_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "type(norm_corpus)\n",
    "# tokenizer.fit_on_texts(norm_corpus)\n",
    "# word2id = tokenizer.word_index\n",
    "\n",
    "# # build vocabulary of unique words\n",
    "# word2id['PAD'] = 0\n",
    "# id2word = {v:k for k, v in word2id.items()}\n",
    "# wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_corpus]\n",
    "\n",
    "# vocab_size = len(word2id)\n",
    "# embed_size = 100\n",
    "# window_size = 2 # context window size\n",
    "\n",
    "# print('Vocabulary Size:', vocab_size)\n",
    "# print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
