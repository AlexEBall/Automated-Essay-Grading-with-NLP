{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"../utils\")\n",
    "\n",
    "from helpers import adding_stanford_nlp_groups_NER_to_stop_words, removing_stanford_nlp_groups_NER_from_stop_words, punct_space_stop, line_review, lemmatized_sentence_corpus, trigram_bow_generator, explore_topic\n",
    "\n",
    "os.chdir(\"../notebooks\")\n",
    "\n",
    "essays = pd.read_csv('../data/intermediate/prepped_essays_df.csv')\n",
    "\n",
    "# try svm, k-nn, random forrest\n",
    "# remove @Person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essays = essays[essays['essay_set'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12978"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essays.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essays.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('../data/intermediate')\n",
    "\n",
    "essay_set1_txt_filepath = os.path.join(intermediate_directory, 'essay_set1_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 1,783 essays in the txt file.\n",
      "CPU times: user 51 ms, sys: 3.53 ms, total: 54.5 ms\n",
      "Wall time: 591 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    essay_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(essay_set1_txt_filepath, 'w', encoding='utf_8') as essay_set1_txt_file:\n",
    "\n",
    "        # loop through all essays in the dataframe\n",
    "        for row in essays.itertuples():\n",
    "\n",
    "            # write the essay as a line in the new file and escape newline characters in the original essays\n",
    "            essay_set1_txt_file.write(row.essay.replace('\\n', '\\\\n') + '\\n')\n",
    "            essay_count += 1\n",
    "\n",
    "    print('Text from {:,} essays written to the new txt file.'.format(essay_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(essay_set1_txt_filepath, encoding='utf_8') as essay_set1_txt_file:\n",
    "        for essay_count, line in enumerate(essay_set1_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print('Text from {:,} essays in the txt file.'.format(essay_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_essay = essays.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_essay       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# parsed_essay = nlp(test_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num, sentence in enumerate(parsed_essay.sents):\n",
    "#     print('Sentence {}:'.format(num + 1))\n",
    "#     print(sentence)\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for num, entity in enumerate(parsed_essay.ents):\n",
    "#     print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_text = [token.orth_ for token in parsed_essay]\n",
    "# token_pos = [token.pos_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_pos),\n",
    "#              columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lemma = [token.lemma_ for token in parsed_essay]\n",
    "# token_shape = [token.shape_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "#              columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_entity_type = [token.ent_type_ for token in parsed_essay]\n",
    "# token_entity_iob = [token.ent_iob_ for token in parsed_essay]\n",
    "\n",
    "# pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n",
    "#              columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_attributes = [(token.orth_,\n",
    "#                      token.prob,\n",
    "#                      token.is_stop,\n",
    "#                      token.is_punct,\n",
    "#                      token.is_space,\n",
    "#                      token.like_num,\n",
    "#                      token.is_oov)\n",
    "#                     for token in parsed_essay]\n",
    "\n",
    "# df = pd.DataFrame(token_attributes,\n",
    "#                   columns=['text',\n",
    "#                            'log_probability',\n",
    "#                            'stop?',\n",
    "#                            'punctuation?',\n",
    "#                            'whitespace?',\n",
    "#                            'number?',\n",
    "#                            'out of vocab.?'])\n",
    "\n",
    "# df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "#                                        .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing_stanford_nlp_groups_NER_from_stop_words(nlp)\n",
    "adding_stanford_nlp_groups_NER_to_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory, 'unigram_sentences_all_essays.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_set1_all_filepath = os.path.join(intermediate_directory, 'essay_set1_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 23.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(essays_set1_all_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for unigram_sentence in it.islice(unigram_sentences, 19, 42):\n",
    "#     print(' '.join(unigram_sentence))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(unigram_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 23.3 ms, total: 126 ms\n",
      "Wall time: 608 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory, 'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bigram_sentence in it.islice(bigram_sentences, 19, 42):\n",
    "#     print(' '.join(bigram_sentence))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory, 'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.7 ms, sys: 9.32 ms, total: 105 ms\n",
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory, 'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trigram_sentence in it.islice(trigram_sentences, 205, 245):\n",
    "#     print(' '.join(trigram_sentence))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_essays_all_filepath = os.path.join(intermediate_directory, 'trigram_essays_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "#     with codecs.open(trigram_essays_all_filepath, 'w', encoding='utf_8') as f:\n",
    "#         for sentence in lemmatized_sentence_corpus(essays_set1_all_filepath, codecs, nlp):\n",
    "#             f.write(sentence + '\\n')\n",
    "    \n",
    "    \n",
    "    with codecs.open(trigram_essays_all_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_essay in nlp.pipe(line_review(essays_set1_all_filepath), batch_size=100, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_essays = [token.lemma_ for token in parsed_essay\n",
    "                              if not punct_space_stop(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_essays = bigram_model[unigram_essays]\n",
    "            trigram_essays = trigram_model[bigram_essays]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_essays = ' '.join(trigram_essays)\n",
    "            f.write(trigram_essays + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Dear @CAPS1 times, @CAPS2 you think computers benefit society? Well I think so! There are countless reasons why computers are both resourceful and helpful. Many citizens in our own community of watertown think computers are a great resource for many things while others disagree with this completely. Computers can benefit society because you can learn many new things on the internet, also you can interact with your friends and family, and lastly there are many applications used for business. On both a computer and the internet there are more than @NUM1 million things you can learn. When you are struggling with homework a computer is a great resource. You can quickly open @CAPS3.com and search any topic at any time. For example, if you did not know a conversion it is easily found on the internet. Another thing you can be taught or informed about is news. There are websites such as nytimes.com and cnn.com that give you daily news. I personally use these websites weekly. On the computer you can find vacation sports you want to learn about and go to. When my family was planning our trip to @ORGANIZATION1 in @LOCATION1 we used the computer constantly. In the end, computers can teach you many things. Secondly you can interact with people on the internet in many ways. These include social networking websites, webchat and email. Social networking websites are a great way to connect to family members and friends from the past. Websites that were created for this include: facebook, myspace, and twitter. These free websites let you add friends, send messages, and post pictures. Web chatting is a way to video people. You can personally see someone from acroos the @ORGANIZATION1 from your webcam. Lastly, emailing is a great way to inform people. You can write a visual letter to one of your friends. In conclusion, the computer is one excellent way to interact. Lastly, the computer is used for business. An application called microsoft office is used in countless bussinesses, microsoft office word, powerpoint, excel and @CAPS4. Each of these are used to plan, sell, present, manage, and write about products. Microsoft is something you should download in the near future. Business also use webchatting, as I previously state, for conference calls. A production company can be face to face with a company from china without being in the room with them. This makes it much easier than to fly half way across the @ORGANIZATION1. Finally, computers are used when manufacturing a product in or at a business. Work sites such as @CAPS5 companies use computers to control their machines. I know that the @ORGANIZATION2 factory uses computers in their factory due to the fact that I watche the show unwrapped about them on @CAPS5 @CAPS6. In the end computers are a necessary item in a business. In conslusion, computers have a great affect on people. The can teach people many new and exciting things they did not know. Also, connect humans with each from half way across the @ORGANIZATION1. Lastly, businesses would not run well without a computer. Computers can benefit society because you can learn many new things on the internet. Also you can interact with your friends and family, and there are many applications used for business. If I were an expert I would have nothing to worry about concerning computers. They are a magnificent thing. @CAPS2 you have the same belief and understanding as me?\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "dear time think computer benefit_society think countless reason computer resourceful helpful citizen community watertown think computer great_resource thing disagree completely computer benefit_society learn_new thing internet interact friend family lastly application business computer internet million thing learn struggle homework computer great_resource quickly open @CAPS3.com search_topic time example know conversion easily find internet thing teach inform news website nytimes.com cnn.com daily news personally use website weekly computer find vacation sport want learn family plan_trip computer constantly end computer teach thing secondly interact people internet way include social_networking website webchat email social_networking website great_way connect family_member friend past website create include facebook_myspace_twitter free website let add friend send_message post_picture web chatting way video people personally acroo webcam lastly emailing great_way inform people write visual letter friend conclusion computer excellent way interact lastly computer business application call microsoft office countless bussinesse microsoft office word powerpoint excel plan sell present manage write product Microsoft download near future business use webchatting previously state conference call production company face_face company china room make easy fly half way finally computer manufacture product business work site company use computer control machine know factory use computer factory fact watche unwrap end computer necessary item business conslusion computer great affect people teach people new exciting thing know connect human half way lastly business run computer computer benefit_society learn_new thing internet interact friend family application business expert worry concern computer magnificent thing belief understanding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original:' + '\\n')\n",
    "\n",
    "for essay in it.islice(line_review(essays_set1_all_filepath), 301, 302):\n",
    "    print(essay)\n",
    "\n",
    "print('----' + '\\n')\n",
    "print('Transformed:' + '\\n')\n",
    "\n",
    "with codecs.open(trigram_essays_all_filepath, encoding='utf_8') as f:\n",
    "    for essay in it.islice(f, 301, 302):\n",
    "        print(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topic modeling* is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) or LDA, a popular approach to topic modeling.\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a *vector* of token counts. There are two layers in this model &mdash; documents and tokens &mdash; and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "* Document vectors tend to be large (one dimension for each token $\\Rightarrow$ lots of dimensions)\n",
    "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "* The dimensions are fully indepedent from each other &mdash; there's no sense of connection between related tokens, such as _knife_ and _fork_.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of *topics*, and the *topics* are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow [*Dirichlet*](https://en.wikipedia.org/wiki/Dirichlet_distribution) probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html) class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory, 'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 ms, sys: 1.43 ms, total: 3.17 ms\n",
      "Wall time: 2.44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_essays = LineSentence(trigram_essays_all_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_essays)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many NLP techniques, LDA uses a simplifying assumption known as the [*bag-of-words* model](https://en.wikipedia.org/wiki/Bag-of-words_model). In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded. \n",
    "\n",
    "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The `trigram_bow_generator` function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n",
    "\n",
    "In the following code, \"bag-of-words\" is abbreviated as `bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory, 'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trigram_bow_generator(filepath):\n",
    "#     \"\"\"\n",
    "#     generator function to read reviews from a file\n",
    "#     and yield a bag-of-words representation\n",
    "#     \"\"\"\n",
    "    \n",
    "#     for essay in LineSentence(filepath):\n",
    "#         yield trigram_dictionary.doc2bow(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 ms, sys: 2.4 ms, total: 4.53 ms\n",
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_essays_all_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bag-of-words corpus, we're finally ready to learn our topic model from the essays. We simply need to pass the bag-of-words matrix and Dictionary from our previous steps to `LdaMulticore` as inputs, along with the number of topics the model should learn. For this demo, we're asking for 5 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I need to expand the number of topics along with the most import/unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.2 ms, sys: 2.93 ms, total: 7.13 ms\n",
      "Wall time: 6.18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=5,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explore_topic(topic_number, topn=5):\n",
    "#     \"\"\"\n",
    "#     accept a user-supplied topic number and\n",
    "#     print out a formatted list of the top terms\n",
    "#     \"\"\"\n",
    "        \n",
    "#     print('{:20} {}'.format('term', 'frequency') + '\\n')\n",
    "\n",
    "#     for term, frequency in lda.show_topic(topic_number, topn=5):\n",
    "#         print('{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "spend_time           0.012\n",
      "internet             0.010\n",
      "game                 0.010\n",
      "get                  0.010\n",
      "believe              0.009\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: 'looking_at_websites_for_school',\n",
    "               1: 'spend_time_online_playing_games',\n",
    "               2: 'help_kids_learn_about_world',\n",
    "               3: 'look_for_information',\n",
    "               4: 'access_information_at_school'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'wb') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 s, sys: 297 ms, total: 3.44 s\n",
      "Wall time: 56.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus, trigram_dictionary)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el145263126017848220948018\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el145263126017848220948018_data = {\"mdsDat\": {\"x\": [0.011901613691480446, -0.004331936542000665, 0.005469401281212863, 0.005076339643040666, -0.018115418073733326], \"y\": [-0.011155523546197033, 0.004269022511376844, -0.000207588730341577, 0.0121124779689177, -0.00501838820375591], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.696487426757812, 26.34523582458496, 19.199373245239258, 14.592832565307617, 13.166074752807617]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [1218.0, 320.0, 559.0, 645.0, 933.0, 986.0, 753.0, 993.0, 917.0, 730.0, 932.0, 1316.0, 697.0, 902.0, 833.0, 384.0, 696.0, 327.0, 528.0, 770.0, 1058.0, 446.0, 527.0, 547.0, 1340.0, 487.0, 603.0, 1221.0, 299.0, 346.0, 10.281171798706055, 12.247733116149902, 8.437698364257812, 9.13564682006836, 10.492158889770508, 9.883209228515625, 23.02244758605957, 10.44859790802002, 13.473402976989746, 6.035342693328857, 7.596879959106445, 11.838764190673828, 7.914144992828369, 14.292076110839844, 8.193034172058105, 12.902023315429688, 85.23646545410156, 9.247488021850586, 7.091142654418945, 8.178796768188477, 34.42527770996094, 7.927716255187988, 8.564737319946289, 11.01640796661377, 11.89010238647461, 35.290443420410156, 14.059276580810547, 11.501025199890137, 5.84574031829834, 9.223506927490234, 14.569735527038574, 259.2975769042969, 144.3606414794922, 330.7141418457031, 273.8869934082031, 273.639404296875, 166.78805541992188, 124.58937072753906, 84.24404907226562, 101.34590148925781, 215.62939453125, 133.9891815185547, 43.3117561340332, 163.33628845214844, 261.3247985839844, 213.74276733398438, 310.6898193359375, 308.2925109863281, 164.20664978027344, 79.25151824951172, 222.8577117919922, 371.6728820800781, 92.32527923583984, 85.80891418457031, 356.4033508300781, 188.3583221435547, 295.70050048828125, 228.42156982421875, 361.4330139160156, 242.7418975830078, 204.60118103027344, 225.9228057861328, 246.17440795898438, 167.84580993652344, 261.78472900390625, 240.45741271972656, 238.1648406982422, 178.78863525390625, 195.15826416015625, 249.65219116210938, 187.51512145996094, 181.9787139892578, 187.16940307617188, 174.3909912109375, 6.775497913360596, 8.33680248260498, 27.687746047973633, 7.342703819274902, 11.753715515136719, 9.012953758239746, 5.344782829284668, 7.944887638092041, 5.609869480133057, 27.21275520324707, 6.1603312492370605, 11.357322692871094, 12.340696334838867, 8.674090385437012, 8.16701889038086, 4.842278003692627, 566.5523681640625, 20.847387313842773, 13.593241691589355, 13.382651329040527, 6.118373870849609, 20.756853103637695, 7.424358367919922, 5.1782307624816895, 18.93490982055664, 14.105687141418457, 595.4622802734375, 6.4154791831970215, 33.535438537597656, 5.062924861907959, 39.63584899902344, 65.11549377441406, 78.35002899169922, 294.98187255859375, 24.078264236450195, 366.1947021484375, 487.6759948730469, 231.9091033935547, 152.40589904785156, 230.0493927001953, 130.1491241455078, 90.06639099121094, 148.0787811279297, 57.25938415527344, 256.3966064453125, 102.12268829345703, 321.4009704589844, 379.2691650390625, 60.65717315673828, 65.15888214111328, 242.31732177734375, 208.64910888671875, 282.1204528808594, 215.89120483398438, 275.787109375, 230.354248046875, 286.6692810058594, 337.2663879394531, 271.1272277832031, 231.5133514404297, 288.9440002441406, 214.46786499023438, 144.34268188476562, 213.2711181640625, 210.79188537597656, 222.8057861328125, 166.52688598632812, 210.71897888183594, 208.48388671875, 167.35057067871094, 159.76121520996094, 163.03787231445312, 45.10888671875, 15.271210670471191, 8.95603084564209, 4.856781005859375, 6.386455059051514, 5.391680717468262, 6.497309684753418, 10.167377471923828, 3.9464032649993896, 7.953343391418457, 17.587444305419922, 6.8082685470581055, 15.874281883239746, 58.9463996887207, 6.643515586853027, 8.913363456726074, 8.301091194152832, 6.807155132293701, 10.95047378540039, 16.413179397583008, 28.547313690185547, 16.49256134033203, 7.942002773284912, 4.036643028259277, 9.46300983428955, 40.76951217651367, 5.4715704917907715, 4.493422031402588, 5.355051040649414, 167.59385681152344, 71.80619812011719, 73.72651672363281, 14.255888938903809, 53.127254486083984, 272.5137634277344, 111.27881622314453, 102.71411895751953, 184.33645629882812, 185.84320068359375, 42.61985397338867, 70.61305236816406, 43.45192337036133, 192.8899688720703, 66.88984680175781, 242.25363159179688, 154.15159606933594, 236.534423828125, 65.95870971679688, 222.26438903808594, 84.56603240966797, 89.3204116821289, 143.01820373535156, 105.67549133300781, 66.10284423828125, 66.72813415527344, 173.81753540039062, 97.95146942138672, 102.50302124023438, 112.66938018798828, 132.47640991210938, 206.49038696289062, 162.94406127929688, 180.33438110351562, 143.09255981445312, 133.87338256835938, 203.1024932861328, 168.4159393310547, 155.54348754882812, 145.5626678466797, 169.61328125, 195.8626708984375, 137.86094665527344, 175.13510131835938, 179.040283203125, 149.44407653808594, 127.60581970214844, 158.59197998046875, 140.97219848632812, 129.79556274414062, 5.857665538787842, 12.623132705688477, 5.844409465789795, 6.170626163482666, 4.726626873016357, 3.6468300819396973, 5.532495975494385, 6.760992050170898, 4.203298091888428, 29.294265747070312, 7.583799839019775, 7.105742454528809, 4.350751876831055, 4.008737087249756, 9.675601959228516, 8.864069938659668, 4.519687652587891, 4.1083903312683105, 43.003631591796875, 3.1987030506134033, 11.205262184143066, 4.849322319030762, 3.1460769176483154, 8.16138744354248, 4.245864391326904, 6.864539623260498, 7.538182258605957, 13.975581169128418, 6.629908561706543, 4.405074596405029, 30.410568237304688, 16.864395141601562, 25.52960968017578, 15.440176963806152, 71.61919403076172, 122.42782592773438, 97.74711608886719, 21.882957458496094, 77.58273315429688, 197.8018341064453, 162.03099060058594, 149.17010498046875, 34.63286590576172, 85.0868911743164, 182.2425994873047, 138.62440490722656, 66.04708862304688, 192.78152465820312, 143.94056701660156, 46.0988883972168, 210.578857421875, 85.09790802001953, 83.14362335205078, 92.84213256835938, 88.42938995361328, 55.8631706237793, 71.95269775390625, 130.8373260498047, 100.31847381591797, 74.58026123046875, 150.3121795654297, 185.76817321777344, 73.36035919189453, 139.54104614257812, 164.31747436523438, 124.44148254394531, 122.95651245117188, 106.1274642944336, 113.53353881835938, 120.23482513427734, 92.21216583251953, 103.86553192138672, 117.21546936035156, 123.45333862304688, 100.09864807128906, 105.63395690917969, 97.63616943359375, 108.66307830810547, 100.06241607666016, 5.152469158172607, 121.1253890991211, 4.196621417999268, 24.517141342163086, 8.587984085083008, 4.338050842285156, 3.37477445602417, 13.957548141479492, 4.8986687660217285, 12.920154571533203, 3.9184086322784424, 3.6793010234832764, 4.307373046875, 7.848655700683594, 6.696857452392578, 3.417971134185791, 9.55978012084961, 4.86325216293335, 7.691275119781494, 9.892068862915039, 25.75352668762207, 6.171240329742432, 5.470656394958496, 8.100640296936035, 22.337223052978516, 3.6672792434692383, 4.409063339233398, 3.875225782394409, 7.104034900665283, 3.9516100883483887, 150.89601135253906, 41.706356048583984, 164.473388671875, 11.776305198669434, 21.51778793334961, 12.472107887268066, 13.863842010498047, 29.372682571411133, 65.46989440917969, 152.63426208496094, 20.189056396484375, 55.17958450317383, 30.761306762695312, 230.76943969726562, 178.67721557617188, 23.535493850708008, 80.2322006225586, 31.199411392211914, 104.42931365966797, 177.3042449951172, 52.686222076416016, 70.94058990478516, 72.18024444580078, 81.0985336303711, 44.595523834228516, 186.48724365234375, 118.33516693115234, 71.69680786132812, 134.74444580078125, 123.22493743896484, 64.6368637084961, 94.37090301513672, 73.96640014648438, 125.3265151977539, 89.97904968261719, 68.8960952758789, 81.7076416015625, 101.96198272705078, 116.3332290649414, 118.81169128417969, 132.5035400390625, 127.31255340576172, 93.23004150390625, 107.0565185546875, 108.15331268310547, 100.81951141357422, 102.0477066040039, 92.86226654052734, 106.3597640991211, 83.88484191894531], \"Term\": [\"spend_time\", \"nature\", \"chat\", \"believe\", \"website\", \"exercise\", \"great\", \"get\", \"game\", \"child\", \"not\", \"kid\", \"easy\", \"place\", \"type\", \"cause\", \"person\", \"email\", \"homework\", \"say\", \"school\", \"spend\", \"have\", \"come\", \"internet\", \"positive_effect\", \"book\", \"look\", \"big\", \"fact\", \"white\", \"portable\", \"disaster\", \"evolve\", \"thay\", \"buissness\", \"buisnesse\", \"enhance\", \"road\", \"professional\", \"researching\", \"click_away\", \"star\", \"government\", \"incredible\", \"criminal\", \"laptop\", \"method\", \"power_point\", \"channel\", \"school_project\", \"teach_hand_eye\", \"natural_disaster\", \"aid\", \"lesson\", \"rely\", \"stock\", \"career\", \"pose\", \"video_chatting\", \"crime\", \"job\", \"benefit\", \"type\", \"society\", \"fun\", \"teach\", \"hand\", \"sport\", \"skill\", \"research\", \"teacher\", \"trip\", \"hand_eye_coordination\", \"student\", \"book\", \"game\", \"lot\", \"make\", \"adult\", \"effect\", \"look\", \"education\", \"meet\", \"information\", \"communicate\", \"work\", \"important\", \"internet\", \"example\", \"easy\", \"allow\", \"place\", \"dear\", \"school\", \"bad\", \"not\", \"technology\", \"play\", \"kid\", \"People\", \"outside\", \"get\", \"say\", \"watch_news\", \"beautiful_nature\", \"reson\", \"frend\", \"get_fat\", \"will_able\", \"produce\", \"fresh\", \"com\", \"past\", \"kick\", \"advice\", \"big_problem\", \"exercis\", \"board_game\", \"conclution\", \"spend_time\", \"suppose\", \"clean\", \"jog\", \"delete\", \"download\", \"post_picture\", \"explore_nature\", \"beneficial_society\", \"ect\", \"kid\", \"plug\", \"story\", \"storm\", \"take_away\", \"school_work\", \"watch\", \"outside\", \"thi\", \"exercise\", \"internet\", \"instead\", \"start\", \"technology\", \"play_game\", \"stuff\", \"parent\", \"dear_local_newspaper\", \"People\", \"try\", \"work\", \"information\", \"business\", \"e_mail\", \"say\", \"let\", \"bad\", \"person\", \"website\", \"play\", \"get\", \"look\", \"lot\", \"allow\", \"school\", \"child\", \"spend\", \"great\", \"important\", \"example\", \"able\", \"game\", \"not\", \"society\", \"job\", \"place\", \"bad_effect\", \"u\", \"alow\", \"tradition\", \"quiz\", \"quality\", \"foreign_place\", \"cheap\", \"relative_live\", \"degree\", \"interact_family\", \"live_faraway\", \"damage\", \"travel\", \"dear_reader\", \"health_issue\", \"schoolwork\", \"teach_skill\", \"usefull\", \"lack_exercise\", \"addicting\", \"shopping\", \"shop_online\", \"throught\", \"majority\", \"meet_new\", \"depressed\", \"eye_hand\", \"new_one\", \"positive_effect\", \"eye\", \"program\", \"physically\", \"negative_effect\", \"place\", \"communication\", \"interact\", \"effect\", \"easy\", \"future\", \"change\", \"better\", \"important\", \"money\", \"exercise\", \"live\", \"get\", \"study\", \"website\", \"site\", \"email\", \"new\", \"well\", \"useful\", \"news\", \"People\", \"cause\", \"opinion\", \"tell\", \"able\", \"school\", \"allow\", \"bad\", \"person\", \"believe\", \"information\", \"not\", \"example\", \"play\", \"work\", \"internet\", \"great\", \"look\", \"kid\", \"game\", \"let\", \"spend_time\", \"lot\", \"student\", \"excersice\", \"abuse\", \"neatly\", \"massive\", \"material\", \"instal\", \"current_event\", \"alternative\", \"Technology\", \"negative\", \"dollar\", \"well_grade\", \"claim\", \"radio\", \"twitter\", \"electricity\", \"challenge\", \"page_essay\", \"letter\", \"twitter_facebook\", \"daily_life\", \"great_resource\", \"spending_time_family\", \"final\", \"chating\", \"shoot\", \"acess\", \"war\", \"point_view\", \"plan_vacation\", \"benifit\", \"famous\", \"page\", \"inform\", \"phone\", \"have\", \"helpful\", \"computor\", \"email\", \"website\", \"great\", \"person\", \"pay\", \"write\", \"not\", \"easy\", \"big\", \"school\", \"say\", \"affect\", \"look\", \"agree\", \"stay\", \"make\", \"hand_eye_coordination\", \"save\", \"fact\", \"student\", \"come\", \"cause\", \"exercise\", \"kid\", \"country\", \"bad\", \"information\", \"type\", \"example\", \"society\", \"allow\", \"place\", \"communicate\", \"outside\", \"work\", \"spend_time\", \"child\", \"get\", \"play\", \"internet\", \"lot\", \"obease\", \"nature\", \"personal_experience\", \"virus\", \"honestly\", \"lake\", \"watch_tv\", \"love_one\", \"burn_calorie\", \"apart\", \"fail_class\", \"customer\", \"seriously\", \"entertaining\", \"exercise_enjoy_nature_interact\", \"o\", \"u\", \"intouch\", \"depression\", \"design\", \"kill\", \"Facebook\", \"waist\", \"guess\", \"park\", \"unfortunately\", \"till\", \"headache\", \"television\", \"status\", \"chat\", \"close\", \"believe\", \"child_adult\", \"addiction\", \"outdoors\", \"neat\", \"waste\", \"love\", \"child\", \"addicting\", \"little\", \"brain\", \"spend_time\", \"game\", \"die\", \"hour\", \"question\", \"homework\", \"get\", \"ask\", \"take\", \"cause\", \"spend\", \"connect\", \"internet\", \"great\", \"parent\", \"not\", \"type\", \"benefit_society\", \"book\", \"away\", \"lot\", \"instead\", \"lastly\", \"come\", \"say\", \"exercise\", \"work\", \"information\", \"look\", \"fun\", \"website\", \"school\", \"place\", \"bad\", \"play\", \"kid\", \"outside\"], \"Total\": [1218.0, 320.0, 559.0, 645.0, 933.0, 986.0, 753.0, 993.0, 917.0, 730.0, 932.0, 1316.0, 697.0, 902.0, 833.0, 384.0, 696.0, 327.0, 528.0, 770.0, 1058.0, 446.0, 527.0, 547.0, 1340.0, 487.0, 603.0, 1221.0, 299.0, 346.0, 15.802966117858887, 20.029010772705078, 14.369268417358398, 16.245986938476562, 18.77360725402832, 18.12311553955078, 42.377803802490234, 19.430686950683594, 25.927982330322266, 11.794910430908203, 14.891450881958008, 23.585472106933594, 15.979326248168945, 28.868408203125, 16.863618850708008, 26.622268676757812, 175.9638214111328, 19.233762741088867, 14.778719902038574, 17.082727432250977, 72.0165023803711, 16.76383399963379, 18.11237907409668, 23.327425003051758, 25.31487464904785, 75.21007537841797, 30.388519287109375, 24.995113372802734, 12.716446876525879, 20.070838928222656, 31.835975646972656, 602.0405883789062, 345.61895751953125, 833.8296508789062, 694.5408935546875, 697.3896484375, 414.0248718261719, 303.5950012207031, 203.0559539794922, 249.65771484375, 566.4097290039062, 344.5412902832031, 102.07978820800781, 443.76971435546875, 744.5210571289062, 603.35400390625, 917.3494873046875, 945.7809448242188, 466.7457580566406, 204.0460662841797, 671.842529296875, 1221.9656982421875, 243.81515502929688, 223.7723846435547, 1235.595947265625, 588.5498657226562, 1022.7418823242188, 746.8512573242188, 1340.1220703125, 821.375, 697.5638427734375, 796.442626953125, 902.7803955078125, 542.562744140625, 1058.1539306640625, 944.5009765625, 932.0517578125, 634.4242553710938, 761.5736083984375, 1316.28271484375, 774.1871337890625, 757.972412109375, 993.3113403320312, 770.4879150390625, 11.385523796081543, 14.862674713134766, 50.58643341064453, 13.96762752532959, 22.497222900390625, 17.680269241333008, 10.662446022033691, 16.03902816772461, 11.336668968200684, 55.78706741333008, 12.63302993774414, 23.693649291992188, 25.81416130065918, 18.19358253479004, 17.384140014648438, 10.335009574890137, 1218.313232421875, 44.87027359008789, 29.400712966918945, 29.033159255981445, 13.29417896270752, 45.332115173339844, 16.218181610107422, 11.368520736694336, 41.65726089477539, 31.073211669921875, 1316.28271484375, 14.34231185913086, 75.08989715576172, 11.39051342010498, 90.36089324951172, 153.24864196777344, 189.05467224121094, 757.972412109375, 55.72563934326172, 986.3820190429688, 1340.1220703125, 610.54150390625, 404.1793518066406, 634.4242553710938, 341.9061584472656, 229.4853057861328, 403.53448486328125, 143.73582458496094, 774.1871337890625, 277.4718322753906, 1022.7418823242188, 1235.595947265625, 155.73233032226562, 168.95921325683594, 770.4879150390625, 656.0466918945312, 944.5009765625, 696.8919067382812, 933.2783203125, 761.5736083984375, 993.3113403320312, 1221.9656982421875, 945.7809448242188, 796.442626953125, 1058.1539306640625, 730.86962890625, 446.1785888671875, 753.1882934570312, 746.8512573242188, 821.375, 580.2567138671875, 917.3494873046875, 932.0517578125, 694.5408935546875, 602.0405883789062, 902.7803955078125, 90.6163101196289, 33.003623962402344, 20.67823600769043, 11.245172500610352, 15.061702728271484, 13.060391426086426, 15.991350173950195, 25.2838134765625, 10.222525596618652, 20.712562561035156, 46.14189147949219, 18.04064178466797, 42.43820571899414, 159.4511260986328, 17.99542808532715, 24.157602310180664, 22.633176803588867, 18.631895065307617, 30.295480728149414, 45.41695785522461, 79.46605682373047, 47.046722412109375, 22.690589904785156, 11.599164962768555, 27.198810577392578, 117.20155334472656, 15.789863586425781, 12.987947463989258, 15.502367973327637, 487.3552551269531, 215.9361114501953, 221.87001037597656, 41.71108627319336, 163.12686157226562, 902.7803955078125, 366.9407958984375, 344.42950439453125, 671.842529296875, 697.5638427734375, 138.73020935058594, 242.56387329101562, 141.72914123535156, 746.8512573242188, 231.15945434570312, 986.3820190429688, 601.885498046875, 993.3113403320312, 228.9417266845703, 933.2783203125, 305.67816162109375, 327.980224609375, 579.6803588867188, 413.3408508300781, 235.27162170410156, 238.8212127685547, 774.1871337890625, 384.41729736328125, 410.0275573730469, 467.50579833984375, 580.2567138671875, 1058.1539306640625, 796.442626953125, 944.5009765625, 696.8919067382812, 645.3272705078125, 1235.595947265625, 932.0517578125, 821.375, 761.5736083984375, 1022.7418823242188, 1340.1220703125, 753.1882934570312, 1221.9656982421875, 1316.28271484375, 917.3494873046875, 656.0466918945312, 1218.313232421875, 945.7809448242188, 744.5210571289062, 14.294634819030762, 35.150367736816406, 16.689117431640625, 17.78443717956543, 13.731072425842285, 10.730868339538574, 16.47907829284668, 20.233409881591797, 12.737086296081543, 88.85077667236328, 23.53115463256836, 22.08561134338379, 13.614394187927246, 12.590999603271484, 31.030418395996094, 28.629253387451172, 14.63642692565918, 13.528194427490234, 141.99801635742188, 10.611414909362793, 37.278751373291016, 16.2078857421875, 10.529325485229492, 27.354598999023438, 14.26976203918457, 23.29429817199707, 25.58145523071289, 47.63780212402344, 22.80409049987793, 15.195287704467773, 105.47406005859375, 59.82414245605469, 93.00250244140625, 55.5523796081543, 285.11663818359375, 527.7866821289062, 417.5722351074219, 82.10153198242188, 327.980224609375, 933.2783203125, 753.1882934570312, 696.8919067382812, 138.55963134765625, 387.13580322265625, 932.0517578125, 697.5638427734375, 299.1407165527344, 1058.1539306640625, 770.4879150390625, 199.7733612060547, 1221.9656982421875, 417.42620849609375, 405.9432373046875, 466.7457580566406, 443.76971435546875, 254.01846313476562, 346.46844482421875, 744.5210571289062, 547.1317138671875, 384.41729736328125, 986.3820190429688, 1316.28271484375, 379.3884582519531, 944.5009765625, 1235.595947265625, 833.8296508789062, 821.375, 694.5408935546875, 796.442626953125, 902.7803955078125, 588.5498657226562, 757.972412109375, 1022.7418823242188, 1218.313232421875, 730.86962890625, 993.3113403320312, 761.5736083984375, 1340.1220703125, 945.7809448242188, 13.513404846191406, 320.1298828125, 11.615934371948242, 68.23272705078125, 24.37786102294922, 12.508416175842285, 10.579483032226562, 43.83055877685547, 15.424797058105469, 42.265933990478516, 12.91992473602295, 12.534128189086914, 14.70318603515625, 26.938234329223633, 23.05643653869629, 11.78278923034668, 33.003623962402344, 16.841598510742188, 26.659786224365234, 34.35456848144531, 90.13124084472656, 21.666906356811523, 19.284074783325195, 28.579681396484375, 78.87057495117188, 12.95324993133545, 15.58576488494873, 13.884644508361816, 26.09908676147461, 14.542896270751953, 559.52734375, 154.29061889648438, 645.3272705078125, 43.472625732421875, 81.88497924804688, 46.25464630126953, 51.90782165527344, 116.49568939208984, 281.2820129394531, 730.86962890625, 79.46605682373047, 240.93984985351562, 126.80546569824219, 1218.313232421875, 917.3494873046875, 94.69190979003906, 381.6341857910156, 131.8147735595703, 528.8638916015625, 993.3113403320312, 258.5085754394531, 374.1129150390625, 384.41729736328125, 446.1785888671875, 216.70152282714844, 1340.1220703125, 753.1882934570312, 403.53448486328125, 932.0517578125, 833.8296508789062, 359.37615966796875, 603.35400390625, 435.3005065917969, 945.7809448242188, 610.54150390625, 413.49835205078125, 547.1317138671875, 770.4879150390625, 986.3820190429688, 1022.7418823242188, 1235.595947265625, 1221.9656982421875, 697.3896484375, 933.2783203125, 1058.1539306640625, 902.7803955078125, 944.5009765625, 761.5736083984375, 1316.28271484375, 757.972412109375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8907999992370605, 0.8288000226020813, 0.7882999777793884, 0.7450000047683716, 0.7387999892234802, 0.7142999768257141, 0.7105000019073486, 0.7002999782562256, 0.6660000085830688, 0.650600016117096, 0.647599995136261, 0.6313999891281128, 0.6179999709129333, 0.6176000237464905, 0.598800003528595, 0.5963000059127808, 0.59579998254776, 0.5882999897003174, 0.5863000154495239, 0.5841000080108643, 0.5824999809265137, 0.5717999935150146, 0.5716999769210815, 0.5703999996185303, 0.5649999976158142, 0.5640000104904175, 0.5498999953269958, 0.5443999767303467, 0.5435000061988831, 0.5430999994277954, 0.5389999747276306, 0.478300005197525, 0.44760000705718994, 0.39590001106262207, 0.39010000228881836, 0.38510000705718994, 0.4113999903202057, 0.4300000071525574, 0.4408999979496002, 0.41909998655319214, 0.3549000024795532, 0.37619999051094055, 0.4632999897003174, 0.32109999656677246, 0.2736999988555908, 0.28290000557899475, 0.2379000037908554, 0.1996999979019165, 0.2759999930858612, 0.3749000132083893, 0.21709999442100525, 0.13040000200271606, 0.34950000047683716, 0.3621000051498413, 0.07739999890327454, 0.18129999935626984, 0.07970000058412552, 0.13600000739097595, 0.010200000368058681, 0.10170000046491623, 0.0940999984741211, 0.06069999933242798, 0.021199999377131462, 0.14740000665187836, -0.07609999924898148, -0.04749999940395355, -0.043800000101327896, 0.054099999368190765, -0.04089999943971634, -0.3418999910354614, -0.09730000048875809, -0.10610000044107437, -0.3483999967575073, -0.16509999334812164, 0.8148999810218811, 0.7556999921798706, 0.7311999797821045, 0.6908000111579895, 0.6847000122070312, 0.660099983215332, 0.6432999968528748, 0.6313999891281128, 0.6304000020027161, 0.6159999966621399, 0.6157000064849854, 0.5985000133514404, 0.5958999991416931, 0.5932000279426575, 0.5784000158309937, 0.5756999850273132, 0.5681999921798706, 0.567300021648407, 0.5623999834060669, 0.5594000220298767, 0.5579000115394592, 0.5526999831199646, 0.5525000095367432, 0.5475000143051147, 0.5454000234603882, 0.5440999865531921, 0.5407000184059143, 0.5293999910354614, 0.5278000235557556, 0.5230000019073486, 0.5098000168800354, 0.4779999852180481, 0.453000009059906, 0.39010000228881836, 0.49480000138282776, 0.34299999475479126, 0.3230000138282776, 0.3659000098705292, 0.358599990606308, 0.31949999928474426, 0.36800000071525574, 0.3986000120639801, 0.3314000070095062, 0.41350001096725464, 0.2287999987602234, 0.3343000113964081, 0.17630000412464142, 0.15279999375343323, 0.39100000262260437, 0.38109999895095825, 0.17710000276565552, 0.1882999986410141, 0.12559999525547028, 0.16200000047683716, 0.11479999870061874, 0.13809999823570251, 0.09120000153779984, 0.04650000110268593, 0.08449999988079071, 0.09839999675750732, 0.03579999879002571, 0.10779999941587448, 0.2054000049829483, 0.07209999859333038, 0.06889999657869339, 0.029200000688433647, 0.08560000360012054, -0.1370999962091446, -0.16359999775886536, -0.0892999991774559, 0.007199999876320362, -0.3776000142097473, 0.9527000188827515, 0.8795999884605408, 0.8134999871253967, 0.810699999332428, 0.7922999858856201, 0.7656000256538391, 0.7495999932289124, 0.739300012588501, 0.6984999775886536, 0.6930999755859375, 0.6858000159263611, 0.6758000254631042, 0.6668999791145325, 0.6552000045776367, 0.6538000106811523, 0.6531999707221985, 0.6473000049591064, 0.6434000134468079, 0.6327000260353088, 0.6324999928474426, 0.6265000104904175, 0.6021000146865845, 0.6004999876022339, 0.5947999954223633, 0.5945000052452087, 0.5942999720573425, 0.590499997138977, 0.5889000296592712, 0.5873000025749207, 0.5827999711036682, 0.5493000149726868, 0.5486000180244446, 0.57669997215271, 0.5285000205039978, 0.45249998569488525, 0.4571000039577484, 0.44040000438690186, 0.3569999933242798, 0.32760000228881836, 0.4700999855995178, 0.41620001196861267, 0.46799999475479126, 0.29649999737739563, 0.41019999980926514, 0.24619999527931213, 0.2881999909877777, 0.21529999375343323, 0.4059000015258789, 0.21549999713897705, 0.3652999997138977, 0.3495999872684479, 0.2508000135421753, 0.2863999903202057, 0.3808000087738037, 0.3752000033855438, 0.15649999678134918, 0.28299999237060547, 0.2639999985694885, 0.2273000031709671, 0.17319999635219574, 0.016300000250339508, 0.06350000202655792, -0.00559999980032444, 0.06719999760389328, 0.07739999890327454, -0.15530000627040863, -0.06069999933242798, -0.013799999840557575, -0.0044999998062849045, -0.14640000462532043, -0.2727999985218048, -0.04780000075697899, -0.2924000024795532, -0.34470000863075256, -0.16429999470710754, 0.013000000268220901, -0.3885999917984009, -0.2531999945640564, -0.09650000184774399, 1.0325000286102295, 0.9004999995231628, 0.8754000067710876, 0.866100013256073, 0.8582000136375427, 0.8453999757766724, 0.8331999778747559, 0.828499972820282, 0.8159999847412109, 0.8151000142097473, 0.7922999858856201, 0.7906000018119812, 0.7839000225067139, 0.7800999879837036, 0.7592999935150146, 0.7522000074386597, 0.7495999932289124, 0.7329000234603882, 0.7300999760627747, 0.7254999876022339, 0.722599983215332, 0.7179999947547913, 0.7166000008583069, 0.7152000069618225, 0.7124000191688538, 0.7027999758720398, 0.7027999758720398, 0.6983000040054321, 0.689300000667572, 0.6863999962806702, 0.6809999942779541, 0.6583999991416931, 0.6319000124931335, 0.6442999839782715, 0.5430999994277954, 0.4634999930858612, 0.4726000130176544, 0.602400004863739, 0.4830000102519989, 0.373199999332428, 0.3880999982357025, 0.3831000030040741, 0.538100004196167, 0.40950000286102295, 0.29260000586509705, 0.30880001187324524, 0.4140999913215637, 0.22190000116825104, 0.24699999392032623, 0.45820000767707825, 0.1662999987602234, 0.3343000113964081, 0.33899998664855957, 0.30979999899864197, 0.31150001287460327, 0.4101000130176544, 0.3528999984264374, 0.1859000027179718, 0.22830000519752502, 0.2847999930381775, 0.043299999088048935, -0.033399999141693115, 0.2815000116825104, 0.012299999594688416, -0.09290000051259995, 0.02239999920129776, 0.025499999523162842, 0.04600000008940697, -0.023399999365210533, -0.09139999747276306, 0.07109999656677246, -0.06289999932050705, -0.24160000681877136, -0.36469998955726624, -0.06340000033378601, -0.3163999915122986, -0.12950000166893005, -0.5875999927520752, -0.3215999901294708, 1.0633000135421753, 1.0556000471115112, 1.0094000101089478, 1.003999948501587, 0.9842000007629395, 0.9685999751091003, 0.8848999738693237, 0.8831999897956848, 0.8805000185966492, 0.8422999978065491, 0.8343999981880188, 0.801800012588501, 0.7997999787330627, 0.7943000197410583, 0.7911999821662903, 0.789900004863739, 0.7885000109672546, 0.7853999733924866, 0.784500002861023, 0.7825000286102295, 0.7748000025749207, 0.7716000080108643, 0.7675999999046326, 0.7667999863624573, 0.765999972820282, 0.7656000256538391, 0.7648000121116638, 0.7512999773025513, 0.7263000011444092, 0.7245000004768372, 0.7170000076293945, 0.7192999720573425, 0.6604999899864197, 0.7214999794960022, 0.691100001335144, 0.7168999910354614, 0.7073000073432922, 0.6496999859809875, 0.5698000192642212, 0.46129998564720154, 0.6572999954223633, 0.553600013256073, 0.6111000180244446, 0.3637000024318695, 0.39160001277923584, 0.6353999972343445, 0.46799999475479126, 0.5864999890327454, 0.40529999136924744, 0.3043000102043152, 0.43700000643730164, 0.36480000615119934, 0.35499998927116394, 0.32249999046325684, 0.4465999901294708, 0.055399999022483826, 0.17669999599456787, 0.299699991941452, 0.09350000321865082, 0.11550000309944153, 0.31189998984336853, 0.17229999601840973, 0.2551000118255615, 0.006399999838322401, 0.1128000020980835, 0.23549999296665192, 0.12600000202655792, 0.005100000184029341, -0.11010000109672546, -0.12520000338554382, -0.20520000159740448, -0.23399999737739563, 0.015300000086426735, -0.13779999315738678, -0.2531999945640564, -0.16459999978542328, -0.19769999384880066, -0.07670000195503235, -0.48820000886917114, -0.1737000048160553], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.20989990234375, -8.034899711608887, -8.407500267028809, -8.32800006866455, -8.189599990844727, -8.24940013885498, -7.403800010681152, -8.19379997253418, -7.939499855041504, -8.742600440979004, -8.512499809265137, -8.068900108337402, -8.471599578857422, -7.880499839782715, -8.43690013885498, -7.982800006866455, -6.094799995422363, -8.315899848937988, -8.581399917602539, -8.438699722290039, -7.001399993896484, -8.469900131225586, -8.392600059509277, -8.140800476074219, -8.064499855041504, -6.976600170135498, -7.896900177001953, -8.097800254821777, -8.774499893188477, -8.318499565124512, -7.861299991607666, -4.9822998046875, -5.56790018081665, -4.738999843597412, -4.927499771118164, -4.928400039672852, -5.423500061035156, -5.715199947357178, -6.106500148773193, -5.9217000007629395, -5.1666998863220215, -5.642499923706055, -6.7718000411987305, -5.444399833679199, -4.9745001792907715, -5.17549991607666, -4.801400184631348, -4.809199810028076, -5.4390997886657715, -6.167600154876709, -5.133699893951416, -4.622200012207031, -6.014900207519531, -6.088099956512451, -4.6641998291015625, -5.3018999099731445, -4.850900173187256, -5.109000205993652, -4.650199890136719, -5.0482001304626465, -5.219200134277344, -5.119999885559082, -5.034200191497803, -5.417200088500977, -4.972700119018555, -5.057700157165527, -5.067299842834473, -5.354000091552734, -5.26639986038208, -5.020199775695801, -5.306399822235107, -5.336299896240234, -5.308199882507324, -5.378900051116943, -8.613699913024902, -8.406299591064453, -7.205999851226807, -8.533300399780273, -8.062800407409668, -8.328300476074219, -8.850899696350098, -8.454500198364258, -8.802499771118164, -7.223299980163574, -8.708900451660156, -8.097100257873535, -8.014100074768066, -8.366600036621094, -8.426899909973145, -8.949600219726562, -4.187399864196777, -7.489799976348877, -7.917399883270264, -7.933000087738037, -8.715700149536133, -7.494100093841553, -8.522199630737305, -8.882499694824219, -7.585999965667725, -7.88040018081665, -4.137599945068359, -8.668299674987793, -7.014400005340576, -8.904999732971191, -6.847300052642822, -6.350800037384033, -6.165800094604492, -4.840099811553955, -7.345699787139893, -4.623799800872803, -4.337299823760986, -5.080599784851074, -5.500400066375732, -5.088699817657471, -5.658299922943115, -6.026400089263916, -5.529200077056885, -6.479400157928467, -4.980299949645996, -5.9008002281188965, -4.754300117492676, -4.588699817657471, -6.4217000007629395, -6.350200176239014, -5.0366997718811035, -5.186299800872803, -4.884699821472168, -5.152200222015381, -4.907400131225586, -5.087399959564209, -4.86870002746582, -4.706099987030029, -4.9243998527526855, -5.082300186157227, -4.860799789428711, -5.15880012512207, -5.554800033569336, -5.164400100708008, -5.17609977722168, -5.120699882507324, -5.411799907684326, -5.176499843597412, -5.187099933624268, -5.406899929046631, -5.4532999992370605, -5.433000087738037, -6.401500225067139, -7.484600067138672, -8.018199920654297, -8.630200386047363, -8.356399536132812, -8.525699615478516, -8.339200019836426, -7.89139986038208, -8.837800025939941, -8.13700008392334, -7.343400001525879, -8.292400360107422, -7.445899963378906, -6.133900165557861, -8.316900253295898, -8.02299976348877, -8.094200134277344, -8.29259967803955, -7.817200183868408, -7.412499904632568, -6.859000205993652, -7.407700061798096, -8.138400077819824, -8.815199851989746, -7.963200092315674, -6.502600193023682, -8.51099967956543, -8.708000183105469, -8.532500267028809, -5.089000225067139, -5.936600208282471, -5.910200119018555, -7.553400039672852, -6.2378997802734375, -4.60290002822876, -5.498499870300293, -5.57859992980957, -4.993800163269043, -4.9857001304626465, -6.4583001136779785, -5.953400135040283, -6.438899993896484, -4.948500156402588, -6.007500171661377, -4.720600128173828, -5.172599792480469, -4.744500160217285, -6.021500110626221, -4.806700229644775, -5.7729997634887695, -5.718299865722656, -5.247600078582764, -5.55019998550415, -6.019400119781494, -6.009900093078613, -5.052599906921387, -5.626100063323975, -5.580699920654297, -5.486100196838379, -5.32420015335083, -4.880300045013428, -5.117199897766113, -5.0157999992370605, -5.247099876403809, -5.313700199127197, -4.896900177001953, -5.084099769592285, -5.163599967956543, -5.230000019073486, -5.077099800109863, -4.933199882507324, -5.284299850463867, -5.045000076293945, -5.0229997634887695, -5.203700065612793, -5.361599922180176, -5.144199848175049, -5.26200008392334, -5.344600200653076, -8.168499946594238, -7.400700092315674, -8.170700073242188, -8.116399765014648, -8.383000373840332, -8.642399787902832, -8.225600242614746, -8.025099754333496, -8.500399589538574, -6.558800220489502, -7.910200119018555, -7.975299835205078, -8.465900421142578, -8.547800064086914, -7.666600227355957, -7.754199981689453, -8.427800178527832, -8.523200035095215, -6.174900054931641, -8.773500442504883, -7.519800186157227, -8.357399940490723, -8.79010009765625, -7.8368000984191895, -8.490300178527832, -8.009900093078613, -7.916200160980225, -7.298900127410889, -8.044599533081055, -8.453499794006348, -6.521399974822998, -7.111000061035156, -6.696400165557861, -7.1992998123168945, -5.664899826049805, -5.128699779510498, -5.353799819946289, -6.850500106811523, -5.58489990234375, -4.64900016784668, -4.848400115966797, -4.931099891662598, -6.39139986038208, -5.492599964141846, -4.730899810791016, -5.004499912261963, -5.7459001541137695, -4.674699783325195, -4.966800212860107, -6.105400085449219, -4.586400032043457, -5.492400169372559, -5.515699863433838, -5.405300140380859, -5.453999996185303, -5.913300037384033, -5.660200119018555, -5.062300205230713, -5.327899932861328, -5.6244001388549805, -4.923500061035156, -4.711699962615967, -5.6407999992370605, -4.997900009155273, -4.834400177001953, -5.112400054931641, -5.1244001388549805, -5.271599769592285, -5.204100131988525, -5.1468000411987305, -5.412099838256836, -5.293099880218506, -5.1722002029418945, -5.1203999519348145, -5.330100059509277, -5.276199817657471, -5.355000019073486, -5.248000144958496, -5.330399990081787, -8.193900108337402, -5.036499977111816, -8.399100303649902, -6.633999824523926, -7.683000087738037, -8.365900039672852, -8.616999626159668, -7.197299957275391, -8.244400024414062, -7.274600028991699, -8.467700004577637, -8.530599594116211, -8.373000144958496, -7.7729997634887695, -7.931700229644775, -8.604299545288086, -7.575799942016602, -8.25160026550293, -7.793300151824951, -7.541600227355957, -6.584799766540527, -8.013400077819824, -8.133899688720703, -7.741399765014648, -6.727099895477295, -8.533900260925293, -8.349699974060059, -8.478699684143066, -7.872700214385986, -8.459199905395508, -4.816800117492676, -6.102700233459473, -4.730599880218506, -7.367300033569336, -6.764500141143799, -7.309800148010254, -7.204100131988525, -6.4532999992370605, -5.651800155639648, -4.805300235748291, -6.828199863433838, -5.822700023651123, -6.407100200653076, -4.391900062561035, -4.647799968719482, -6.674799919128418, -5.448400020599365, -6.392899990081787, -5.184800148010254, -4.6554999351501465, -5.86899995803833, -5.571499824523926, -5.554200172424316, -5.437699794769287, -6.035699844360352, -4.605000019073486, -5.059800148010254, -5.5609002113342285, -4.929999828338623, -5.0192999839782715, -5.664599895477295, -5.286099910736084, -5.529699802398682, -5.002399921417236, -5.333799839019775, -5.6006999015808105, -5.430200099945068, -5.208700180053711, -5.076900005340576, -5.055799961090088, -4.946700096130371, -4.986700057983398, -5.298299789428711, -5.159999847412109, -5.149799823760986, -5.21999979019165, -5.207900047302246, -5.302199840545654, -5.166500091552734, -5.403900146484375]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5], \"Freq\": [0.2307666838169098, 0.09230667352676392, 0.27692002058029175, 0.04615333676338196, 0.27692002058029175, 0.24283534288406372, 0.3306694030761719, 0.22475185990333557, 0.11625096201896667, 0.08525070548057556, 0.23553267121315002, 0.3140435516834259, 0.07851088792085648, 0.3140435516834259, 0.07851088792085648, 0.289527028799057, 0.28780364990234375, 0.2274855226278305, 0.11546613276004791, 0.07927525788545609, 0.08534761518239975, 0.28449204564094543, 0.11379681527614594, 0.3698396682739258, 0.1706952303647995, 0.23454490303993225, 0.35181736946105957, 0.07818163186311722, 0.31272652745246887, 0.07818163186311722, 0.1006719172000885, 0.18875983357429504, 0.3649356961250305, 0.08808792382478714, 0.25167977809906006, 0.14654701948165894, 0.1953960359096527, 0.31751856207847595, 0.07327350974082947, 0.2686695456504822, 0.38716748356819153, 0.2793486714363098, 0.15192647278308868, 0.13232305645942688, 0.04900854080915451, 0.1266162097454071, 0.46425941586494446, 0.1266162097454071, 0.16882160305976868, 0.08441080152988434, 0.21524390578269958, 0.2502836287021637, 0.1751985400915146, 0.2302609235048294, 0.12514181435108185, 0.26112398505210876, 0.26831090450286865, 0.1796724796295166, 0.203628808259964, 0.08384715765714645, 0.4715479612350464, 0.128603994846344, 0.1714719831943512, 0.128603994846344, 0.128603994846344, 0.28376179933547974, 0.2912953197956085, 0.2046600580215454, 0.14313648641109467, 0.07910174131393433, 0.24180012941360474, 0.1450800746679306, 0.4352402091026306, 0.09672004729509354, 0.09672004729509354, 0.1482696235179901, 0.1482696235179901, 0.19769282639026642, 0.34596243500709534, 0.19769282639026642, 0.16561801731586456, 0.26025688648223877, 0.1419582962989807, 0.11829858273267746, 0.3075762987136841, 0.2127588987350464, 0.26304736733436584, 0.13539202511310577, 0.18568049371242523, 0.20502221584320068, 0.18607835471630096, 0.3124278485774994, 0.1768893003463745, 0.15391665697097778, 0.16999749839305878, 0.2541024386882782, 0.2985703647136688, 0.19057682156562805, 0.14822642505168915, 0.10799353569746017, 0.14346203207969666, 0.13242648541927338, 0.49659934639930725, 0.13242648541927338, 0.08828432857990265, 0.06728263944387436, 0.5382611155509949, 0.13456527888774872, 0.13456527888774872, 0.13456527888774872, 0.2200433909893036, 0.226241797208786, 0.20764657855033875, 0.09142647683620453, 0.2541346251964569, 0.16803793609142303, 0.45610296726226807, 0.09602167457342148, 0.14403250813484192, 0.14403250813484192, 0.41664379835128784, 0.2256820648908615, 0.13888126611709595, 0.09837423264980316, 0.12152111530303955, 0.23373837769031525, 0.2866077721118927, 0.19756458699703217, 0.10295619070529938, 0.1808689832687378, 0.25598710775375366, 0.2180631011724472, 0.11377204954624176, 0.2844301164150238, 0.12325305491685867, 0.2116713523864746, 0.19050422310829163, 0.3033956289291382, 0.16933709383010864, 0.11994710564613342, 0.213946133852005, 0.19054578244686127, 0.19388867914676666, 0.22063195705413818, 0.1805170476436615, 0.1549537032842636, 0.464861124753952, 0.116215281188488, 0.1549537032842636, 0.0774768516421318, 0.1725710928440094, 0.4601895809173584, 0.1725710928440094, 0.0575236976146698, 0.1150473952293396, 0.3546839952468872, 0.16574017703533173, 0.1823142021894455, 0.14087915420532227, 0.15579576790332794, 0.2760133445262909, 0.25235503911972046, 0.15772190690040588, 0.07097485661506653, 0.24446895718574524, 0.5427369475364685, 0.25956985354423523, 0.07079177349805832, 0.04719451814889908, 0.07079177349805832, 0.5517815351486206, 0.11035630106925964, 0.2207126021385193, 0.05517815053462982, 0.05517815053462982, 0.12966135144233704, 0.12966135144233704, 0.19449202716350555, 0.2593227028846741, 0.3241533637046814, 0.2632722556591034, 0.3916977345943451, 0.1284254789352417, 0.09631911665201187, 0.12200421094894409, 0.4800938367843628, 0.2000391036272049, 0.2000391036272049, 0.0400078184902668, 0.0800156369805336, 0.13006699085235596, 0.23412057757377625, 0.2549313008785248, 0.19510048627853394, 0.18729646503925323, 0.273290753364563, 0.1366453766822815, 0.20496806502342224, 0.34161344170570374, 0.06832268834114075, 0.2391122728586197, 0.1813955157995224, 0.2927064001560211, 0.13192401826381683, 0.15665976703166962, 0.46830928325653076, 0.23415464162826538, 0.11707732081413269, 0.058538660407066345, 0.11707732081413269, 0.22340284287929535, 0.1894456148147583, 0.16263726353645325, 0.15548838675022125, 0.2698706388473511, 0.3503912687301636, 0.14015650749206543, 0.14015650749206543, 0.28031301498413086, 0.07007825374603271, 0.19775497913360596, 0.15820398926734924, 0.3955099582672119, 0.15820398926734924, 0.07910199463367462, 0.20660319924354553, 0.2928018867969513, 0.15461033582687378, 0.13682331144809723, 0.2093396633863449, 0.18402385711669922, 0.32204174995422363, 0.11501490324735641, 0.11501490324735641, 0.27603578567504883, 0.14690333604812622, 0.22035501897335052, 0.14690333604812622, 0.29380667209625244, 0.14690333604812622, 0.06802555918693542, 0.476178914308548, 0.13605111837387085, 0.10203833878040314, 0.238089457154274, 0.5087877511978149, 0.16959592700004578, 0.12719693779945374, 0.12719693779945374, 0.12719693779945374, 0.20740081369876862, 0.1814757138490677, 0.22036336362361908, 0.12314423173666, 0.27221357822418213, 0.08820933103561401, 0.5292559862136841, 0.17641866207122803, 0.17641866207122803, 0.08820933103561401, 0.20287619531154633, 0.25405216217041016, 0.21018704771995544, 0.18277134001255035, 0.14987249672412872, 0.3194291889667511, 0.24466915428638458, 0.1885991394519806, 0.15631639957427979, 0.09005184471607208, 0.2997758686542511, 0.2016674131155014, 0.3025011122226715, 0.10900940746068954, 0.08720752596855164, 0.26796090602874756, 0.2801409363746643, 0.12180040776729584, 0.26796090602874756, 0.06090020388364792, 0.09675849974155426, 0.4837924838066101, 0.19351699948310852, 0.09675849974155426, 0.09675849974155426, 0.2584199607372284, 0.17997105419635773, 0.2584199607372284, 0.09690748900175095, 0.20765890181064606, 0.3242059648036957, 0.2135014832019806, 0.21613730490207672, 0.19241492450237274, 0.052716415375471115, 0.47116509079933167, 0.18846602737903595, 0.09423301368951797, 0.15705502033233643, 0.1256440281867981, 0.4883129894733429, 0.1878127008676529, 0.1126876175403595, 0.1502501517534256, 0.0751250758767128, 0.24273201823234558, 0.24273201823234558, 0.18204902112483978, 0.36409804224967957, 0.060683004558086395, 0.15956434607505798, 0.15956434607505798, 0.31912869215011597, 0.07978217303752899, 0.31912869215011597, 0.21459946036338806, 0.18777452409267426, 0.13412466645240784, 0.2950742542743683, 0.16094960272312164, 0.14138203859329224, 0.25920039415359497, 0.3770187795162201, 0.16494570672512054, 0.04712734743952751, 0.30964159965515137, 0.2672502100467682, 0.20274153351783752, 0.13086044788360596, 0.09031213819980621, 0.16697297990322113, 0.3965608477592468, 0.20175902545452118, 0.1391441524028778, 0.0904437005519867, 0.16670900583267212, 0.16670900583267212, 0.3889876902103424, 0.11113934218883514, 0.16670900583267212, 0.19311951100826263, 0.19311951100826263, 0.38623902201652527, 0.09655975550413132, 0.09655975550413132, 0.07522089034318924, 0.45132535696029663, 0.07522089034318924, 0.22566267848014832, 0.07522089034318924, 0.06333176791667938, 0.25332707166671753, 0.3166588544845581, 0.12666353583335876, 0.25332707166671753, 0.11252903193235397, 0.1500387191772461, 0.3000774383544922, 0.1500387191772461, 0.3000774383544922, 0.26197388768196106, 0.17464926838874817, 0.17464926838874817, 0.11643283814191818, 0.29108211398124695, 0.3484986126422882, 0.1689690351486206, 0.0950450748205185, 0.13728733360767365, 0.2534535527229309, 0.5567437410354614, 0.2783718705177307, 0.06959296762943268, 0.06959296762943268, 0.06959296762943268, 0.29747796058654785, 0.08499370515346527, 0.2549811005592346, 0.3399748206138611, 0.042496852576732635, 0.15441592037677765, 0.46324774622917175, 0.11029708385467529, 0.11029708385467529, 0.13235649466514587, 0.19531340897083282, 0.3847082257270813, 0.21306917071342468, 0.15388329327106476, 0.05326729267835617, 0.293879896402359, 0.1519574224948883, 0.2666422724723816, 0.19926491379737854, 0.088880755007267, 0.06436412036418915, 0.45054885745048523, 0.1287282407283783, 0.16091030836105347, 0.19309236109256744, 0.3773350417613983, 0.20917485654354095, 0.17636311054229736, 0.14355136454105377, 0.09433376044034958, 0.3319230079650879, 0.2336857169866562, 0.2738736867904663, 0.10419107973575592, 0.05656087398529053, 0.139717236161232, 0.279434472322464, 0.139717236161232, 0.3143637776374817, 0.139717236161232, 0.14635029435157776, 0.20428061485290527, 0.27135783433914185, 0.23781920969486237, 0.1372033953666687, 0.5146498680114746, 0.1543949544429779, 0.1543949544429779, 0.1029299721121788, 0.1029299721121788, 0.22273175418376923, 0.18560978770256042, 0.18560978770256042, 0.11136587709188461, 0.29697567224502563, 0.5539829730987549, 0.12310732901096344, 0.12310732901096344, 0.12310732901096344, 0.06155366450548172, 0.2958453893661499, 0.2714959681034088, 0.1899254322052002, 0.14974889159202576, 0.09374524652957916, 0.13991263508796692, 0.27982527017593384, 0.06995631754398346, 0.41973790526390076, 0.13991263508796692, 0.16489331424236298, 0.49467992782592773, 0.16489331424236298, 0.10992886871099472, 0.05496443435549736, 0.11253246665000916, 0.37105298042297363, 0.24534104764461517, 0.15207089483737946, 0.11760149151086807, 0.13011550903320312, 0.26023101806640625, 0.26023101806640625, 0.043371837586164474, 0.30360284447669983, 0.08796219527721405, 0.43981096148490906, 0.08796219527721405, 0.08796219527721405, 0.26388657093048096, 0.2315499633550644, 0.26396697759628296, 0.3334319591522217, 0.07409598678350449, 0.09725099056959152, 0.15398891270160675, 0.23098337650299072, 0.3079778254032135, 0.15398891270160675, 0.15398891270160675, 0.2684227228164673, 0.14719955623149872, 0.2280149906873703, 0.2078111320734024, 0.15008582174777985, 0.23219949007034302, 0.23219949007034302, 0.1547996699810028, 0.0773998349905014, 0.3095993399620056, 0.13372527062892914, 0.31759753823280334, 0.16715659201145172, 0.28416621685028076, 0.08357829600572586, 0.32901230454444885, 0.10967077314853668, 0.14622770249843597, 0.29245540499687195, 0.10967077314853668, 0.18760141730308533, 0.18760141730308533, 0.37520283460617065, 0.1250676065683365, 0.06253380328416824, 0.07159411907196045, 0.5011588335037231, 0.1431882381439209, 0.1431882381439209, 0.1431882381439209, 0.12469583749771118, 0.4987833499908447, 0.18704375624656677, 0.12469583749771118, 0.06234791874885559, 0.3928937017917633, 0.2308609038591385, 0.1534292846918106, 0.08890295773744583, 0.13335442543029785, 0.23787176609039307, 0.24508000910282135, 0.30995410680770874, 0.08649882674217224, 0.12974824011325836, 0.3390201926231384, 0.2300104796886444, 0.1624244600534439, 0.07412660121917725, 0.19512738287448883, 0.18825919926166534, 0.28893256187438965, 0.2385958880186081, 0.10671377182006836, 0.1781918704509735, 0.13334979116916656, 0.5333991646766663, 0.22224965691566467, 0.044449929147958755, 0.08889985829591751, 0.4849591851234436, 0.13855977356433868, 0.10391982644796371, 0.13855977356433868, 0.10391982644796371, 0.1619780957698822, 0.28279781341552734, 0.18322111666202545, 0.21508565545082092, 0.1566673368215561, 0.12339672446250916, 0.2467934489250183, 0.18509508669376373, 0.3084917962551117, 0.12339672446250916, 0.17494946718215942, 0.38488882780075073, 0.10496968030929565, 0.06997978687286377, 0.2799191474914551, 0.4117327332496643, 0.17457467317581177, 0.15481150150299072, 0.17457467317581177, 0.08564040809869766, 0.3673076331615448, 0.14647236466407776, 0.22083525359630585, 0.1983010470867157, 0.06760262697935104, 0.2235751748085022, 0.2614692747592926, 0.20083871483802795, 0.23115399479866028, 0.08147230744361877, 0.21606603264808655, 0.28808802366256714, 0.14404401183128357, 0.07202200591564178, 0.28808802366256714, 0.1241845116019249, 0.2483690232038498, 0.3725535273551941, 0.1241845116019249, 0.1241845116019249, 0.28258582949638367, 0.2514534890651703, 0.1508720964193344, 0.23468993604183197, 0.08142303675413132, 0.22690148651599884, 0.27984514832496643, 0.14559511840343475, 0.14937680959701538, 0.19664795696735382, 0.12306247651576996, 0.12306247651576996, 0.20510412752628326, 0.1640833020210266, 0.36918744444847107, 0.24106855690479279, 0.25941070914268494, 0.1886623501777649, 0.09695148468017578, 0.20962482690811157, 0.3052816689014435, 0.2825194299221039, 0.2584182620048523, 0.07632041722536087, 0.07765937596559525, 0.4743940234184265, 0.17789776623249054, 0.17789776623249054, 0.11859850585460663, 0.059299252927303314, 0.2880164682865143, 0.18001028895378113, 0.16200926899909973, 0.2700154483318329, 0.09000514447689056, 0.2881200909614563, 0.306734561920166, 0.16429318487644196, 0.13272947072982788, 0.10764036327600479, 0.27956730127334595, 0.09318910539150238, 0.18637821078300476, 0.3727564215660095, 0.09318910539150238, 0.21947729587554932, 0.3799905478954315, 0.15232379734516144, 0.10154920071363449, 0.14741012454032898, 0.18000780045986176, 0.2496882528066635, 0.29904523491859436, 0.17710445821285248, 0.0987139567732811, 0.15170596539974213, 0.28173965215682983, 0.39010104537010193, 0.1300336867570877, 0.043344561010599136, 0.2693784534931183, 0.3641459345817566, 0.14625532925128937, 0.0813358724117279, 0.13879331946372986, 0.2375071495771408, 0.1781303584575653, 0.1781303584575653, 0.1781303584575653, 0.2968839406967163, 0.43020355701446533, 0.2657628059387207, 0.15945768356323242, 0.09633901715278625, 0.04816950857639313, 0.17221687734127045, 0.4477638900279999, 0.10333012789487839, 0.13777349889278412, 0.10333012789487839, 0.15831515192985535, 0.47494545578956604, 0.07915757596492767, 0.07915757596492767, 0.23747272789478302, 0.18992879986763, 0.4520305395126343, 0.1359890252351761, 0.1413070261478424, 0.08052980899810791, 0.1664239764213562, 0.24408850073814392, 0.14423412084579468, 0.1664239764213562, 0.28846824169158936, 0.11009103804826736, 0.24220028519630432, 0.3522913157939911, 0.11009103804826736, 0.17614565789699554, 0.15989235043525696, 0.07994617521762848, 0.3197847008705139, 0.15989235043525696, 0.3197847008705139, 0.4830538332462311, 0.15344063937664032, 0.14207465946674347, 0.07956180721521378, 0.14207465946674347, 0.2877883315086365, 0.2853699326515198, 0.159613698720932, 0.09915396571159363, 0.16686886548995972, 0.4740296006202698, 0.158009871840477, 0.11850740015506744, 0.158009871840477, 0.0790049359202385, 0.2438850849866867, 0.3185748755931854, 0.1951080709695816, 0.12956394255161285, 0.11432113498449326, 0.22535526752471924, 0.19718585908412933, 0.12676233053207397, 0.3028211295604706, 0.14084704220294952, 0.21167108416557312, 0.24487438797950745, 0.1743173599243164, 0.14111405611038208, 0.22827273607254028, 0.2774614095687866, 0.24755539000034332, 0.25586262345314026, 0.10467106848955154, 0.11297830194234848, 0.2217216044664383, 0.2217216044664383, 0.3880127966403961, 0.11086080223321915, 0.11086080223321915, 0.30442753434181213, 0.2757851481437683, 0.14321187138557434, 0.1726725995540619, 0.10393090546131134, 0.32565680146217346, 0.28653568029403687, 0.14908315241336823, 0.10573273152112961, 0.13216590881347656, 0.2701914608478546, 0.23108480870723724, 0.15642663836479187, 0.10665452480316162, 0.23108480870723724, 0.2281513214111328, 0.13689079880714417, 0.20533619821071625, 0.1140756607055664, 0.31941184401512146, 0.14706525206565857, 0.2573641836643219, 0.3308968245983124, 0.07353262603282928, 0.14706525206565857, 0.3513690233230591, 0.17568451166152954, 0.20353692770004272, 0.1992519497871399, 0.07070229947566986, 0.2811446785926819, 0.16868680715560913, 0.16868680715560913, 0.33737361431121826, 0.05622893571853638, 0.1456550508737564, 0.21848256886005402, 0.0728275254368782, 0.3641376197338104, 0.21848256886005402, 0.38431909680366516, 0.22344133257865906, 0.19215954840183258, 0.12959598004817963, 0.07597005367279053, 0.29863086342811584, 0.2389046847820282, 0.3498247265815735, 0.05972617119550705, 0.05972617119550705, 0.46792715787887573, 0.3119514286518097, 0.05199190601706505, 0.05199190601706505, 0.1039838120341301, 0.21630090475082397, 0.19034479558467865, 0.28984323143959045, 0.19467081129550934, 0.10815045237541199, 0.49689772725105286, 0.2760542929172516, 0.05521085858345032, 0.05521085858345032, 0.11042171716690063, 0.18117646872997284, 0.25614604353904724, 0.09995942562818527, 0.08746449649333954, 0.3779715895652771, 0.28897377848625183, 0.1541193574666977, 0.11558951437473297, 0.17338427901268005, 0.26970887184143066, 0.17975786328315735, 0.17975786328315735, 0.11983857303857803, 0.3595157265663147, 0.17975786328315735, 0.20258685946464539, 0.16882237792015076, 0.20258685946464539, 0.32638993859291077, 0.10129342973232269, 0.17777575552463531, 0.22681733965873718, 0.3249005079269409, 0.17164555191993713, 0.10421337187290192, 0.22598661482334137, 0.2501378655433655, 0.24668768048286438, 0.1552579700946808, 0.12075620144605637, 0.2580251097679138, 0.1290125548839569, 0.3225313723087311, 0.06450627744197845, 0.19351881742477417, 0.27217012643814087, 0.24285949766635895, 0.28054457902908325, 0.10468082129955292, 0.10049358755350113, 0.25535061955451965, 0.22316357493400574, 0.180247500538826, 0.19526812434196472, 0.1448417454957962, 0.16973909735679626, 0.2546086311340332, 0.08486954867839813, 0.16973909735679626, 0.2546086311340332, 0.07400059700012207, 0.2220017910003662, 0.2220017910003662, 0.07400059700012207, 0.37000298500061035, 0.2731523811817169, 0.2780300974845886, 0.25120264291763306, 0.114626444876194, 0.08292125910520554, 0.10809724777936935, 0.30267229676246643, 0.15133614838123322, 0.17295560240745544, 0.259433388710022, 0.24011428654193878, 0.3891962170600891, 0.12269575893878937, 0.13720816373825073, 0.11082197725772858, 0.23655277490615845, 0.21504797041416168, 0.129028782248497, 0.279562383890152, 0.139781191945076, 0.14783938229084015, 0.2956787645816803, 0.14783938229084015, 0.2956787645816803, 0.22175908088684082, 0.21063874661922455, 0.3667592406272888, 0.13381755352020264, 0.1115146353840828, 0.1784234195947647, 0.26625898480415344, 0.19018499553203583, 0.12679000198841095, 0.1394689977169037, 0.2789379954338074, 0.16132771968841553, 0.4839831292629242, 0.14340241253376007, 0.0896265059709549, 0.12547710537910461, 0.18042773008346558, 0.23094749450683594, 0.20207905769348145, 0.2525988221168518, 0.13712507486343384, 0.20089198648929596, 0.3099476397037506, 0.205196812748909, 0.21380646526813507, 0.06887724995613098, 0.1721772849559784, 0.1721772849559784, 0.2582659125328064, 0.0860886424779892, 0.3443545699119568, 0.25252822041511536, 0.21394753456115723, 0.20693285763263702, 0.25252822041511536, 0.07716140151023865, 0.2876932919025421, 0.14384664595127106, 0.3356421887874603, 0.11987220495939255, 0.09589776396751404, 0.2724915146827698, 0.18055331707000732, 0.3023991286754608, 0.1329226940870285, 0.11187659949064255, 0.13161975145339966, 0.32904937863349915, 0.1974296271800995, 0.2632395029067993, 0.06580987572669983, 0.2560487985610962, 0.3020062744617462, 0.19170832633972168, 0.12868092954158783, 0.12211557477712631, 0.22520799934864044, 0.38022127747535706, 0.11114160716533661, 0.11406638473272324, 0.16963718831539154, 0.13944752514362335, 0.41834259033203125, 0.13944752514362335, 0.20917129516601562, 0.06972376257181168, 0.2631106972694397, 0.21925890445709229, 0.13155534863471985, 0.3069624602794647, 0.08770356327295303, 0.5991309285163879, 0.19971030950546265, 0.04992757737636566, 0.09985515475273132, 0.04992757737636566, 0.47182992100715637, 0.15727663040161133, 0.15727663040161133, 0.15727663040161133, 0.15727663040161133, 0.21339669823646545, 0.22981181740760803, 0.34471774101257324, 0.13542482256889343, 0.07797186821699142, 0.1233183890581131, 0.43161436915397644, 0.1233183890581131, 0.18497757613658905, 0.06165919452905655, 0.47365400195121765, 0.1353297233581543, 0.1353297233581543, 0.1353297233581543, 0.06766486167907715, 0.18757422268390656, 0.4689355492591858, 0.09378711134195328, 0.09378711134195328, 0.09378711134195328, 0.5086939930915833, 0.16956466436386108, 0.08478233218193054, 0.08478233218193054, 0.08478233218193054, 0.20282146334648132, 0.1802857518196106, 0.33352863788604736, 0.15775002539157867, 0.12620002031326294, 0.1531347632408142, 0.1531347632408142, 0.3828369081020355, 0.0765673816204071, 0.1531347632408142, 0.1820736676454544, 0.2124192863702774, 0.2275920957326889, 0.14414165914058685, 0.23517850041389465, 0.19918066263198853, 0.13278710842132568, 0.39836132526397705, 0.13278710842132568, 0.06639355421066284, 0.23826543986797333, 0.07942181080579758, 0.23826543986797333, 0.3176872432231903, 0.15884362161159515, 0.19564636051654816, 0.29346954822540283, 0.3912927210330963, 0.09782318025827408, 0.09782318025827408, 0.46536317467689514, 0.2260335385799408, 0.10636872798204422, 0.10636872798204422, 0.09307263791561127, 0.3813493847846985, 0.1783161461353302, 0.1730196326971054, 0.13417848944664001, 0.13417848944664001, 0.5372210144996643, 0.13430525362491608, 0.13430525362491608, 0.06715262681245804, 0.06715262681245804, 0.07907258719205856, 0.5535081028938293, 0.1976814568042755, 0.15814517438411713, 0.01976814679801464, 0.5013887882232666, 0.11570511013269424, 0.11570511013269424, 0.11570511013269424, 0.11570511013269424, 0.25982362031936646, 0.2086462527513504, 0.2086462527513504, 0.22045642137527466, 0.10235476493835449, 0.22583092749118805, 0.3140867054462433, 0.14017091691493988, 0.18689456582069397, 0.13238364458084106, 0.24760103225708008, 0.27311715483665466, 0.1946786642074585, 0.1823931187391281, 0.10206454247236252, 0.47211402654647827, 0.18051418662071228, 0.12497135996818542, 0.15274277329444885, 0.08331423997879028, 0.18270961940288544, 0.4241473078727722, 0.1239815205335617, 0.13703221082687378, 0.13050687313079834, 0.17673170566558838, 0.22091463208198547, 0.35346341133117676, 0.08836585283279419, 0.13254877924919128, 0.20403741300106049, 0.20403741300106049, 0.20403741300106049, 0.13602493703365326, 0.2720498740673065, 0.3005027174949646, 0.1287868767976761, 0.1287868767976761, 0.3005027174949646, 0.1287868767976761, 0.1762845367193222, 0.22035565972328186, 0.3525690734386444, 0.0881422683596611, 0.13221339881420135, 0.1700437217950821, 0.23381012678146362, 0.3400874435901642, 0.12753279507160187, 0.12753279507160187, 0.31078439950942993, 0.21264195442199707, 0.27807024121284485, 0.10141385346651077, 0.09814243763685226, 0.4045538902282715, 0.1401919424533844, 0.22831259667873383, 0.12016452103853226, 0.10414258390665054, 0.39450520277023315, 0.24044661223888397, 0.1151840016245842, 0.15261881053447723, 0.09790640324354172, 0.14344032108783722, 0.32274073362350464, 0.20395420491695404, 0.1479228287935257, 0.18154165148735046, 0.11409217119216919, 0.46539756655693054, 0.13050830364227295, 0.10095925629138947, 0.1896064132452011, 0.09497284889221191, 0.28491854667663574, 0.09497284889221191, 0.28491854667663574, 0.18994569778442383, 0.4136790931224823, 0.19699004292488098, 0.162516787648201, 0.10834451764822006, 0.11819402128458023, 0.5006468892097473, 0.12516172230243683, 0.18774259090423584, 0.06258086115121841, 0.12516172230243683, 0.21525096893310547, 0.37607067823410034, 0.20288023352622986, 0.09649181365966797, 0.10886256396770477, 0.13752418756484985, 0.13752418756484985, 0.20628628134727478, 0.2750483751296997, 0.2750483751296997, 0.2635836601257324, 0.23894965648651123, 0.14287711679935455, 0.20446208119392395, 0.1502673178911209, 0.46070030331611633, 0.16453582048416138, 0.16453582048416138, 0.13162866234779358, 0.09872149676084518, 0.17558470368385315, 0.43896177411079407, 0.17558470368385315, 0.17558470368385315, 0.08779235184192657, 0.18644319474697113, 0.45279061794281006, 0.14649108052253723, 0.1198563352227211, 0.10653896629810333, 0.3505609333515167, 0.2055012434720993, 0.17460890114307404, 0.17595204710960388, 0.0926770269870758, 0.2795471251010895, 0.16598109900951385, 0.28828296065330505, 0.1485094130039215, 0.11793394386768341, 0.34424862265586853, 0.39218196272850037, 0.10458186268806458, 0.07407882064580917, 0.08279397338628769, 0.2005782276391983, 0.4680158793926239, 0.11143235117197037, 0.0668594092130661, 0.1337188184261322, 0.2886829078197479, 0.21651217341423035, 0.1577063947916031, 0.14701443910598755, 0.1897822767496109, 0.13280081748962402, 0.4426693618297577, 0.14386755228042603, 0.15493428707122803, 0.13280081748962402, 0.4033574163913727, 0.21737824380397797, 0.14733414351940155, 0.14974945783615112, 0.07970535755157471, 0.47721779346466064, 0.17895667254924774, 0.17895667254924774, 0.11930444836616516, 0.05965222418308258, 0.32202842831611633, 0.10734280943870544, 0.37569984793663025, 0.10734280943870544, 0.05367140471935272, 0.38892289996147156, 0.24380241334438324, 0.14802289009094238, 0.10738915950059891, 0.11029157042503357, 0.28214558959007263, 0.36253342032432556, 0.1402846723794937, 0.1229461207985878, 0.092997707426548, 0.22989310324192047, 0.22989310324192047, 0.15326206386089325, 0.11494655162096024, 0.2682086229324341, 0.2181791067123413, 0.24598625302314758, 0.24170823395252228, 0.13689669966697693, 0.1561477929353714, 0.5326626896858215, 0.10653253644704819, 0.053266268223524094, 0.15979880094528198, 0.10653253644704819, 0.1076703667640686, 0.4306814670562744, 0.1973956674337387, 0.0717802420258522, 0.1794506162405014, 0.25863930583000183, 0.17242620885372162, 0.34485241770744324, 0.08621310442686081, 0.08621310442686081, 0.1924833357334137, 0.25664445757865906, 0.1924833357334137, 0.06416111439466476, 0.25664445757865906, 0.17785409092903137, 0.17785409092903137, 0.4446352422237396, 0.08892704546451569, 0.08892704546451569, 0.3073042035102844, 0.15678785741329193, 0.3700193464756012, 0.08780119568109512, 0.08780119568109512, 0.42123910784721375, 0.2253139466047287, 0.12735135853290558, 0.13714762032032013, 0.08816632628440857, 0.2234461009502411, 0.3676048815250397, 0.19101038575172424, 0.1477627456188202, 0.06847541779279709, 0.3222644329071045, 0.16113221645355225, 0.16113221645355225, 0.3222644329071045, 0.0644528865814209, 0.18847627937793732, 0.2827144265174866, 0.09423813968896866, 0.2827144265174866, 0.09423813968896866, 0.39696356654167175, 0.1714978665113449, 0.13432000577449799, 0.1487114280462265, 0.14751213788986206, 0.06059940531849861, 0.15149851143360138, 0.45449554920196533, 0.030299702659249306, 0.30299702286720276, 0.23160210251808167, 0.23160210251808167, 0.15440140664577484, 0.07720070332288742, 0.3088028132915497, 0.33578211069107056, 0.1445138156414032, 0.2805268168449402, 0.1530146300792694, 0.08500812947750092, 0.19804933667182922, 0.19804933667182922, 0.363090455532074, 0.16504111886024475, 0.09902466833591461, 0.448411762714386, 0.09964705258607864, 0.09964705258607864, 0.1992941051721573, 0.09964705258607864, 0.1905244141817093, 0.1612129658460617, 0.10259006917476654, 0.20518013834953308, 0.3663930892944336, 0.15556877851486206, 0.25928130745887756, 0.2074250429868698, 0.15556877851486206, 0.25928130745887756, 0.2519007921218872, 0.1889255940914154, 0.1259503960609436, 0.2938842475414276, 0.1259503960609436, 0.1373441368341446, 0.2231842279434204, 0.25752025842666626, 0.12876012921333313, 0.2489362508058548, 0.22744743525981903, 0.4125790596008301, 0.14281582832336426, 0.08463159948587418, 0.13223688304424286, 0.17566165328025818, 0.6148158311843872, 0.08783082664012909, 0.08783082664012909, 0.08783082664012909, 0.09452258050441742, 0.28356772661209106, 0.09452258050441742, 0.28356772661209106, 0.28356772661209106, 0.13929392397403717, 0.29573172330856323, 0.2378711700439453, 0.21215535700321198, 0.11464961618185043, 0.20806072652339935, 0.2346731573343277, 0.2564469575881958, 0.18386761844158173, 0.11854623258113861, 0.13583505153656006, 0.2263917475938797, 0.13583505153656006, 0.31694844365119934, 0.18111339211463928, 0.632792592048645, 0.12655851244926453, 0.06327925622463226, 0.06327925622463226, 0.06327925622463226, 0.11312045156955719, 0.5090420246124268, 0.1696806699037552, 0.11312045156955719, 0.11312045156955719, 0.28941810131073, 0.3138621747493744, 0.16621984541416168, 0.11439836770296097, 0.11635389178991318, 0.3125518262386322, 0.24797499179840088, 0.12657056748867035, 0.21956118941307068, 0.09299062192440033], \"Term\": [\"Facebook\", \"Facebook\", \"Facebook\", \"Facebook\", \"Facebook\", \"People\", \"People\", \"People\", \"People\", \"People\", \"Technology\", \"Technology\", \"Technology\", \"Technology\", \"Technology\", \"able\", \"able\", \"able\", \"able\", \"able\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"acess\", \"acess\", \"acess\", \"acess\", \"acess\", \"addicting\", \"addicting\", \"addicting\", \"addicting\", \"addicting\", \"addiction\", \"addiction\", \"addiction\", \"addiction\", \"addiction\", \"adult\", \"adult\", \"adult\", \"adult\", \"adult\", \"advice\", \"advice\", \"advice\", \"advice\", \"advice\", \"affect\", \"affect\", \"affect\", \"affect\", \"affect\", \"agree\", \"agree\", \"agree\", \"agree\", \"agree\", \"aid\", \"aid\", \"aid\", \"aid\", \"aid\", \"allow\", \"allow\", \"allow\", \"allow\", \"allow\", \"alow\", \"alow\", \"alow\", \"alow\", \"alow\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"alternative\", \"apart\", \"apart\", \"apart\", \"apart\", \"apart\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"away\", \"away\", \"away\", \"away\", \"away\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad_effect\", \"bad_effect\", \"bad_effect\", \"bad_effect\", \"bad_effect\", \"beautiful_nature\", \"beautiful_nature\", \"beautiful_nature\", \"beautiful_nature\", \"beautiful_nature\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"beneficial_society\", \"beneficial_society\", \"beneficial_society\", \"beneficial_society\", \"beneficial_society\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit_society\", \"benefit_society\", \"benefit_society\", \"benefit_society\", \"benefit_society\", \"benifit\", \"benifit\", \"benifit\", \"benifit\", \"benifit\", \"better\", \"better\", \"better\", \"better\", \"better\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big_problem\", \"big_problem\", \"big_problem\", \"big_problem\", \"big_problem\", \"board_game\", \"board_game\", \"board_game\", \"board_game\", \"board_game\", \"book\", \"book\", \"book\", \"book\", \"book\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"buisnesse\", \"buisnesse\", \"buisnesse\", \"buisnesse\", \"buisnesse\", \"buissness\", \"buissness\", \"buissness\", \"buissness\", \"buissness\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"burn_calorie\", \"business\", \"business\", \"business\", \"business\", \"business\", \"career\", \"career\", \"career\", \"career\", \"career\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"challenge\", \"change\", \"change\", \"change\", \"change\", \"change\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"chat\", \"chat\", \"chat\", \"chat\", \"chat\", \"chating\", \"chating\", \"chating\", \"chating\", \"chating\", \"cheap\", \"cheap\", \"cheap\", \"cheap\", \"cheap\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child_adult\", \"child_adult\", \"child_adult\", \"child_adult\", \"child_adult\", \"claim\", \"claim\", \"claim\", \"claim\", \"claim\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"click_away\", \"click_away\", \"click_away\", \"click_away\", \"click_away\", \"close\", \"close\", \"close\", \"close\", \"close\", \"com\", \"com\", \"com\", \"com\", \"com\", \"come\", \"come\", \"come\", \"come\", \"come\", \"communicate\", \"communicate\", \"communicate\", \"communicate\", \"communicate\", \"communication\", \"communication\", \"communication\", \"communication\", \"communication\", \"computor\", \"computor\", \"computor\", \"computor\", \"computor\", \"conclution\", \"conclution\", \"conclution\", \"conclution\", \"conclution\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"country\", \"country\", \"country\", \"country\", \"country\", \"crime\", \"crime\", \"crime\", \"crime\", \"crime\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"current_event\", \"current_event\", \"current_event\", \"current_event\", \"current_event\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"daily_life\", \"daily_life\", \"daily_life\", \"daily_life\", \"daily_life\", \"damage\", \"damage\", \"damage\", \"damage\", \"damage\", \"dear\", \"dear\", \"dear\", \"dear\", \"dear\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_local_newspaper\", \"dear_reader\", \"dear_reader\", \"dear_reader\", \"dear_reader\", \"dear_reader\", \"degree\", \"degree\", \"degree\", \"degree\", \"degree\", \"delete\", \"delete\", \"delete\", \"delete\", \"delete\", \"depressed\", \"depressed\", \"depressed\", \"depressed\", \"depressed\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"design\", \"design\", \"design\", \"design\", \"design\", \"die\", \"die\", \"die\", \"die\", \"die\", \"disaster\", \"disaster\", \"disaster\", \"disaster\", \"disaster\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"dollar\", \"download\", \"download\", \"download\", \"download\", \"download\", \"e_mail\", \"e_mail\", \"e_mail\", \"e_mail\", \"e_mail\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"ect\", \"ect\", \"ect\", \"ect\", \"ect\", \"education\", \"education\", \"education\", \"education\", \"education\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"electricity\", \"electricity\", \"electricity\", \"electricity\", \"electricity\", \"email\", \"email\", \"email\", \"email\", \"email\", \"enhance\", \"enhance\", \"enhance\", \"enhance\", \"enhance\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"evolve\", \"evolve\", \"evolve\", \"evolve\", \"evolve\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excersice\", \"excersice\", \"excersice\", \"excersice\", \"excersice\", \"exercis\", \"exercis\", \"exercis\", \"exercis\", \"exercis\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"exercise_enjoy_nature_interact\", \"explore_nature\", \"explore_nature\", \"explore_nature\", \"explore_nature\", \"explore_nature\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye_hand\", \"eye_hand\", \"eye_hand\", \"eye_hand\", \"eye_hand\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fail_class\", \"fail_class\", \"fail_class\", \"fail_class\", \"fail_class\", \"famous\", \"famous\", \"famous\", \"famous\", \"famous\", \"final\", \"final\", \"final\", \"final\", \"final\", \"foreign_place\", \"foreign_place\", \"foreign_place\", \"foreign_place\", \"foreign_place\", \"frend\", \"frend\", \"frend\", \"frend\", \"frend\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"future\", \"future\", \"future\", \"future\", \"future\", \"game\", \"game\", \"game\", \"game\", \"game\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get_fat\", \"get_fat\", \"get_fat\", \"get_fat\", \"get_fat\", \"government\", \"government\", \"government\", \"government\", \"government\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great_resource\", \"great_resource\", \"great_resource\", \"great_resource\", \"great_resource\", \"guess\", \"guess\", \"guess\", \"guess\", \"guess\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"hand_eye_coordination\", \"have\", \"have\", \"have\", \"have\", \"have\", \"headache\", \"headache\", \"headache\", \"headache\", \"headache\", \"health_issue\", \"health_issue\", \"health_issue\", \"health_issue\", \"health_issue\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"homework\", \"homework\", \"homework\", \"homework\", \"homework\", \"honestly\", \"honestly\", \"honestly\", \"honestly\", \"honestly\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"important\", \"important\", \"important\", \"important\", \"important\", \"incredible\", \"incredible\", \"incredible\", \"incredible\", \"incredible\", \"inform\", \"inform\", \"inform\", \"inform\", \"inform\", \"information\", \"information\", \"information\", \"information\", \"information\", \"instal\", \"instal\", \"instal\", \"instal\", \"instal\", \"instead\", \"instead\", \"instead\", \"instead\", \"instead\", \"interact\", \"interact\", \"interact\", \"interact\", \"interact\", \"interact_family\", \"interact_family\", \"interact_family\", \"interact_family\", \"interact_family\", \"internet\", \"internet\", \"internet\", \"internet\", \"internet\", \"intouch\", \"intouch\", \"intouch\", \"intouch\", \"intouch\", \"job\", \"job\", \"job\", \"job\", \"job\", \"jog\", \"jog\", \"jog\", \"jog\", \"jog\", \"kick\", \"kick\", \"kick\", \"kick\", \"kick\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"lack_exercise\", \"lack_exercise\", \"lack_exercise\", \"lack_exercise\", \"lack_exercise\", \"lake\", \"lake\", \"lake\", \"lake\", \"lake\", \"laptop\", \"laptop\", \"laptop\", \"laptop\", \"laptop\", \"lastly\", \"lastly\", \"lastly\", \"lastly\", \"lastly\", \"lesson\", \"lesson\", \"lesson\", \"lesson\", \"lesson\", \"let\", \"let\", \"let\", \"let\", \"let\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"little\", \"little\", \"little\", \"little\", \"little\", \"live\", \"live\", \"live\", \"live\", \"live\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"live_faraway\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love_one\", \"love_one\", \"love_one\", \"love_one\", \"love_one\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"make\", \"make\", \"make\", \"make\", \"make\", \"massive\", \"massive\", \"massive\", \"massive\", \"massive\", \"material\", \"material\", \"material\", \"material\", \"material\", \"meet\", \"meet\", \"meet\", \"meet\", \"meet\", \"meet_new\", \"meet_new\", \"meet_new\", \"meet_new\", \"meet_new\", \"method\", \"method\", \"method\", \"method\", \"method\", \"money\", \"money\", \"money\", \"money\", \"money\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"natural_disaster\", \"nature\", \"nature\", \"nature\", \"nature\", \"nature\", \"neat\", \"neat\", \"neat\", \"neat\", \"neat\", \"neatly\", \"neatly\", \"neatly\", \"neatly\", \"neatly\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative_effect\", \"negative_effect\", \"negative_effect\", \"negative_effect\", \"negative_effect\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new_one\", \"new_one\", \"new_one\", \"new_one\", \"new_one\", \"news\", \"news\", \"news\", \"news\", \"news\", \"not\", \"not\", \"not\", \"not\", \"not\", \"o\", \"o\", \"o\", \"o\", \"o\", \"obease\", \"obease\", \"obease\", \"obease\", \"obease\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"opinion\", \"outdoors\", \"outdoors\", \"outdoors\", \"outdoors\", \"outdoors\", \"outside\", \"outside\", \"outside\", \"outside\", \"outside\", \"page\", \"page\", \"page\", \"page\", \"page\", \"page_essay\", \"page_essay\", \"page_essay\", \"page_essay\", \"page_essay\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"park\", \"park\", \"park\", \"park\", \"park\", \"past\", \"past\", \"past\", \"past\", \"past\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"person\", \"person\", \"person\", \"person\", \"person\", \"personal_experience\", \"personal_experience\", \"personal_experience\", \"personal_experience\", \"personal_experience\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"physically\", \"physically\", \"physically\", \"physically\", \"physically\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plan_vacation\", \"plan_vacation\", \"plan_vacation\", \"plan_vacation\", \"plan_vacation\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play_game\", \"play_game\", \"play_game\", \"play_game\", \"play_game\", \"plug\", \"plug\", \"plug\", \"plug\", \"plug\", \"point_view\", \"point_view\", \"point_view\", \"point_view\", \"point_view\", \"portable\", \"portable\", \"portable\", \"portable\", \"portable\", \"pose\", \"pose\", \"pose\", \"pose\", \"pose\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"positive_effect\", \"post_picture\", \"post_picture\", \"post_picture\", \"post_picture\", \"post_picture\", \"power_point\", \"power_point\", \"power_point\", \"power_point\", \"power_point\", \"produce\", \"produce\", \"produce\", \"produce\", \"produce\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"program\", \"program\", \"program\", \"program\", \"program\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"question\", \"question\", \"question\", \"question\", \"question\", \"quiz\", \"quiz\", \"quiz\", \"quiz\", \"quiz\", \"radio\", \"radio\", \"radio\", \"radio\", \"radio\", \"relative_live\", \"relative_live\", \"relative_live\", \"relative_live\", \"relative_live\", \"rely\", \"rely\", \"rely\", \"rely\", \"rely\", \"research\", \"research\", \"research\", \"research\", \"research\", \"researching\", \"researching\", \"researching\", \"researching\", \"researching\", \"reson\", \"reson\", \"reson\", \"reson\", \"reson\", \"road\", \"road\", \"road\", \"road\", \"road\", \"save\", \"save\", \"save\", \"save\", \"save\", \"say\", \"say\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school_project\", \"school_project\", \"school_project\", \"school_project\", \"school_project\", \"school_work\", \"school_work\", \"school_work\", \"school_work\", \"school_work\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"schoolwork\", \"seriously\", \"seriously\", \"seriously\", \"seriously\", \"seriously\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shoot\", \"shop_online\", \"shop_online\", \"shop_online\", \"shop_online\", \"shop_online\", \"shopping\", \"shopping\", \"shopping\", \"shopping\", \"shopping\", \"site\", \"site\", \"site\", \"site\", \"site\", \"skill\", \"skill\", \"skill\", \"skill\", \"skill\", \"society\", \"society\", \"society\", \"society\", \"society\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend_time\", \"spend_time\", \"spend_time\", \"spend_time\", \"spend_time\", \"spending_time_family\", \"spending_time_family\", \"spending_time_family\", \"spending_time_family\", \"spending_time_family\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"star\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"start\", \"status\", \"status\", \"status\", \"status\", \"status\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"storm\", \"storm\", \"storm\", \"storm\", \"storm\", \"story\", \"story\", \"story\", \"story\", \"story\", \"student\", \"student\", \"student\", \"student\", \"student\", \"study\", \"study\", \"study\", \"study\", \"study\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"stuff\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take_away\", \"take_away\", \"take_away\", \"take_away\", \"take_away\", \"teach\", \"teach\", \"teach\", \"teach\", \"teach\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_hand_eye\", \"teach_skill\", \"teach_skill\", \"teach_skill\", \"teach_skill\", \"teach_skill\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"thay\", \"thay\", \"thay\", \"thay\", \"thay\", \"thi\", \"thi\", \"thi\", \"thi\", \"thi\", \"throught\", \"throught\", \"throught\", \"throught\", \"throught\", \"till\", \"till\", \"till\", \"till\", \"till\", \"tradition\", \"tradition\", \"tradition\", \"tradition\", \"tradition\", \"travel\", \"travel\", \"travel\", \"travel\", \"travel\", \"trip\", \"trip\", \"trip\", \"trip\", \"trip\", \"try\", \"try\", \"try\", \"try\", \"try\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter_facebook\", \"twitter_facebook\", \"twitter_facebook\", \"twitter_facebook\", \"twitter_facebook\", \"type\", \"type\", \"type\", \"type\", \"type\", \"u\", \"u\", \"u\", \"u\", \"u\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"unfortunately\", \"useful\", \"useful\", \"useful\", \"useful\", \"useful\", \"usefull\", \"usefull\", \"usefull\", \"usefull\", \"usefull\", \"video_chatting\", \"video_chatting\", \"video_chatting\", \"video_chatting\", \"video_chatting\", \"virus\", \"virus\", \"virus\", \"virus\", \"virus\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"war\", \"war\", \"war\", \"war\", \"war\", \"waste\", \"waste\", \"waste\", \"waste\", \"waste\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch_news\", \"watch_news\", \"watch_news\", \"watch_news\", \"watch_news\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"watch_tv\", \"website\", \"website\", \"website\", \"website\", \"website\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well_grade\", \"well_grade\", \"well_grade\", \"well_grade\", \"well_grade\", \"white\", \"white\", \"white\", \"white\", \"white\", \"will_able\", \"will_able\", \"will_able\", \"will_able\", \"will_able\", \"work\", \"work\", \"work\", \"work\", \"work\", \"write\", \"write\", \"write\", \"write\", \"write\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 5, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el145263126017848220948018\", ldavis_el145263126017848220948018_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el145263126017848220948018\", ldavis_el145263126017848220948018_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el145263126017848220948018\", ldavis_el145263126017848220948018_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing text with LDA\n",
    "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% _Topic A_, 20% _Topic B_, 20% _Topic C_, and 10% _Topic D_.\n",
    "\n",
    "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n",
    "1. Using spaCy to remove punctuation and lemmatize the text\n",
    "1. Applying our first-order phrase model to join word pairs\n",
    "1. Applying our second-order phrase model to join longer phrases\n",
    "1. Removing stopwords\n",
    "1. Creating a bag-of-words representation\n",
    "\n",
    "Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The `lda_description(...)` function will perform all these steps for us, including printing the resulting topical description of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_essay(essay_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(essay_set1_txt_filepath),essay_number, essay_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(essay_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the essay text with spaCy\n",
    "    parsed_essay = nlp(essay_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_essay = [token.lemma_ for token in parsed_essay if not punct_space_stop(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_essay = bigram_model[unigram_essay]\n",
    "    trigram_essay = trigram_model[bigram_essay]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    essay_bow = trigram_dictionary.doc2bow(trigram_essay)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    essay_lda = lda[essay_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    essay_lda = sorted(essay_lda)\n",
    "    \n",
    "    topics = [];\n",
    "    freqs = [];\n",
    "    for topic_number, freq in essay_lda:\n",
    "#         if freq < min_topic_freq:\n",
    "#             break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "#         print('{:25} {}'.format(topic_names[topic_number],round(freq, 3)))\n",
    "        \n",
    "#         print(topic_names[topic_number])\n",
    "        topics.append(topic_names[topic_number])\n",
    "        freqs.append(round(freq, 3))\n",
    "        \n",
    "    # return topic and freq\n",
    "#     print(topics)\n",
    "#     print(freqs)\n",
    "    return list(zip(topics, freqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_essay = get_sample_essay(0)\n",
    "print(sample_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple = lda_description(sample_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('looking_at_websites_for_school', 0.355),\n",
       " ('spend_time_online_playing_games', 0.223),\n",
       " ('help_kids_learn_about_world', 0.352),\n",
       " ('access_information_at_school', 0.067)]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDFFromTuple(tuple):\n",
    "    return pd.DataFrame({k:v for k,*v in tuple})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = pd.DataFrame(columns=list(topic_names.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>looking_at_websites_for_school</th>\n",
       "      <th>spend_time_online_playing_games</th>\n",
       "      <th>help_kids_learn_about_world</th>\n",
       "      <th>look_for_information</th>\n",
       "      <th>access_information_at_school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [looking_at_websites_for_school, spend_time_online_playing_games, help_kids_learn_about_world, look_for_information, access_information_at_school]\n",
       "Index: []"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "steve = createDFFromTuple(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>looking_at_websites_for_school</th>\n",
       "      <th>spend_time_online_playing_games</th>\n",
       "      <th>help_kids_learn_about_world</th>\n",
       "      <th>access_information_at_school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.355</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   looking_at_websites_for_school  spend_time_online_playing_games  \\\n",
       "0                           0.355                            0.223   \n",
       "\n",
       "   help_kids_learn_about_world  access_information_at_school  \n",
       "0                        0.352                         0.067  "
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left.join(right, Seq(\"firstname\", \"lastname\")).show\n",
    "jo = pd.concat([steve, colNames], sort=True).drop_duplicates().reset_index(drop=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access_information_at_school</th>\n",
       "      <th>help_kids_learn_about_world</th>\n",
       "      <th>look_for_information</th>\n",
       "      <th>looking_at_websites_for_school</th>\n",
       "      <th>spend_time_online_playing_games</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   access_information_at_school  help_kids_learn_about_world  \\\n",
       "0                         0.067                        0.352   \n",
       "\n",
       "   look_for_information  looking_at_websites_for_school  \\\n",
       "0                     0                           0.355   \n",
       "\n",
       "   spend_time_online_playing_games  \n",
       "0                            0.223  "
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>prompt</th>\n",
       "      <th>has_source_material</th>\n",
       "      <th>source_text</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0            NaN            NaN            NaN   \n",
       "\n",
       "                                              prompt  has_source_material  \\\n",
       "0  More and more people use computers, but not ev...                    0   \n",
       "\n",
       "   source_text  grade_7  grade_8  grade_10  \n",
       "0          NaN        0        1         0  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>prompt</th>\n",
       "      <th>has_source_material</th>\n",
       "      <th>source_text</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>More and more people use computers, but not ev...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater3_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater3_trait4  rater3_trait5  rater3_trait6  \\\n",
       "0            NaN            NaN            NaN   \n",
       "\n",
       "                                              prompt  has_source_material  \\\n",
       "0  More and more people use computers, but not ev...                    0   \n",
       "\n",
       "   source_text  grade_7  grade_8  grade_10  \n",
       "0          NaN        0        1         0  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.loc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access_information_at_school</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>essay</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>grade_10</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>has_source_material</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>source_text</th>\n",
       "      <th>spend_time_online_playing_games</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   access_information_at_school  domain1_score  domain2_score  \\\n",
       "0                           NaN            8.0            NaN   \n",
       "0                         0.067            NaN            NaN   \n",
       "\n",
       "                                               essay  essay_id  essay_set  \\\n",
       "0  Dear local newspaper, I think effects computer...       1.0        1.0   \n",
       "0                                                NaN       NaN        NaN   \n",
       "\n",
       "   grade_10  grade_7  grade_8  has_source_material  ...  rater2_trait6  \\\n",
       "0       0.0      0.0      1.0                  0.0  ...            NaN   \n",
       "0       NaN      NaN      NaN                  NaN  ...            NaN   \n",
       "\n",
       "   rater3_domain1  rater3_trait1 rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
       "0             NaN            NaN           NaN            NaN            NaN   \n",
       "0             NaN            NaN           NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait5  rater3_trait6  source_text  spend_time_online_playing_games  \n",
       "0            NaN            NaN          NaN                              NaN  \n",
       "0            NaN            NaN          NaN                            0.223  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.concat([essays.loc[[0]], jo], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n",
    "\n",
    "# word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 training epochs so far.\n",
      "CPU times: user 37.2 s, sys: 496 ms, total: 37.7 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "#     t = time()\n",
    "    # initiate the model and perform 15 epochs of training\n",
    "    # workers should be cores - 1\n",
    "    essay2vec_model = Word2Vec(min_count=20, window=5, size=100, sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20, workers=4)    \n",
    "    essay2vec_model.build_vocab(trigram_sentences)\n",
    "    \n",
    "    for i in range(6):\n",
    "        essay2vec_model.train(trigram_sentences, total_examples=essay2vec_model.corpus_count, epochs=15, report_delay=1)\n",
    "    \n",
    "    essay2vec_model.save(word2vec_filepath)\n",
    "\n",
    "        \n",
    "# load the finished model from disk\n",
    "essay2vec_model = Word2Vec.load(word2vec_filepath)\n",
    "essay2vec_model.init_sims()\n",
    "\n",
    "print('{} training epochs so far.'.format(essay2vec_model.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,250 terms in the essay2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print('{:,} terms in the essay2vec vocabulary.'.format(len(essay2vec_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dear_Local_Newspaper</th>\n",
       "      <td>-0.080966</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.160840</td>\n",
       "      <td>0.153978</td>\n",
       "      <td>-0.150762</td>\n",
       "      <td>-0.227000</td>\n",
       "      <td>-0.119609</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.119844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083057</td>\n",
       "      <td>-0.036303</td>\n",
       "      <td>-0.172746</td>\n",
       "      <td>0.107085</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>-0.054023</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.089317</td>\n",
       "      <td>-0.006336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dear_Newspaper</th>\n",
       "      <td>-0.160894</td>\n",
       "      <td>-0.078554</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.128657</td>\n",
       "      <td>0.114997</td>\n",
       "      <td>-0.129569</td>\n",
       "      <td>-0.273821</td>\n",
       "      <td>-0.217280</td>\n",
       "      <td>-0.019305</td>\n",
       "      <td>0.088784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131236</td>\n",
       "      <td>-0.100623</td>\n",
       "      <td>-0.083030</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>0.176573</td>\n",
       "      <td>-0.096566</td>\n",
       "      <td>-0.069011</td>\n",
       "      <td>0.045533</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>0.079021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr.</th>\n",
       "      <td>-0.048280</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>-0.078311</td>\n",
       "      <td>-0.077604</td>\n",
       "      <td>0.105933</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>-0.245606</td>\n",
       "      <td>-0.125072</td>\n",
       "      <td>0.157281</td>\n",
       "      <td>0.109850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042051</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>-0.073568</td>\n",
       "      <td>0.167597</td>\n",
       "      <td>-0.038416</td>\n",
       "      <td>0.056799</td>\n",
       "      <td>0.049680</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>-0.012547</td>\n",
       "      <td>0.038800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook</th>\n",
       "      <td>0.160234</td>\n",
       "      <td>0.161505</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.027028</td>\n",
       "      <td>0.042157</td>\n",
       "      <td>0.097615</td>\n",
       "      <td>-0.074445</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>-0.013195</td>\n",
       "      <td>0.026856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045397</td>\n",
       "      <td>0.061529</td>\n",
       "      <td>-0.110230</td>\n",
       "      <td>-0.138034</td>\n",
       "      <td>-0.101535</td>\n",
       "      <td>0.122856</td>\n",
       "      <td>0.225491</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>0.037411</td>\n",
       "      <td>-0.037007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>People</th>\n",
       "      <td>-0.054791</td>\n",
       "      <td>0.059666</td>\n",
       "      <td>-0.006936</td>\n",
       "      <td>-0.049464</td>\n",
       "      <td>0.207019</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.008313</td>\n",
       "      <td>0.076224</td>\n",
       "      <td>-0.133590</td>\n",
       "      <td>-0.162718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117228</td>\n",
       "      <td>-0.056509</td>\n",
       "      <td>-0.127031</td>\n",
       "      <td>-0.096924</td>\n",
       "      <td>0.016651</td>\n",
       "      <td>-0.033707</td>\n",
       "      <td>-0.080514</td>\n",
       "      <td>-0.129022</td>\n",
       "      <td>0.033612</td>\n",
       "      <td>-0.022756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>0.061915</td>\n",
       "      <td>0.095871</td>\n",
       "      <td>-0.082138</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.046556</td>\n",
       "      <td>-0.055796</td>\n",
       "      <td>-0.045085</td>\n",
       "      <td>-0.001371</td>\n",
       "      <td>-0.175662</td>\n",
       "      <td>-0.038806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>-0.003393</td>\n",
       "      <td>0.111721</td>\n",
       "      <td>-0.185697</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.260463</td>\n",
       "      <td>0.133326</td>\n",
       "      <td>-0.022905</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>-0.020684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn</th>\n",
       "      <td>0.078620</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>0.106955</td>\n",
       "      <td>-0.152974</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>-0.215835</td>\n",
       "      <td>-0.024729</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>-0.039019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200842</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>-0.149448</td>\n",
       "      <td>-0.010260</td>\n",
       "      <td>-0.103228</td>\n",
       "      <td>0.045211</td>\n",
       "      <td>0.028685</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.066922</td>\n",
       "      <td>0.105161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn_far_away</th>\n",
       "      <td>0.068378</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.083156</td>\n",
       "      <td>-0.106033</td>\n",
       "      <td>0.218467</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>-0.183279</td>\n",
       "      <td>-0.111118</td>\n",
       "      <td>-0.029186</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214203</td>\n",
       "      <td>-0.048395</td>\n",
       "      <td>-0.052231</td>\n",
       "      <td>-0.211182</td>\n",
       "      <td>-0.067038</td>\n",
       "      <td>-0.046495</td>\n",
       "      <td>-0.022695</td>\n",
       "      <td>-0.043916</td>\n",
       "      <td>-0.026545</td>\n",
       "      <td>0.121319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability_learn_faraway_place</th>\n",
       "      <td>-0.043237</td>\n",
       "      <td>-0.050755</td>\n",
       "      <td>0.093305</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.205478</td>\n",
       "      <td>-0.025085</td>\n",
       "      <td>-0.212736</td>\n",
       "      <td>-0.158275</td>\n",
       "      <td>-0.032956</td>\n",
       "      <td>0.036593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275119</td>\n",
       "      <td>-0.010331</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>-0.158186</td>\n",
       "      <td>-0.004436</td>\n",
       "      <td>-0.042032</td>\n",
       "      <td>0.026861</td>\n",
       "      <td>-0.012597</td>\n",
       "      <td>0.061112</td>\n",
       "      <td>0.114431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.023570</td>\n",
       "      <td>-0.053252</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>-0.085435</td>\n",
       "      <td>0.164259</td>\n",
       "      <td>-0.058869</td>\n",
       "      <td>0.080843</td>\n",
       "      <td>0.106354</td>\n",
       "      <td>-0.153176</td>\n",
       "      <td>-0.034961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>-0.001847</td>\n",
       "      <td>-0.063932</td>\n",
       "      <td>-0.096556</td>\n",
       "      <td>-0.110397</td>\n",
       "      <td>0.145599</td>\n",
       "      <td>0.058895</td>\n",
       "      <td>-0.044561</td>\n",
       "      <td>0.179839</td>\n",
       "      <td>-0.101470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.009471</td>\n",
       "      <td>-0.037858</td>\n",
       "      <td>0.118418</td>\n",
       "      <td>0.268994</td>\n",
       "      <td>0.313056</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>-0.136287</td>\n",
       "      <td>-0.139082</td>\n",
       "      <td>-0.000933</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134323</td>\n",
       "      <td>-0.086783</td>\n",
       "      <td>-0.121617</td>\n",
       "      <td>-0.086920</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>-0.044351</td>\n",
       "      <td>-0.079989</td>\n",
       "      <td>0.007832</td>\n",
       "      <td>-0.135252</td>\n",
       "      <td>-0.084513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>-0.060129</td>\n",
       "      <td>0.100730</td>\n",
       "      <td>0.098544</td>\n",
       "      <td>0.117001</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.050971</td>\n",
       "      <td>0.028348</td>\n",
       "      <td>-0.110445</td>\n",
       "      <td>-0.042671</td>\n",
       "      <td>0.020960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138893</td>\n",
       "      <td>-0.028911</td>\n",
       "      <td>-0.187556</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>-0.081402</td>\n",
       "      <td>-0.078189</td>\n",
       "      <td>-0.054812</td>\n",
       "      <td>-0.072359</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.008121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>-0.016774</td>\n",
       "      <td>0.015856</td>\n",
       "      <td>-0.144403</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>-0.151972</td>\n",
       "      <td>-0.002618</td>\n",
       "      <td>-0.021094</td>\n",
       "      <td>-0.106052</td>\n",
       "      <td>-0.072616</td>\n",
       "      <td>0.058271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138290</td>\n",
       "      <td>0.049777</td>\n",
       "      <td>-0.109659</td>\n",
       "      <td>-0.086719</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.214862</td>\n",
       "      <td>0.108611</td>\n",
       "      <td>-0.051788</td>\n",
       "      <td>-0.025838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.023603</td>\n",
       "      <td>0.048625</td>\n",
       "      <td>-0.031065</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.164677</td>\n",
       "      <td>0.130134</td>\n",
       "      <td>-0.074918</td>\n",
       "      <td>-0.004850</td>\n",
       "      <td>0.192992</td>\n",
       "      <td>0.155685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029439</td>\n",
       "      <td>-0.012145</td>\n",
       "      <td>-0.067884</td>\n",
       "      <td>-0.066159</td>\n",
       "      <td>-0.064794</td>\n",
       "      <td>0.077432</td>\n",
       "      <td>0.110461</td>\n",
       "      <td>-0.009767</td>\n",
       "      <td>-0.051933</td>\n",
       "      <td>0.118402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account</th>\n",
       "      <td>-0.034982</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>-0.047256</td>\n",
       "      <td>-0.047565</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.026578</td>\n",
       "      <td>-0.001665</td>\n",
       "      <td>-0.015990</td>\n",
       "      <td>-0.091662</td>\n",
       "      <td>0.074502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081644</td>\n",
       "      <td>-0.045451</td>\n",
       "      <td>-0.062248</td>\n",
       "      <td>-0.130844</td>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>0.112684</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>-0.031739</td>\n",
       "      <td>-0.023910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accurate</th>\n",
       "      <td>0.086073</td>\n",
       "      <td>-0.032643</td>\n",
       "      <td>0.139284</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>-0.101803</td>\n",
       "      <td>-0.074570</td>\n",
       "      <td>-0.103560</td>\n",
       "      <td>0.101796</td>\n",
       "      <td>-0.118272</td>\n",
       "      <td>-0.060125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.004177</td>\n",
       "      <td>-0.047235</td>\n",
       "      <td>0.125650</td>\n",
       "      <td>0.033922</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.088164</td>\n",
       "      <td>0.177889</td>\n",
       "      <td>-0.017273</td>\n",
       "      <td>-0.106321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acess</th>\n",
       "      <td>0.080051</td>\n",
       "      <td>-0.026162</td>\n",
       "      <td>-0.011615</td>\n",
       "      <td>-0.231321</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.039610</td>\n",
       "      <td>-0.229335</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.050250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139043</td>\n",
       "      <td>0.123306</td>\n",
       "      <td>-0.073263</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.102984</td>\n",
       "      <td>0.055651</td>\n",
       "      <td>-0.032433</td>\n",
       "      <td>-0.082574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>-0.249494</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>-0.168136</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>0.136604</td>\n",
       "      <td>0.054115</td>\n",
       "      <td>-0.005799</td>\n",
       "      <td>0.041054</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067933</td>\n",
       "      <td>0.086227</td>\n",
       "      <td>0.030296</td>\n",
       "      <td>0.039736</td>\n",
       "      <td>0.058830</td>\n",
       "      <td>0.093552</td>\n",
       "      <td>0.140933</td>\n",
       "      <td>-0.012769</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>-0.048643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>-0.099725</td>\n",
       "      <td>0.280859</td>\n",
       "      <td>0.035305</td>\n",
       "      <td>0.055052</td>\n",
       "      <td>0.066803</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>-0.038965</td>\n",
       "      <td>-0.197699</td>\n",
       "      <td>0.072265</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143444</td>\n",
       "      <td>0.135385</td>\n",
       "      <td>-0.052328</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>-0.082166</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.094843</td>\n",
       "      <td>0.064940</td>\n",
       "      <td>0.072419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active</th>\n",
       "      <td>-0.002011</td>\n",
       "      <td>0.124149</td>\n",
       "      <td>-0.012404</td>\n",
       "      <td>0.076643</td>\n",
       "      <td>0.300641</td>\n",
       "      <td>0.060930</td>\n",
       "      <td>0.082660</td>\n",
       "      <td>-0.099111</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.088329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025298</td>\n",
       "      <td>-0.188665</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>-0.121720</td>\n",
       "      <td>-0.019516</td>\n",
       "      <td>-0.057793</td>\n",
       "      <td>-0.134708</td>\n",
       "      <td>-0.021576</td>\n",
       "      <td>0.065409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <td>-0.014207</td>\n",
       "      <td>-0.051895</td>\n",
       "      <td>-0.065365</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>0.242847</td>\n",
       "      <td>-0.060283</td>\n",
       "      <td>-0.081119</td>\n",
       "      <td>-0.153714</td>\n",
       "      <td>-0.105502</td>\n",
       "      <td>0.068179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088216</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>0.063110</td>\n",
       "      <td>-0.042854</td>\n",
       "      <td>-0.248585</td>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.140672</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>-0.057184</td>\n",
       "      <td>-0.010675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actually</th>\n",
       "      <td>-0.124487</td>\n",
       "      <td>0.123299</td>\n",
       "      <td>-0.047490</td>\n",
       "      <td>0.076760</td>\n",
       "      <td>0.122557</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>-0.086286</td>\n",
       "      <td>-0.007715</td>\n",
       "      <td>-0.100940</td>\n",
       "      <td>-0.014237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090606</td>\n",
       "      <td>-0.065074</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.023920</td>\n",
       "      <td>0.155269</td>\n",
       "      <td>0.134975</td>\n",
       "      <td>0.084377</td>\n",
       "      <td>-0.108588</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>-0.051784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <td>-0.052581</td>\n",
       "      <td>0.090988</td>\n",
       "      <td>0.119070</td>\n",
       "      <td>0.018722</td>\n",
       "      <td>0.055564</td>\n",
       "      <td>0.043385</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>-0.114068</td>\n",
       "      <td>0.162840</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065438</td>\n",
       "      <td>-0.074953</td>\n",
       "      <td>0.056090</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>-0.053827</td>\n",
       "      <td>-0.055392</td>\n",
       "      <td>0.041043</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>-0.047681</td>\n",
       "      <td>-0.048392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>add</th>\n",
       "      <td>-0.185433</td>\n",
       "      <td>-0.083624</td>\n",
       "      <td>-0.036671</td>\n",
       "      <td>-0.040051</td>\n",
       "      <td>0.042287</td>\n",
       "      <td>0.065492</td>\n",
       "      <td>-0.015215</td>\n",
       "      <td>0.014397</td>\n",
       "      <td>-0.114772</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004022</td>\n",
       "      <td>-0.200743</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.147879</td>\n",
       "      <td>-0.054279</td>\n",
       "      <td>0.063821</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.072677</td>\n",
       "      <td>-0.126859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addict</th>\n",
       "      <td>-0.022299</td>\n",
       "      <td>0.191387</td>\n",
       "      <td>-0.074030</td>\n",
       "      <td>-0.055573</td>\n",
       "      <td>0.310384</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>-0.085369</td>\n",
       "      <td>-0.100936</td>\n",
       "      <td>0.161640</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189984</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>-0.100763</td>\n",
       "      <td>0.062804</td>\n",
       "      <td>-0.072491</td>\n",
       "      <td>-0.054097</td>\n",
       "      <td>-0.050763</td>\n",
       "      <td>-0.024606</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.021930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addicted</th>\n",
       "      <td>-0.180987</td>\n",
       "      <td>0.108654</td>\n",
       "      <td>-0.209175</td>\n",
       "      <td>0.048430</td>\n",
       "      <td>0.220865</td>\n",
       "      <td>-0.021063</td>\n",
       "      <td>-0.116353</td>\n",
       "      <td>-0.085872</td>\n",
       "      <td>-0.082910</td>\n",
       "      <td>-0.066901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121385</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>-0.041007</td>\n",
       "      <td>0.033138</td>\n",
       "      <td>-0.053710</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>-0.151991</td>\n",
       "      <td>-0.012026</td>\n",
       "      <td>-0.053500</td>\n",
       "      <td>0.047624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addicting</th>\n",
       "      <td>0.124235</td>\n",
       "      <td>0.220020</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.060788</td>\n",
       "      <td>0.151284</td>\n",
       "      <td>0.075862</td>\n",
       "      <td>-0.154718</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>-0.035943</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020696</td>\n",
       "      <td>0.044556</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>-0.120047</td>\n",
       "      <td>0.072648</td>\n",
       "      <td>-0.037826</td>\n",
       "      <td>0.137466</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>-0.072847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addiction</th>\n",
       "      <td>-0.041202</td>\n",
       "      <td>0.211525</td>\n",
       "      <td>-0.219157</td>\n",
       "      <td>0.047944</td>\n",
       "      <td>0.194943</td>\n",
       "      <td>-0.022224</td>\n",
       "      <td>-0.121570</td>\n",
       "      <td>-0.164046</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.037053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180623</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>-0.135802</td>\n",
       "      <td>-0.037686</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>-0.016029</td>\n",
       "      <td>-0.183707</td>\n",
       "      <td>0.182267</td>\n",
       "      <td>-0.015915</td>\n",
       "      <td>0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addictive</th>\n",
       "      <td>-0.135284</td>\n",
       "      <td>0.265247</td>\n",
       "      <td>-0.142602</td>\n",
       "      <td>0.277454</td>\n",
       "      <td>0.203590</td>\n",
       "      <td>0.044446</td>\n",
       "      <td>-0.005216</td>\n",
       "      <td>-0.042800</td>\n",
       "      <td>0.039721</td>\n",
       "      <td>0.108935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034896</td>\n",
       "      <td>-0.014617</td>\n",
       "      <td>-0.072891</td>\n",
       "      <td>-0.068128</td>\n",
       "      <td>-0.142953</td>\n",
       "      <td>-0.042299</td>\n",
       "      <td>-0.033439</td>\n",
       "      <td>0.133010</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>0.158381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addition</th>\n",
       "      <td>0.067104</td>\n",
       "      <td>0.186199</td>\n",
       "      <td>-0.142296</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.023728</td>\n",
       "      <td>-0.247746</td>\n",
       "      <td>-0.159316</td>\n",
       "      <td>-0.016654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006716</td>\n",
       "      <td>-0.130061</td>\n",
       "      <td>0.046863</td>\n",
       "      <td>0.101913</td>\n",
       "      <td>0.006348</td>\n",
       "      <td>-0.054943</td>\n",
       "      <td>0.125698</td>\n",
       "      <td>0.100385</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>-0.031337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>-0.210274</td>\n",
       "      <td>0.107353</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>0.136388</td>\n",
       "      <td>0.195531</td>\n",
       "      <td>0.053409</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.109844</td>\n",
       "      <td>-0.016581</td>\n",
       "      <td>-0.047279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108592</td>\n",
       "      <td>0.088655</td>\n",
       "      <td>-0.043294</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>-0.045943</td>\n",
       "      <td>-0.083427</td>\n",
       "      <td>-0.278405</td>\n",
       "      <td>-0.181813</td>\n",
       "      <td>0.149525</td>\n",
       "      <td>0.186288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will_not</th>\n",
       "      <td>-0.129235</td>\n",
       "      <td>0.015577</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.126605</td>\n",
       "      <td>0.071264</td>\n",
       "      <td>0.038460</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.035405</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.108512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039451</td>\n",
       "      <td>0.173878</td>\n",
       "      <td>0.027983</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>-0.006654</td>\n",
       "      <td>-0.099778</td>\n",
       "      <td>-0.146238</td>\n",
       "      <td>-0.187370</td>\n",
       "      <td>-0.020311</td>\n",
       "      <td>-0.008023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>win</th>\n",
       "      <td>-0.065997</td>\n",
       "      <td>0.051860</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>-0.128991</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.201266</td>\n",
       "      <td>-0.089693</td>\n",
       "      <td>0.198117</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057563</td>\n",
       "      <td>0.220913</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>-0.068581</td>\n",
       "      <td>-0.105390</td>\n",
       "      <td>-0.068331</td>\n",
       "      <td>0.083326</td>\n",
       "      <td>0.055815</td>\n",
       "      <td>-0.045385</td>\n",
       "      <td>-0.174301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window</th>\n",
       "      <td>-0.124794</td>\n",
       "      <td>0.047326</td>\n",
       "      <td>-0.135098</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.024452</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>0.103574</td>\n",
       "      <td>-0.065450</td>\n",
       "      <td>-0.160271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157480</td>\n",
       "      <td>0.075503</td>\n",
       "      <td>0.128558</td>\n",
       "      <td>-0.207491</td>\n",
       "      <td>0.143063</td>\n",
       "      <td>0.048720</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>0.151653</td>\n",
       "      <td>0.118451</td>\n",
       "      <td>-0.048937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>0.157742</td>\n",
       "      <td>0.211656</td>\n",
       "      <td>0.020748</td>\n",
       "      <td>-0.136406</td>\n",
       "      <td>0.062554</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.095738</td>\n",
       "      <td>0.090334</td>\n",
       "      <td>-0.082855</td>\n",
       "      <td>-0.145515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046139</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>-0.134025</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>0.025053</td>\n",
       "      <td>-0.010894</td>\n",
       "      <td>-0.032358</td>\n",
       "      <td>0.013457</td>\n",
       "      <td>0.069877</td>\n",
       "      <td>0.100498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>-0.036164</td>\n",
       "      <td>0.063715</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.248363</td>\n",
       "      <td>-0.063726</td>\n",
       "      <td>-0.038604</td>\n",
       "      <td>0.141808</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>-0.121845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006780</td>\n",
       "      <td>0.145184</td>\n",
       "      <td>0.071114</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.111052</td>\n",
       "      <td>-0.197036</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.042903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.110637</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.036366</td>\n",
       "      <td>-0.035389</td>\n",
       "      <td>-0.121819</td>\n",
       "      <td>-0.019157</td>\n",
       "      <td>0.108040</td>\n",
       "      <td>-0.199105</td>\n",
       "      <td>-0.103509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032440</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>0.142903</td>\n",
       "      <td>-0.063284</td>\n",
       "      <td>0.077990</td>\n",
       "      <td>-0.083893</td>\n",
       "      <td>0.029566</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.029312</td>\n",
       "      <td>0.077423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>-0.045878</td>\n",
       "      <td>-0.054326</td>\n",
       "      <td>0.106562</td>\n",
       "      <td>0.250029</td>\n",
       "      <td>0.028261</td>\n",
       "      <td>-0.076788</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>-0.094184</td>\n",
       "      <td>0.037543</td>\n",
       "      <td>0.172067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>-0.045535</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>-0.041055</td>\n",
       "      <td>0.293838</td>\n",
       "      <td>-0.082126</td>\n",
       "      <td>-0.001794</td>\n",
       "      <td>0.103639</td>\n",
       "      <td>-0.033845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.116637</td>\n",
       "      <td>-0.073446</td>\n",
       "      <td>-0.211282</td>\n",
       "      <td>-0.112790</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.096693</td>\n",
       "      <td>0.166608</td>\n",
       "      <td>-0.013387</td>\n",
       "      <td>-0.154851</td>\n",
       "      <td>-0.034203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>-0.172235</td>\n",
       "      <td>-0.083438</td>\n",
       "      <td>-0.119303</td>\n",
       "      <td>-0.044952</td>\n",
       "      <td>0.023880</td>\n",
       "      <td>-0.010212</td>\n",
       "      <td>-0.037849</td>\n",
       "      <td>-0.073563</td>\n",
       "      <td>-0.151407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worker</th>\n",
       "      <td>-0.022504</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>-0.090817</td>\n",
       "      <td>-0.057094</td>\n",
       "      <td>-0.108500</td>\n",
       "      <td>0.057872</td>\n",
       "      <td>0.029849</td>\n",
       "      <td>-0.098129</td>\n",
       "      <td>0.028791</td>\n",
       "      <td>-0.088584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>-0.220155</td>\n",
       "      <td>-0.013423</td>\n",
       "      <td>-0.171200</td>\n",
       "      <td>0.163627</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>0.070564</td>\n",
       "      <td>-0.101153</td>\n",
       "      <td>-0.005687</td>\n",
       "      <td>0.089583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>-0.031874</td>\n",
       "      <td>0.139463</td>\n",
       "      <td>-0.143214</td>\n",
       "      <td>-0.134749</td>\n",
       "      <td>-0.035267</td>\n",
       "      <td>-0.100296</td>\n",
       "      <td>-0.087241</td>\n",
       "      <td>0.060740</td>\n",
       "      <td>-0.058429</td>\n",
       "      <td>-0.146486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228013</td>\n",
       "      <td>-0.096753</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.044096</td>\n",
       "      <td>0.204079</td>\n",
       "      <td>-0.045788</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>-0.024602</td>\n",
       "      <td>-0.044594</td>\n",
       "      <td>0.056458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worried</th>\n",
       "      <td>-0.149530</td>\n",
       "      <td>0.066812</td>\n",
       "      <td>-0.116020</td>\n",
       "      <td>0.057878</td>\n",
       "      <td>0.186423</td>\n",
       "      <td>0.031979</td>\n",
       "      <td>-0.065942</td>\n",
       "      <td>-0.130018</td>\n",
       "      <td>-0.005233</td>\n",
       "      <td>-0.087113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092626</td>\n",
       "      <td>-0.053344</td>\n",
       "      <td>-0.051258</td>\n",
       "      <td>0.146055</td>\n",
       "      <td>-0.053174</td>\n",
       "      <td>-0.227502</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.078444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worry</th>\n",
       "      <td>-0.115249</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>-0.144226</td>\n",
       "      <td>0.076544</td>\n",
       "      <td>0.149391</td>\n",
       "      <td>-0.062495</td>\n",
       "      <td>0.074391</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.009033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173196</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.086681</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>-0.174869</td>\n",
       "      <td>0.051112</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>0.095718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0.066123</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.183848</td>\n",
       "      <td>-0.068840</td>\n",
       "      <td>0.141505</td>\n",
       "      <td>-0.098121</td>\n",
       "      <td>-0.058005</td>\n",
       "      <td>-0.064479</td>\n",
       "      <td>0.136190</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104596</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>-0.104555</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.083674</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>-0.317687</td>\n",
       "      <td>-0.199293</td>\n",
       "      <td>-0.058497</td>\n",
       "      <td>-0.112080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>-0.129512</td>\n",
       "      <td>-0.066839</td>\n",
       "      <td>-0.048920</td>\n",
       "      <td>0.171823</td>\n",
       "      <td>-0.177231</td>\n",
       "      <td>-0.010816</td>\n",
       "      <td>-0.117696</td>\n",
       "      <td>0.048246</td>\n",
       "      <td>-0.043790</td>\n",
       "      <td>0.162369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081206</td>\n",
       "      <td>-0.146629</td>\n",
       "      <td>0.106912</td>\n",
       "      <td>-0.111590</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>0.156758</td>\n",
       "      <td>-0.080771</td>\n",
       "      <td>-0.085659</td>\n",
       "      <td>0.063928</td>\n",
       "      <td>-0.056529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_essay</th>\n",
       "      <td>-0.276980</td>\n",
       "      <td>-0.092888</td>\n",
       "      <td>0.029463</td>\n",
       "      <td>-0.055676</td>\n",
       "      <td>-0.178226</td>\n",
       "      <td>0.107930</td>\n",
       "      <td>-0.072077</td>\n",
       "      <td>0.043175</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>0.241753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096314</td>\n",
       "      <td>-0.044447</td>\n",
       "      <td>0.018310</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>-0.042781</td>\n",
       "      <td>0.205939</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.039154</td>\n",
       "      <td>0.167516</td>\n",
       "      <td>-0.085724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_letter</th>\n",
       "      <td>-0.094048</td>\n",
       "      <td>-0.188189</td>\n",
       "      <td>0.183612</td>\n",
       "      <td>-0.083681</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>-0.073572</td>\n",
       "      <td>-0.097978</td>\n",
       "      <td>0.031064</td>\n",
       "      <td>0.052986</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122145</td>\n",
       "      <td>-0.225296</td>\n",
       "      <td>-0.089557</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.155174</td>\n",
       "      <td>0.060816</td>\n",
       "      <td>-0.104516</td>\n",
       "      <td>0.033624</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>-0.053705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write_paper</th>\n",
       "      <td>-0.039994</td>\n",
       "      <td>-0.162954</td>\n",
       "      <td>0.073475</td>\n",
       "      <td>0.183336</td>\n",
       "      <td>-0.065429</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>-0.081525</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.197878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022393</td>\n",
       "      <td>-0.094506</td>\n",
       "      <td>0.112994</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>-0.064076</td>\n",
       "      <td>0.253173</td>\n",
       "      <td>0.083666</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.068730</td>\n",
       "      <td>-0.072751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>0.003102</td>\n",
       "      <td>-0.065624</td>\n",
       "      <td>0.116759</td>\n",
       "      <td>0.221152</td>\n",
       "      <td>-0.092194</td>\n",
       "      <td>-0.004935</td>\n",
       "      <td>0.056955</td>\n",
       "      <td>0.010158</td>\n",
       "      <td>0.036182</td>\n",
       "      <td>0.262661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052755</td>\n",
       "      <td>-0.107070</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>-0.044422</td>\n",
       "      <td>-0.146656</td>\n",
       "      <td>0.136562</td>\n",
       "      <td>-0.155263</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>-0.121560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong</th>\n",
       "      <td>-0.159924</td>\n",
       "      <td>-0.071432</td>\n",
       "      <td>0.066980</td>\n",
       "      <td>0.063522</td>\n",
       "      <td>0.202766</td>\n",
       "      <td>-0.027204</td>\n",
       "      <td>-0.115663</td>\n",
       "      <td>-0.068590</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.168906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037976</td>\n",
       "      <td>0.031521</td>\n",
       "      <td>0.092895</td>\n",
       "      <td>0.315482</td>\n",
       "      <td>0.080589</td>\n",
       "      <td>-0.107264</td>\n",
       "      <td>0.047725</td>\n",
       "      <td>0.101677</td>\n",
       "      <td>-0.060642</td>\n",
       "      <td>0.067841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo</th>\n",
       "      <td>0.118643</td>\n",
       "      <td>0.078907</td>\n",
       "      <td>0.077289</td>\n",
       "      <td>0.124349</td>\n",
       "      <td>0.050914</td>\n",
       "      <td>-0.077911</td>\n",
       "      <td>-0.201298</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.066481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080930</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>-0.174040</td>\n",
       "      <td>0.058013</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>0.105651</td>\n",
       "      <td>0.076351</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>-0.025008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>-0.084354</td>\n",
       "      <td>0.051403</td>\n",
       "      <td>-0.092265</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>0.095282</td>\n",
       "      <td>0.027088</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>0.102618</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016205</td>\n",
       "      <td>0.062934</td>\n",
       "      <td>-0.077875</td>\n",
       "      <td>0.093595</td>\n",
       "      <td>-0.052626</td>\n",
       "      <td>0.079668</td>\n",
       "      <td>-0.189148</td>\n",
       "      <td>-0.176204</td>\n",
       "      <td>0.061498</td>\n",
       "      <td>-0.014047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_ago</th>\n",
       "      <td>0.143823</td>\n",
       "      <td>0.146433</td>\n",
       "      <td>0.056964</td>\n",
       "      <td>-0.035602</td>\n",
       "      <td>0.032863</td>\n",
       "      <td>-0.061437</td>\n",
       "      <td>-0.182808</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>-0.046499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061817</td>\n",
       "      <td>0.123939</td>\n",
       "      <td>-0.093537</td>\n",
       "      <td>0.038132</td>\n",
       "      <td>-0.105527</td>\n",
       "      <td>-0.060461</td>\n",
       "      <td>0.151425</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>-0.035396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year_old</th>\n",
       "      <td>-0.265906</td>\n",
       "      <td>0.137993</td>\n",
       "      <td>0.102887</td>\n",
       "      <td>-0.057300</td>\n",
       "      <td>0.240518</td>\n",
       "      <td>0.011261</td>\n",
       "      <td>-0.163350</td>\n",
       "      <td>0.102529</td>\n",
       "      <td>0.084942</td>\n",
       "      <td>-0.083565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021522</td>\n",
       "      <td>0.070995</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>-0.011797</td>\n",
       "      <td>-0.099303</td>\n",
       "      <td>-0.123955</td>\n",
       "      <td>-0.019513</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.006188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yes</th>\n",
       "      <td>-0.113401</td>\n",
       "      <td>-0.039949</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.141987</td>\n",
       "      <td>0.136337</td>\n",
       "      <td>-0.221333</td>\n",
       "      <td>-0.167595</td>\n",
       "      <td>-0.077956</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.090622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025857</td>\n",
       "      <td>-0.018503</td>\n",
       "      <td>-0.081860</td>\n",
       "      <td>0.103133</td>\n",
       "      <td>0.075920</td>\n",
       "      <td>-0.016118</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-0.014454</td>\n",
       "      <td>-0.042515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>-0.090288</td>\n",
       "      <td>0.205408</td>\n",
       "      <td>0.083452</td>\n",
       "      <td>-0.021273</td>\n",
       "      <td>0.105388</td>\n",
       "      <td>0.089513</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.122347</td>\n",
       "      <td>0.026380</td>\n",
       "      <td>-0.106663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.205986</td>\n",
       "      <td>0.165020</td>\n",
       "      <td>-0.191593</td>\n",
       "      <td>-0.152620</td>\n",
       "      <td>0.069355</td>\n",
       "      <td>0.092479</td>\n",
       "      <td>0.042936</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>-0.003103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young_child</th>\n",
       "      <td>-0.088664</td>\n",
       "      <td>0.103843</td>\n",
       "      <td>0.080633</td>\n",
       "      <td>0.131359</td>\n",
       "      <td>0.181052</td>\n",
       "      <td>-0.050389</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>-0.133385</td>\n",
       "      <td>-0.011312</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087158</td>\n",
       "      <td>0.076534</td>\n",
       "      <td>-0.033679</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-0.271744</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>-0.021692</td>\n",
       "      <td>0.116293</td>\n",
       "      <td>-0.151703</td>\n",
       "      <td>-0.009119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>-0.011421</td>\n",
       "      <td>0.034357</td>\n",
       "      <td>-0.077571</td>\n",
       "      <td>-0.061826</td>\n",
       "      <td>-0.050209</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>-0.023727</td>\n",
       "      <td>-0.232815</td>\n",
       "      <td>0.234117</td>\n",
       "      <td>0.036047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>-0.023236</td>\n",
       "      <td>-0.018881</td>\n",
       "      <td>0.106277</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>0.147399</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.256224</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.011791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>-0.039809</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.109364</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.127894</td>\n",
       "      <td>-0.047535</td>\n",
       "      <td>-0.050082</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.067090</td>\n",
       "      <td>-0.066447</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053343</td>\n",
       "      <td>-0.015251</td>\n",
       "      <td>-0.038807</td>\n",
       "      <td>-0.086042</td>\n",
       "      <td>-0.101111</td>\n",
       "      <td>-0.011986</td>\n",
       "      <td>0.064623</td>\n",
       "      <td>0.043936</td>\n",
       "      <td>-0.041173</td>\n",
       "      <td>-0.009698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’</th>\n",
       "      <td>-0.140483</td>\n",
       "      <td>0.123493</td>\n",
       "      <td>0.016328</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>-0.166562</td>\n",
       "      <td>-0.131816</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>-0.027659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.092527</td>\n",
       "      <td>-0.125255</td>\n",
       "      <td>0.232657</td>\n",
       "      <td>0.127279</td>\n",
       "      <td>-0.003595</td>\n",
       "      <td>-0.043506</td>\n",
       "      <td>-0.029938</td>\n",
       "      <td>-0.100074</td>\n",
       "      <td>0.238505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1         2         3         4   \\\n",
       "Dear_Local_Newspaper        -0.080966  0.055885  0.006367  0.160840  0.153978   \n",
       "Dear_Newspaper              -0.160894 -0.078554 -0.005497  0.128657  0.114997   \n",
       "Dr.                         -0.048280  0.085749 -0.078311 -0.077604  0.105933   \n",
       "Facebook                     0.160234  0.161505 -0.004742  0.027028  0.042157   \n",
       "People                      -0.054791  0.059666 -0.006936 -0.049464  0.207019   \n",
       "ability                      0.061915  0.095871 -0.082138  0.038366  0.046556   \n",
       "ability_learn                0.078620 -0.060835  0.106955 -0.152974  0.055909   \n",
       "ability_learn_far_away       0.068378  0.036706  0.083156 -0.106033  0.218467   \n",
       "ability_learn_faraway_place -0.043237 -0.050755  0.093305  0.010634  0.205478   \n",
       "able                         0.023570 -0.053252  0.063354 -0.085435  0.164259   \n",
       "absolutely                   0.009471 -0.037858  0.118418  0.268994  0.313056   \n",
       "abuse                       -0.060129  0.100730  0.098544  0.117001  0.130719   \n",
       "access                      -0.016774  0.015856 -0.144403 -0.247768 -0.151972   \n",
       "accord                       0.023603  0.048625 -0.031065 -0.005102  0.164677   \n",
       "account                     -0.034982  0.061856 -0.047256 -0.047565  0.001725   \n",
       "accurate                     0.086073 -0.032643  0.139284  0.028096 -0.101803   \n",
       "acess                        0.080051 -0.026162 -0.011615 -0.231321  0.000147   \n",
       "act                         -0.249494  0.017735 -0.168136  0.125373  0.117684   \n",
       "action                      -0.099725  0.280859  0.035305  0.055052  0.066803   \n",
       "active                      -0.002011  0.124149 -0.012404  0.076643  0.300641   \n",
       "activity                    -0.014207 -0.051895 -0.065365  0.077290  0.242847   \n",
       "actually                    -0.124487  0.123299 -0.047490  0.076760  0.122557   \n",
       "ad                          -0.052581  0.090988  0.119070  0.018722  0.055564   \n",
       "add                         -0.185433 -0.083624 -0.036671 -0.040051  0.042287   \n",
       "addict                      -0.022299  0.191387 -0.074030 -0.055573  0.310384   \n",
       "addicted                    -0.180987  0.108654 -0.209175  0.048430  0.220865   \n",
       "addicting                    0.124235  0.220020  0.008297  0.060788  0.151284   \n",
       "addiction                   -0.041202  0.211525 -0.219157  0.047944  0.194943   \n",
       "addictive                   -0.135284  0.265247 -0.142602  0.277454  0.203590   \n",
       "addition                     0.067104  0.186199 -0.142296  0.034541 -0.001580   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                        -0.210274  0.107353  0.011885  0.136388  0.195531   \n",
       "will_not                    -0.129235  0.015577  0.009054  0.126605  0.071264   \n",
       "win                         -0.065997  0.051860  0.036722 -0.128991  0.104487   \n",
       "window                      -0.124794  0.047326 -0.135098  0.049944  0.008427   \n",
       "wish                         0.157742  0.211656  0.020748 -0.136406  0.062554   \n",
       "wonder                      -0.036164  0.063715  0.017805 -0.000229  0.248363   \n",
       "wonderful                    0.200375  0.110637  0.073242 -0.036366 -0.035389   \n",
       "word                        -0.045878 -0.054326  0.106562  0.250029  0.028261   \n",
       "work                        -0.116637 -0.073446 -0.211282 -0.112790  0.074468   \n",
       "worker                      -0.022504  0.003409 -0.090817 -0.057094 -0.108500   \n",
       "world                       -0.031874  0.139463 -0.143214 -0.134749 -0.035267   \n",
       "worried                     -0.149530  0.066812 -0.116020  0.057878  0.186423   \n",
       "worry                       -0.115249  0.006581  0.003609 -0.144226  0.076544   \n",
       "worth                        0.066123  0.025590  0.183848 -0.068840  0.141505   \n",
       "write                       -0.129512 -0.066839 -0.048920  0.171823 -0.177231   \n",
       "write_essay                 -0.276980 -0.092888  0.029463 -0.055676 -0.178226   \n",
       "write_letter                -0.094048 -0.188189  0.183612 -0.083681  0.003714   \n",
       "write_paper                 -0.039994 -0.162954  0.073475  0.183336 -0.065429   \n",
       "writing                      0.003102 -0.065624  0.116759  0.221152 -0.092194   \n",
       "wrong                       -0.159924 -0.071432  0.066980  0.063522  0.202766   \n",
       "yahoo                        0.118643  0.078907  0.077289  0.124349  0.050914   \n",
       "year                        -0.084354  0.051403 -0.092265  0.017354  0.049213   \n",
       "year_ago                     0.143823  0.146433  0.056964 -0.035602  0.032863   \n",
       "year_old                    -0.265906  0.137993  0.102887 -0.057300  0.240518   \n",
       "yes                         -0.113401 -0.039949  0.003711  0.141987  0.136337   \n",
       "young                       -0.090288  0.205408  0.083452 -0.021273  0.105388   \n",
       "young_child                 -0.088664  0.103843  0.080633  0.131359  0.181052   \n",
       "youth                       -0.011421  0.034357 -0.077571 -0.061826 -0.050209   \n",
       "youtube                     -0.039809  0.143098  0.109364  0.029712  0.127894   \n",
       "’                           -0.140483  0.123493  0.016328  0.021604  0.155623   \n",
       "\n",
       "                                   5         6         7         8         9   \\\n",
       "Dear_Local_Newspaper        -0.150762 -0.227000 -0.119609  0.028311  0.119844   \n",
       "Dear_Newspaper              -0.129569 -0.273821 -0.217280 -0.019305  0.088784   \n",
       "Dr.                          0.043426 -0.245606 -0.125072  0.157281  0.109850   \n",
       "Facebook                     0.097615 -0.074445  0.072042 -0.013195  0.026856   \n",
       "People                       0.001976  0.008313  0.076224 -0.133590 -0.162718   \n",
       "ability                     -0.055796 -0.045085 -0.001371 -0.175662 -0.038806   \n",
       "ability_learn                0.004751 -0.215835 -0.024729  0.032057 -0.039019   \n",
       "ability_learn_far_away       0.031863 -0.183279 -0.111118 -0.029186  0.013766   \n",
       "ability_learn_faraway_place -0.025085 -0.212736 -0.158275 -0.032956  0.036593   \n",
       "able                        -0.058869  0.080843  0.106354 -0.153176 -0.034961   \n",
       "absolutely                   0.026245 -0.136287 -0.139082 -0.000933  0.011531   \n",
       "abuse                        0.050971  0.028348 -0.110445 -0.042671  0.020960   \n",
       "access                      -0.002618 -0.021094 -0.106052 -0.072616  0.058271   \n",
       "accord                       0.130134 -0.074918 -0.004850  0.192992  0.155685   \n",
       "account                      0.026578 -0.001665 -0.015990 -0.091662  0.074502   \n",
       "accurate                    -0.074570 -0.103560  0.101796 -0.118272 -0.060125   \n",
       "acess                        0.039610 -0.229335 -0.029583  0.013439  0.050250   \n",
       "act                          0.136604  0.054115 -0.005799  0.041054 -0.051953   \n",
       "action                       0.053238 -0.038965 -0.197699  0.072265 -0.003704   \n",
       "active                       0.060930  0.082660 -0.099111  0.100015  0.088329   \n",
       "activity                    -0.060283 -0.081119 -0.153714 -0.105502  0.068179   \n",
       "actually                     0.006980 -0.086286 -0.007715 -0.100940 -0.014237   \n",
       "ad                           0.043385  0.015693 -0.114068  0.162840 -0.156114   \n",
       "add                          0.065492 -0.015215  0.014397 -0.114772  0.088571   \n",
       "addict                       0.063680 -0.085369 -0.100936  0.161640  0.005782   \n",
       "addicted                    -0.021063 -0.116353 -0.085872 -0.082910 -0.066901   \n",
       "addicting                    0.075862 -0.154718 -0.115967 -0.035943  0.065051   \n",
       "addiction                   -0.022224 -0.121570 -0.164046  0.000114  0.037053   \n",
       "addictive                    0.044446 -0.005216 -0.042800  0.039721  0.108935   \n",
       "addition                    -0.000304 -0.023728 -0.247746 -0.159316 -0.016654   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                         0.053409  0.076249  0.109844 -0.016581 -0.047279   \n",
       "will_not                     0.038460  0.011696  0.035405  0.003678 -0.108512   \n",
       "win                          0.201266 -0.089693  0.198117  0.128069 -0.058174   \n",
       "window                      -0.024452  0.082130  0.103574 -0.065450 -0.160271   \n",
       "wish                         0.185311  0.095738  0.090334 -0.082855 -0.145515   \n",
       "wonder                      -0.063726 -0.038604  0.141808  0.016108 -0.121845   \n",
       "wonderful                   -0.121819 -0.019157  0.108040 -0.199105 -0.103509   \n",
       "word                        -0.076788  0.055572 -0.094184  0.037543  0.172067   \n",
       "work                         0.096693  0.166608 -0.013387 -0.154851 -0.034203   \n",
       "worker                       0.057872  0.029849 -0.098129  0.028791 -0.088584   \n",
       "world                       -0.100296 -0.087241  0.060740 -0.058429 -0.146486   \n",
       "worried                      0.031979 -0.065942 -0.130018 -0.005233 -0.087113   \n",
       "worry                        0.149391 -0.062495  0.074391  0.156100  0.009033   \n",
       "worth                       -0.098121 -0.058005 -0.064479  0.136190  0.007481   \n",
       "write                       -0.010816 -0.117696  0.048246 -0.043790  0.162369   \n",
       "write_essay                  0.107930 -0.072077  0.043175 -0.009157  0.241753   \n",
       "write_letter                -0.073572 -0.097978  0.031064  0.052986  0.004131   \n",
       "write_paper                  0.028342 -0.081525  0.051980  0.021970  0.197878   \n",
       "writing                     -0.004935  0.056955  0.010158  0.036182  0.262661   \n",
       "wrong                       -0.027204 -0.115663 -0.068590  0.176000  0.168906   \n",
       "yahoo                       -0.077911 -0.201298  0.124579 -0.008861  0.066481   \n",
       "year                         0.095282  0.027088  0.115821  0.102618  0.017117   \n",
       "year_ago                    -0.061437 -0.182808  0.041300  0.014933 -0.046499   \n",
       "year_old                     0.011261 -0.163350  0.102529  0.084942 -0.083565   \n",
       "yes                         -0.221333 -0.167595 -0.077956 -0.124617  0.090622   \n",
       "young                        0.089513 -0.048530 -0.122347  0.026380 -0.106663   \n",
       "young_child                 -0.050389  0.063325 -0.133385 -0.011312  0.003758   \n",
       "youth                       -0.039554 -0.023727 -0.232815  0.234117  0.036047   \n",
       "youtube                     -0.047535 -0.050082  0.001353  0.067090 -0.066447   \n",
       "’                           -0.004921 -0.166562 -0.131816  0.039862 -0.027659   \n",
       "\n",
       "                             ...        90        91        92        93  \\\n",
       "Dear_Local_Newspaper         ...  0.083057 -0.036303 -0.172746  0.107085   \n",
       "Dear_Newspaper               ...  0.131236 -0.100623 -0.083030 -0.025299   \n",
       "Dr.                          ...  0.042051  0.012459 -0.073568  0.167597   \n",
       "Facebook                     ...  0.045397  0.061529 -0.110230 -0.138034   \n",
       "People                       ... -0.117228 -0.056509 -0.127031 -0.096924   \n",
       "ability                      ...  0.113731 -0.003393  0.111721 -0.185697   \n",
       "ability_learn                ...  0.200842  0.038983 -0.149448 -0.010260   \n",
       "ability_learn_far_away       ...  0.214203 -0.048395 -0.052231 -0.211182   \n",
       "ability_learn_faraway_place  ...  0.275119 -0.010331 -0.027732 -0.158186   \n",
       "able                         ...  0.029970 -0.001847 -0.063932 -0.096556   \n",
       "absolutely                   ... -0.134323 -0.086783 -0.121617 -0.086920   \n",
       "abuse                        ...  0.138893 -0.028911 -0.187556  0.004646   \n",
       "access                       ...  0.138290  0.049777 -0.109659 -0.086719   \n",
       "accord                       ...  0.029439 -0.012145 -0.067884 -0.066159   \n",
       "account                      ... -0.081644 -0.045451 -0.062248 -0.130844   \n",
       "accurate                     ...  0.047950 -0.004177 -0.047235  0.125650   \n",
       "acess                        ...  0.139043  0.123306 -0.073263  0.036500   \n",
       "act                          ...  0.067933  0.086227  0.030296  0.039736   \n",
       "action                       ... -0.143444  0.135385 -0.052328  0.090600   \n",
       "active                       ... -0.025298 -0.188665 -0.017035  0.020137   \n",
       "activity                     ... -0.088216 -0.000491  0.063110 -0.042854   \n",
       "actually                     ...  0.090606 -0.065074  0.019000  0.023920   \n",
       "ad                           ...  0.065438 -0.074953  0.056090  0.006360   \n",
       "add                          ... -0.004022 -0.200743  0.000062 -0.147879   \n",
       "addict                       ... -0.189984 -0.014849 -0.100763  0.062804   \n",
       "addicted                     ... -0.121385  0.001723 -0.041007  0.033138   \n",
       "addicting                    ... -0.020696  0.044556 -0.041164 -0.002323   \n",
       "addiction                    ... -0.180623  0.028451 -0.135802 -0.037686   \n",
       "addictive                    ... -0.034896 -0.014617 -0.072891 -0.068128   \n",
       "addition                     ... -0.006716 -0.130061  0.046863  0.101913   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "will                         ... -0.108592  0.088655 -0.043294  0.019784   \n",
       "will_not                     ... -0.039451  0.173878  0.027983  0.000860   \n",
       "win                          ... -0.057563  0.220913  0.009554 -0.068581   \n",
       "window                       ... -0.157480  0.075503  0.128558 -0.207491   \n",
       "wish                         ...  0.046139  0.137625 -0.134025 -0.010959   \n",
       "wonder                       ... -0.006780  0.145184  0.071114  0.042687   \n",
       "wonderful                    ...  0.032440  0.067650  0.142903 -0.063284   \n",
       "word                         ...  0.037526 -0.045535  0.082279  0.013238   \n",
       "work                         ...  0.015053 -0.172235 -0.083438 -0.119303   \n",
       "worker                       ... -0.002502 -0.220155 -0.013423 -0.171200   \n",
       "world                        ...  0.228013 -0.096753 -0.002665 -0.044096   \n",
       "worried                      ...  0.092626 -0.053344 -0.051258  0.146055   \n",
       "worry                        ... -0.173196  0.016125  0.086681 -0.016223   \n",
       "worth                        ... -0.104596  0.087585 -0.104555  0.040626   \n",
       "write                        ...  0.081206 -0.146629  0.106912 -0.111590   \n",
       "write_essay                  ...  0.096314 -0.044447  0.018310 -0.127319   \n",
       "write_letter                 ...  0.122145 -0.225296 -0.089557  0.001705   \n",
       "write_paper                  ...  0.022393 -0.094506  0.112994  0.026295   \n",
       "writing                      ...  0.052755 -0.107070  0.100780 -0.044422   \n",
       "wrong                        ...  0.037976  0.031521  0.092895  0.315482   \n",
       "yahoo                        ...  0.080930  0.005401  0.000851 -0.174040   \n",
       "year                         ... -0.016205  0.062934 -0.077875  0.093595   \n",
       "year_ago                     ... -0.061817  0.123939 -0.093537  0.038132   \n",
       "year_old                     ... -0.021522  0.070995  0.019442 -0.011797   \n",
       "yes                          ... -0.025857 -0.018503 -0.081860  0.103133   \n",
       "young                        ...  0.069264  0.205986  0.165020 -0.191593   \n",
       "young_child                  ...  0.087158  0.076534 -0.033679 -0.017185   \n",
       "youth                        ...  0.002169 -0.023236 -0.018881  0.106277   \n",
       "youtube                      ... -0.053343 -0.015251 -0.038807 -0.086042   \n",
       "’                            ...  0.000081  0.092527 -0.125255  0.232657   \n",
       "\n",
       "                                   94        95        96        97        98  \\\n",
       "Dear_Local_Newspaper        -0.002207 -0.054023 -0.026584  0.041401  0.089317   \n",
       "Dear_Newspaper               0.176573 -0.096566 -0.069011  0.045533  0.033215   \n",
       "Dr.                         -0.038416  0.056799  0.049680  0.022879 -0.012547   \n",
       "Facebook                    -0.101535  0.122856  0.225491  0.016016  0.037411   \n",
       "People                       0.016651 -0.033707 -0.080514 -0.129022  0.033612   \n",
       "ability                      0.000819  0.260463  0.133326 -0.022905  0.135234   \n",
       "ability_learn               -0.103228  0.045211  0.028685  0.023814  0.066922   \n",
       "ability_learn_far_away      -0.067038 -0.046495 -0.022695 -0.043916 -0.026545   \n",
       "ability_learn_faraway_place -0.004436 -0.042032  0.026861 -0.012597  0.061112   \n",
       "able                        -0.110397  0.145599  0.058895 -0.044561  0.179839   \n",
       "absolutely                   0.001776 -0.044351 -0.079989  0.007832 -0.135252   \n",
       "abuse                       -0.081402 -0.078189 -0.054812 -0.072359  0.006348   \n",
       "access                       0.018677  0.038707  0.214862  0.108611 -0.051788   \n",
       "accord                      -0.064794  0.077432  0.110461 -0.009767 -0.051933   \n",
       "account                      0.021602  0.045767  0.112684  0.020622 -0.031739   \n",
       "accurate                     0.033922 -0.016223  0.088164  0.177889 -0.017273   \n",
       "acess                        0.029464  0.010733  0.102984  0.055651 -0.032433   \n",
       "act                          0.058830  0.093552  0.140933 -0.012769 -0.014208   \n",
       "action                       0.018039 -0.082166  0.013245  0.094843  0.064940   \n",
       "active                      -0.121720 -0.019516 -0.057793 -0.134708 -0.021576   \n",
       "activity                    -0.248585  0.037895  0.140672  0.000508 -0.057184   \n",
       "actually                     0.155269  0.134975  0.084377 -0.108588  0.016488   \n",
       "ad                          -0.053827 -0.055392  0.041043  0.039948 -0.047681   \n",
       "add                         -0.054279  0.063821  0.003096  0.019184  0.072677   \n",
       "addict                      -0.072491 -0.054097 -0.050763 -0.024606  0.045586   \n",
       "addicted                    -0.053710  0.002833 -0.151991 -0.012026 -0.053500   \n",
       "addicting                   -0.120047  0.072648 -0.037826  0.137466  0.007619   \n",
       "addiction                    0.013853 -0.016029 -0.183707  0.182267 -0.015915   \n",
       "addictive                   -0.142953 -0.042299 -0.033439  0.133010 -0.002407   \n",
       "addition                     0.006348 -0.054943  0.125698  0.100385  0.147561   \n",
       "...                               ...       ...       ...       ...       ...   \n",
       "will                        -0.045943 -0.083427 -0.278405 -0.181813  0.149525   \n",
       "will_not                    -0.006654 -0.099778 -0.146238 -0.187370 -0.020311   \n",
       "win                         -0.105390 -0.068331  0.083326  0.055815 -0.045385   \n",
       "window                       0.143063  0.048720 -0.010025  0.151653  0.118451   \n",
       "wish                         0.025053 -0.010894 -0.032358  0.013457  0.069877   \n",
       "wonder                       0.111052 -0.197036  0.003673  0.086538  0.035479   \n",
       "wonderful                    0.077990 -0.083893  0.029566  0.009894  0.029312   \n",
       "word                        -0.041055  0.293838 -0.082126 -0.001794  0.103639   \n",
       "work                        -0.044952  0.023880 -0.010212 -0.037849 -0.073563   \n",
       "worker                       0.163627  0.041264  0.070564 -0.101153 -0.005687   \n",
       "world                        0.204079 -0.045788  0.020667 -0.024602 -0.044594   \n",
       "worried                     -0.053174 -0.227502  0.013293  0.035557  0.054435   \n",
       "worry                        0.018278  0.057971 -0.174869  0.051112  0.063893   \n",
       "worth                        0.083674 -0.016526 -0.317687 -0.199293 -0.058497   \n",
       "write                       -0.037091  0.156758 -0.080771 -0.085659  0.063928   \n",
       "write_essay                 -0.042781  0.205939  0.047902  0.039154  0.167516   \n",
       "write_letter                 0.155174  0.060816 -0.104516  0.033624  0.010150   \n",
       "write_paper                 -0.064076  0.253173  0.083666 -0.006361  0.068730   \n",
       "writing                     -0.146656  0.136562 -0.155263 -0.034360  0.040746   \n",
       "wrong                        0.080589 -0.107264  0.047725  0.101677 -0.060642   \n",
       "yahoo                        0.058013  0.026157  0.105651  0.076351  0.092045   \n",
       "year                        -0.052626  0.079668 -0.189148 -0.176204  0.061498   \n",
       "year_ago                    -0.105527 -0.060461  0.151425 -0.005155  0.005458   \n",
       "year_old                    -0.099303 -0.123955 -0.019513 -0.002453 -0.178045   \n",
       "yes                          0.075920 -0.016118  0.022301  0.074965 -0.014454   \n",
       "young                       -0.152620  0.069355  0.092479  0.042936  0.004319   \n",
       "young_child                 -0.271744  0.113278 -0.021692  0.116293 -0.151703   \n",
       "youth                       -0.002704  0.147399  0.020246  0.256224  0.002119   \n",
       "youtube                     -0.101111 -0.011986  0.064623  0.043936 -0.041173   \n",
       "’                            0.127279 -0.003595 -0.043506 -0.029938 -0.100074   \n",
       "\n",
       "                                   99  \n",
       "Dear_Local_Newspaper        -0.006336  \n",
       "Dear_Newspaper               0.079021  \n",
       "Dr.                          0.038800  \n",
       "Facebook                    -0.037007  \n",
       "People                      -0.022756  \n",
       "ability                     -0.020684  \n",
       "ability_learn                0.105161  \n",
       "ability_learn_far_away       0.121319  \n",
       "ability_learn_faraway_place  0.114431  \n",
       "able                        -0.101470  \n",
       "absolutely                  -0.084513  \n",
       "abuse                       -0.008121  \n",
       "access                      -0.025838  \n",
       "accord                       0.118402  \n",
       "account                     -0.023910  \n",
       "accurate                    -0.106321  \n",
       "acess                       -0.082574  \n",
       "act                         -0.048643  \n",
       "action                       0.072419  \n",
       "active                       0.065409  \n",
       "activity                    -0.010675  \n",
       "actually                    -0.051784  \n",
       "ad                          -0.048392  \n",
       "add                         -0.126859  \n",
       "addict                       0.021930  \n",
       "addicted                     0.047624  \n",
       "addicting                   -0.072847  \n",
       "addiction                    0.002725  \n",
       "addictive                    0.158381  \n",
       "addition                    -0.031337  \n",
       "...                               ...  \n",
       "will                         0.186288  \n",
       "will_not                    -0.008023  \n",
       "win                         -0.174301  \n",
       "window                      -0.048937  \n",
       "wish                         0.100498  \n",
       "wonder                       0.042903  \n",
       "wonderful                    0.077423  \n",
       "word                        -0.033845  \n",
       "work                        -0.151407  \n",
       "worker                       0.089583  \n",
       "world                        0.056458  \n",
       "worried                      0.078444  \n",
       "worry                        0.095718  \n",
       "worth                       -0.112080  \n",
       "write                       -0.056529  \n",
       "write_essay                 -0.085724  \n",
       "write_letter                -0.053705  \n",
       "write_paper                 -0.072751  \n",
       "writing                     -0.121560  \n",
       "wrong                        0.067841  \n",
       "yahoo                       -0.025008  \n",
       "year                        -0.014047  \n",
       "year_ago                    -0.035396  \n",
       "year_old                     0.006188  \n",
       "yes                         -0.042515  \n",
       "young                       -0.003103  \n",
       "young_child                 -0.009119  \n",
       "youth                        0.011791  \n",
       "youtube                     -0.009698  \n",
       "’                            0.238505  \n",
       "\n",
       "[1250 rows x 100 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in essay2vec_model.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(essay2vec_model.wv.syn0norm[term_indices, :], index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy wall of numbers! This DataFrame has 1,257 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=5):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in essay2vec_model.wv.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print('{:20} {}'.format(word, round(similarity, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What things are like Facebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter              0.667\n",
      "facebook_myspace     0.66\n",
      "skype                0.649\n",
      "Facebook             0.638\n",
      "yahoo                0.628\n"
     ]
    }
   ],
   "source": [
    "get_related_terms('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strongly_believe     0.655\n",
      "dear                 0.604\n",
      "believe              0.598\n",
      "come_attention       0.598\n",
      "great_invention      0.585\n"
     ]
    }
   ],
   "source": [
    "get_related_terms('society')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Word2Vec From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('dear local newspaper think effects computers people great learning skillsaffects give us time chat friendsnew people helps us learn globeastronomy keeps us troble thing dont think would feel teenager always phone friends ever time chat friends buisness partner things well theres new way chat computer plenty sites internet organization organization caps facebook myspace ect think setting meeting boss computer teenager fun phone rushing get cause want use learn countrysstates outside well computerinternet new way learn going time might think child spends lot time computer ask question economy sea floor spreading even dates youll surprise much heshe knows believe computer much interesting class day reading books child home computer local library better friends fresh perpressured something know isnt right might know child caps forbidde hospital bed driveby rather child computer learning chatting playing games safe sound home community place hope reached point understand agree computers great effects child gives us time chat friendsnew people helps us learn globe believe keeps us troble thank listening',\n",
       "      dtype='<U1114')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(test_essay)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# remove_terms = punctuation + '0123456789'\n",
    "\n",
    "# norm_corpus = [[word.lower() for word in sent if word not in remove_terms] for sent in test_essay]\n",
    "# norm_corpus = [' '.join(tok_sent) for tok_sent in norm_corpus]\n",
    "# norm_corpus = filter(None, normalize_corpus(norm_corpus))\n",
    "# norm_corpus = [tok_sent for tok_sent in norm_corpus if len(tok_sent.split()) > 2]\n",
    "\n",
    "# print('Total lines:', len(test_essay))\n",
    "\n",
    "# norm_corpus\n",
    "# print('\\nSample line:', test_essay[1])\n",
    "# print('\\nProcessed line:', norm_corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "type(norm_corpus)\n",
    "# tokenizer.fit_on_texts(norm_corpus)\n",
    "# word2id = tokenizer.word_index\n",
    "\n",
    "# # build vocabulary of unique words\n",
    "# word2id['PAD'] = 0\n",
    "# id2word = {v:k for k, v in word2id.items()}\n",
    "# wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_corpus]\n",
    "\n",
    "# vocab_size = len(word2id)\n",
    "# embed_size = 100\n",
    "# window_size = 2 # context window size\n",
    "\n",
    "# print('Vocabulary Size:', vocab_size)\n",
    "# print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
